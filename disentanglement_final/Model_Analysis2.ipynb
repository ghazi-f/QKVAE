{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Instanciating the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 443259  examples. statistics:\n -words: 8.881732801815643+-3.6417630572571147(quantiles(0.5, 0.7, 0.9, 0.95, 0.99:9.0,11.0,14.0,15.0,15.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 4000  examples. statistics:\n -words: 8.9255+-3.668371539252806(quantiles(0.5, 0.7, 0.9, 0.95, 0.99:9.0,11.0,14.0,15.0,15.0)\nDataset has 1000  examples. statistics:\n -words: 10.325+-2.8399603870476784(quantiles(0.5, 0.7, 0.9, 0.95, 0.99:10.0,12.0,14.0,15.0,15.0)\ndata loading took 5.954154968261719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words:  9600 , On device:  cuda\nLoss Type:  VAE\nreconstruction net size: 25.54 M\nprior net sizes:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model at step 27704\nUnsupervised training examples:  443264\nUnsupervised val examples:  42752\nNumber of parameters:  30.71 M\nInference parameters:  06.02 M\nGeneration parameters:  25.91 M\nEmbedding parameters:  01.23 M\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "from torch import device\n",
    "import torch\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "\n",
    "from data_prep import NLIGenData2, OntoGenData, HuggingYelp2\n",
    "from disentanglement_final.models import DisentanglementTransformerVAE, LaggingDisentanglementTransformerVAE\n",
    "from disentanglement_final.h_params import DefaultTransformerHParams as HParams\n",
    "from disentanglement_final.graphs import *\n",
    "from components.criteria import *\n",
    "parser = argparse.ArgumentParser()\n",
    "from torch.nn import MultiheadAttention\n",
    "# Training and Optimization\n",
    "k, kz, klstm = 1, 8, 2\n",
    "parser.add_argument(\"--test_name\", default='unnamed', type=str)\n",
    "parser.add_argument(\"--data\", default='nli', choices=[\"nli\", \"ontonotes\", \"yelp\"], type=str)\n",
    "parser.add_argument(\"--csv_out\", default='disentqkv.csv', type=str)\n",
    "parser.add_argument(\"--max_len\", default=17, type=int)\n",
    "parser.add_argument(\"--batch_size\", default=128, type=int)\n",
    "parser.add_argument(\"--grad_accu\", default=1, type=int)\n",
    "parser.add_argument(\"--n_epochs\", default=20, type=int)\n",
    "parser.add_argument(\"--test_freq\", default=32, type=int)\n",
    "parser.add_argument(\"--complete_test_freq\", default=160, type=int)\n",
    "parser.add_argument(\"--generation_weight\", default=1, type=float)\n",
    "parser.add_argument(\"--device\", default='cuda:0', choices=[\"cuda:0\", \"cuda:1\", \"cuda:2\", \"cpu\"], type=str)\n",
    "parser.add_argument(\"--embedding_dim\", default=128, type=int)#################\"\n",
    "parser.add_argument(\"--pretrained_embeddings\", default=False, type=bool)#################\"\n",
    "parser.add_argument(\"--z_size\", default=96*kz, type=int)#################\"\n",
    "parser.add_argument(\"--z_emb_dim\", default=192*k, type=int)#################\"\n",
    "parser.add_argument(\"--n_keys\", default=4, type=int)#################\"\n",
    "parser.add_argument(\"--n_latents\", default=[16, 16, 16], nargs='+', type=int)#################\"\n",
    "parser.add_argument(\"--text_rep_l\", default=3, type=int)\n",
    "parser.add_argument(\"--text_rep_h\", default=192*k, type=int)\n",
    "parser.add_argument(\"--encoder_h\", default=192*k, type=int)#################\"\n",
    "parser.add_argument(\"--encoder_l\", default=2, type=int)#################\"\n",
    "parser.add_argument(\"--decoder_h\", default=192*k, type=int)\n",
    "parser.add_argument(\"--decoder_l\", default=2, type=int)#################\"\n",
    "parser.add_argument(\"--highway\", default=False, type=bool)\n",
    "parser.add_argument(\"--markovian\", default=True, type=bool)\n",
    "parser.add_argument('--minimal_enc', dest='minimal_enc', action='store_true')\n",
    "parser.add_argument('--no-minimal_enc', dest='minimal_enc', action='store_false')\n",
    "parser.set_defaults(minimal_enc=False)\n",
    "parser.add_argument(\"--losses\", default='VAE', choices=[\"VAE\", \"IWAE\" \"LagVAE\"], type=str)\n",
    "parser.add_argument(\"--graph\", default='Normal', choices=[\"Vanilla\", \"IndepInfer\", \"QKV\", \"HQKV\"], type=str)\n",
    "parser.add_argument(\"--training_iw_samples\", default=1, type=int)\n",
    "parser.add_argument(\"--testing_iw_samples\", default=5, type=int)\n",
    "parser.add_argument(\"--test_prior_samples\", default=10, type=int)\n",
    "parser.add_argument(\"--anneal_kl0\", default=3000, type=int)\n",
    "parser.add_argument(\"--anneal_kl1\", default=6000, type=int)\n",
    "parser.add_argument(\"--grad_clip\", default=5., type=float)\n",
    "parser.add_argument(\"--kl_th\", default=0/(768*k/2), type=float or None)\n",
    "parser.add_argument(\"--max_elbo1\", default=6.0, type=float)\n",
    "parser.add_argument(\"--max_elbo2\", default=4.0, type=float)\n",
    "parser.add_argument(\"--max_elbo_choice\", default=10, type=int)\n",
    "parser.add_argument(\"--kl_beta\", default=0.4, type=float)\n",
    "parser.add_argument(\"--dropout\", default=0.3, type=float)\n",
    "parser.add_argument(\"--word_dropout\", default=0.1, type=float)\n",
    "parser.add_argument(\"--l2_reg\", default=0, type=float)\n",
    "parser.add_argument(\"--lr\", default=2e-4, type=float)\n",
    "parser.add_argument(\"--lr_reduction\", default=4., type=float)\n",
    "parser.add_argument(\"--wait_epochs\", default=1, type=float)\n",
    "parser.add_argument(\"--save_all\", default=True, type=bool)\n",
    "\n",
    "flags, _ = parser.parse_known_args()\n",
    "\n",
    "# Manual Settings, Deactivate before pushing\n",
    "if True:\n",
    "    flags.batch_size = 128\n",
    "    flags.grad_accu = 1\n",
    "    flags.max_len = 17\n",
    "    flags.test_name = \"nliLM/HQKVTest2\"\n",
    "    flags.data = \"yelp\"\n",
    "    flags.n_latents = [8]\n",
    "    flags.graph =\"HQKV\"  # \"Vanilla\"\n",
    "    # flags.losses = \"LagVAE\"\n",
    "    flags.kl_beta = 0.5\n",
    "    \n",
    "    # flags.anneal_kl0 = 0\n",
    "    flags.max_elbo_choice = 6\n",
    "    # flags.z_size = 16\n",
    "    # flags.encoder_h = 256\n",
    "    # flags.decoder_h = 256\n",
    "\n",
    "\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "GRAPH = {\"Vanilla\": get_vanilla_graph,\n",
    "         \"IndepInfer\": get_structured_auto_regressive_indep_graph,\n",
    "         \"QKV\": get_qkv_graph,\n",
    "         \"HQKV\": get_hqkv_graph_old}[flags.graph]\n",
    "if flags.graph == \"NormalLSTM\":\n",
    "    flags.encoder_h = int(flags.encoder_h/k*klstm)\n",
    "if flags.graph == \"Vanilla\":\n",
    "    flags.n_latents = [flags.z_size]\n",
    "if flags.losses == \"LagVAE\":\n",
    "    flags.anneal_kl0 = 0\n",
    "    flags.anneal_kl1 = 0\n",
    "Data = {\"nli\": NLIGenData2, \"ontonotes\": OntoGenData, \"yelp\": HuggingYelp2}[flags.data]\n",
    "MAX_LEN = flags.max_len\n",
    "BATCH_SIZE = flags.batch_size\n",
    "GRAD_ACCU = flags.grad_accu\n",
    "N_EPOCHS = flags.n_epochs\n",
    "TEST_FREQ = flags.test_freq\n",
    "COMPLETE_TEST_FREQ = flags.complete_test_freq\n",
    "DEVICE = device(flags.device)\n",
    "# This prevents illegal memory access on multigpu machines (unresolved issue on torch's github)\n",
    "if flags.device.startswith('cuda'):\n",
    "    torch.cuda.set_device(int(flags.device[-1]))\n",
    "LOSSES = {'IWAE': [IWLBo],\n",
    "          'VAE': [ELBo],\n",
    "          'LagVAE': [ELBo]}[flags.losses]\n",
    "\n",
    "ANNEAL_KL = [flags.anneal_kl0*flags.grad_accu, flags.anneal_kl1*flags.grad_accu]\n",
    "LOSS_PARAMS = [1]\n",
    "if flags.grad_accu > 1:\n",
    "    LOSS_PARAMS = [w/flags.grad_accu for w in LOSS_PARAMS]\n",
    "\n",
    "\n",
    "data = Data(MAX_LEN, BATCH_SIZE, N_EPOCHS, DEVICE, pretrained=flags.pretrained_embeddings)\n",
    "h_params = HParams(len(data.vocab.itos), len(data.tags.itos) if flags.data == 'yelp' else None, MAX_LEN, BATCH_SIZE, N_EPOCHS,\n",
    "                   device=DEVICE, vocab_ignore_index=data.vocab.stoi['<pad>'], decoder_h=flags.decoder_h,\n",
    "                   decoder_l=flags.decoder_l, encoder_h=flags.encoder_h, encoder_l=flags.encoder_l,\n",
    "                   text_rep_h=flags.text_rep_h, text_rep_l=flags.text_rep_l,\n",
    "                   test_name=flags.test_name, grad_accumulation_steps=GRAD_ACCU,\n",
    "                   optimizer_kwargs={'lr': flags.lr, #'weight_decay': flags.l2_reg, 't0':100, 'lambd':0.},\n",
    "                                     'weight_decay': flags.l2_reg, 'betas': (0.9, 0.99)},\n",
    "                   is_weighted=[], graph_generator=GRAPH,\n",
    "                   z_size=flags.z_size, embedding_dim=flags.embedding_dim, anneal_kl=ANNEAL_KL,\n",
    "                   grad_clip=flags.grad_clip*flags.grad_accu, kl_th=flags.kl_th, highway=flags.highway,\n",
    "                   losses=LOSSES, dropout=flags.dropout, training_iw_samples=flags.training_iw_samples,\n",
    "                   testing_iw_samples=flags.testing_iw_samples, loss_params=LOSS_PARAMS, optimizer=optim.AdamW,\n",
    "                   markovian=flags.markovian, word_dropout=flags.word_dropout, contiguous_lm=False,\n",
    "                   test_prior_samples=flags.test_prior_samples, n_latents=flags.n_latents, n_keys=flags.n_keys,\n",
    "                   max_elbo=[flags.max_elbo_choice, flags.max_elbo1],  # max_elbo is paper's beta\n",
    "                   z_emb_dim=flags.z_emb_dim, minimal_enc=flags.minimal_enc, kl_beta=flags.kl_beta)\n",
    "val_iterator = iter(data.val_iter)\n",
    "print(\"Words: \", len(data.vocab.itos), \", On device: \", DEVICE.type)\n",
    "print(\"Loss Type: \", flags.losses)\n",
    "if flags.losses == 'LagVAE':\n",
    "    model = LaggingDisentanglementTransformerVAE(data.vocab, data.tags, h_params, wvs=data.wvs, dataset=flags.data,\n",
    "                                                 enc_iter=data.enc_train_iter)\n",
    "else:\n",
    "    model = DisentanglementTransformerVAE(data.vocab, data.tags, h_params, wvs=data.wvs, dataset=flags.data)\n",
    "if DEVICE.type == 'cuda':\n",
    "    model.cuda(DEVICE)\n",
    "\n",
    "total_unsupervised_train_samples = len(data.train_iter)*BATCH_SIZE\n",
    "total_unsupervised_val_samples = len(data.val_iter)*BATCH_SIZE\n",
    "print(\"Unsupervised training examples: \", total_unsupervised_train_samples)\n",
    "print(\"Unsupervised val examples: \", total_unsupervised_val_samples)\n",
    "current_time = time()\n",
    "#print(model)\n",
    "number_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Number of parameters: \", \"{0:05.2f} M\".format(number_parameters/1e6))\n",
    "number_parameters = sum(p.numel() for p in model.infer_bn.parameters() if p.requires_grad)\n",
    "print(\"Inference parameters: \", \"{0:05.2f} M\".format(number_parameters/1e6))\n",
    "number_parameters = sum(p.numel() for p in model.gen_bn.parameters() if p.requires_grad)\n",
    "print(\"Generation parameters: \", \"{0:05.2f} M\".format(number_parameters/1e6))\n",
    "number_parameters = sum(p.numel() for p in model.word_embeddings.parameters() if p.requires_grad)\n",
    "print(\"Embedding parameters: \", \"{0:05.2f} M\".format(number_parameters/1e6))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "      \n",
    "# import spacy\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "# predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/openie-model.2020.03.26.tar.gz\")\n",
    "# const_predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/elmo-constituency-parser-2020.02.10.tar.gz\")\n",
    "\n",
    "# def batch_sent_relations(sents):\n",
    "#     target = [{'sentence': sent} for sent in sents]\n",
    "#     preds = predictor.predict_batch_json(target)\n",
    "#     sent_dicts = []\n",
    "#     for pred in preds:\n",
    "#         sent_dict = {'ARG0': '', 'V': '', 'ARG1': '', 'ARG*': ''}\n",
    "#         if len(pred['verbs']):\n",
    "#             el = pred['verbs'][0]\n",
    "#             for v_i in el['description'].split('[')[1:]:\n",
    "#                 in_bracket = v_i.split(']')[0]\n",
    "#                 try:\n",
    "#                     arg_l, arg_str = in_bracket.split(':')\n",
    "#                     if arg_l in sent_dict:\n",
    "#                         sent_dict[arg_l] = arg_str\n",
    "#                     else:\n",
    "#                         sent_dict['ARG*'] = ''.join([sent_dict['ARG*'], arg_str])\n",
    "#                 except ValueError as e:\n",
    "#                     print('this raised an anomaly:', el)\n",
    "#         if sent_dict['ARG0'] == '':\n",
    "#             sent_dict['ARG0'] = sent_dict['ARG1']\n",
    "#             sent_dict['ARG1'] = ''\n",
    "#         sent_dicts.append(sent_dict)\n",
    "#     return sent_dicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "      \n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def shallow_constituents(sents, verbose=0):\n",
    "    target = [{'sentence': sent} for sent in sents]\n",
    "    preds = const_predictor.predict_batch_json(target)\n",
    "    outputs = []\n",
    "    for pred in preds:\n",
    "        root_c = pred['hierplane_tree']['root']['children']\n",
    "        subj, verb, np, pp = '', '', '', ''\n",
    "        parsing_error = False\n",
    "        try:\n",
    "            subj = [c['word'] for c in root_c if c['nodeType']=='NP'][0]\n",
    "            VP_c = [c for c in root_c if c['nodeType']=='VP'][0]['children']\n",
    "            if not any([c['nodeType'].startswith('VB') for c in VP_c]):\n",
    "                outputs.append({'subj':subj, 'verb':'', 'np':'', 'pp':''})\n",
    "                continue\n",
    "            verb = [c['word'] for c in VP_c if c['nodeType'].startswith('VB')][0]\n",
    "            \n",
    "            np = [c['word'] for c in VP_c if c['nodeType']=='NP'][0] if any([c['nodeType']=='NP' for c in VP_c]) else ''\n",
    "            pp = [c['word'] for c in VP_c if c['nodeType']=='PP'][0] if any([c['nodeType']=='PP' for c in VP_c]) else ''\n",
    "            if verbose: \n",
    "                print([[c['nodeType'],c['word']] for c in VP_c])\n",
    "            while any([c['nodeType'] == 'VP' for c in VP_c]):\n",
    "                VP_c = [c for c in VP_c if c['nodeType']=='VP'][0]['children']\n",
    "                if verbose: \n",
    "                    print([[c['nodeType'],c['word']] for c in VP_c])\n",
    "                verb += ' '+[c['word'] for c in VP_c if c['nodeType'].startswith('VB')][0]\n",
    "                if any([c['nodeType']=='NP' for c in VP_c]):\n",
    "                    for np_i in [c['word'] for c in VP_c if c['nodeType']=='NP']:\n",
    "                        np += ' '+np_i\n",
    "                if any([c['nodeType']=='PP' for c in VP_c]):\n",
    "                    for pp_i in [c['word'] for c in VP_c if c['nodeType']=='PP']:\n",
    "                        pp += ' '+pp_i\n",
    "        except IndexError:\n",
    "            parsing_error = True\n",
    "        outputs.append({'subj':subj, 'verb':verb, 'np':np, 'pp':pp, 'err': parsing_error})\n",
    "    return outputs\n",
    "\n",
    "def shallow_dependencies(sents):\n",
    "    docs = nlp.pipe(sents)\n",
    "    relations = []\n",
    "    for doc in docs:\n",
    "        subj, verb, dobj, pobj = ['', []], ['', []], ['', []], ['', []]\n",
    "        for i, tok in enumerate(doc):\n",
    "            if tok.dep_ =='ROOT':\n",
    "                verb = [tok.text, [tok.i]]\n",
    "            if tok.dep_ == 'nsubj' and subj[0] == '':\n",
    "                subj = [' '.join([toki.text for toki in tok.subtree]), [toki.i for toki in tok.subtree]]\n",
    "            if tok.dep_ == 'dobj' and dobj[0] == '':\n",
    "                dobj = [' '.join([toki.text for toki in tok.subtree]), [toki.i for toki in tok.subtree]]\n",
    "            if tok.dep_ == 'pobj' and pobj[0] == '':\n",
    "                pobj = [' '.join([toki.text for toki in tok.subtree]), [toki.i for toki in tok.subtree]]\n",
    "        relations.append({'text':{'subj': subj[0], 'verb': verb[0], 'dobj': dobj[0], 'pobj': pobj[0]},\n",
    "                         'idx':{'subj': subj[1], 'verb': verb[1], 'dobj': dobj[1], 'pobj': pobj[1]}})\n",
    "    return relations\n",
    "\n",
    "def get_sentence_statistics(orig, sen, orig_relations=None, relations=None):\n",
    "    same_struct = True\n",
    "    error = orig_relations.pop('err', None) or relations.pop('err', None)\n",
    "    for k in orig_relations.keys():\n",
    "        if (orig_relations[k] == '' and relations[k] != '') or (orig_relations[k] == '' and relations[k] != ''):\n",
    "            same_struct = False\n",
    "    def get_diff(arg):\n",
    "        if orig_relations[arg] != '' and relations[arg] != '':\n",
    "            return orig_relations[arg] != relations[arg], False\n",
    "        else: \n",
    "            return False, orig_relations[arg] != relations[arg]\n",
    "    return get_diff('subj'), get_diff('verb'), get_diff('np'), get_diff('pp'), same_struct, error\n",
    "    # return get_diff('ARG0'), get_diff('V'), get_diff('ARG1'), get_diff('ARG*'), same_struct\n",
    "\n",
    "def get_sentence_statistics2(orig, sen, orig_relations=None, relations=None):\n",
    "    orig_relations, relations = orig_relations['text'], relations['text']\n",
    "    same_struct = True\n",
    "    for k in orig_relations.keys():\n",
    "        if (orig_relations[k] == '' and relations[k] != '') or (orig_relations[k] == '' and relations[k] != ''):\n",
    "            same_struct = False\n",
    "    def get_diff(arg):\n",
    "        if orig_relations[arg] != '' and relations[arg] != '':\n",
    "            return orig_relations[arg] != relations[arg], False\n",
    "        else: \n",
    "            return False, orig_relations[arg] != relations[arg]\n",
    "    return get_diff('subj'), get_diff('verb'), get_diff('dobj'), get_diff('pobj'), same_struct\n",
    "\n",
    "\n",
    "\n",
    "def _get_stat_data_frame(model, n_samples=20, n_alterations=10, batch_size=10):\n",
    "    stats = []\n",
    "    nlatents = model.h_params.n_latents\n",
    "    # Generating n_samples sentences    \n",
    "    text, samples, _ = model.get_sentences(n_samples=batch_size, gen_len=model.h_params.max_len-1,\n",
    "                                                sample_w=False, vary_z=True, complete=None)\n",
    "    orig_rels = shallow_dependencies(text)\n",
    "    for _ in tqdm(range(int(n_samples / batch_size)), desc=\"Generating original sentences\"):\n",
    "        text_i, samples_i, _ = model.get_sentences(n_samples=batch_size, gen_len=model.h_params.max_len-1,\n",
    "                                                    sample_w=False, vary_z=True, complete=None)\n",
    "        text.extend(text_i)\n",
    "        for k in samples.keys():\n",
    "            samples[k] = torch.cat([samples[k], samples_i[k]])\n",
    "        orig_rels.extend(shallow_dependencies(text_i))\n",
    "    for i in range(int(n_samples / batch_size)):\n",
    "        for j in tqdm(range(sum(nlatents)), desc=\"Processing sample {}\".format(str(i))):\n",
    "            # Altering the sentences\n",
    "            alt_text, _ = model._get_alternative_sentences(\n",
    "                                                       prev_latent_vals={k: v[i * batch_size:(i + 1) * batch_size]\n",
    "                                                                         for k, v in samples.items()},\n",
    "                                                       params=None, var_z_ids=[j], n_samples=n_alterations,\n",
    "                                                       gen_len=model.h_params.max_len-1, complete=None)\n",
    "            alt_rels = shallow_dependencies(alt_text)\n",
    "            # Getting alteration statistics\n",
    "            for k in range(n_alterations * batch_size):\n",
    "                orig_text = text[(i * batch_size) + k % batch_size]\n",
    "                try:\n",
    "                    arg0_diff, v_diff, arg1_diff, arg_star_diff, same_struct = \\\n",
    "                        get_sentence_statistics2(orig_text, alt_text[k], orig_rels[(i * batch_size) + k % batch_size],\n",
    "                                                alt_rels[k])\n",
    "                except RecursionError or IndexError:\n",
    "                    continue\n",
    "                stats.append([orig_text, alt_text[k], j, int(arg0_diff[0]), int(v_diff[0]), \n",
    "                              int(arg1_diff[0]), int(arg_star_diff[0]), int(arg0_diff[1]), int(v_diff[1]), \n",
    "                              int(arg1_diff[1]), int(arg_star_diff[1]), same_struct])\n",
    "\n",
    "    header = ['original', 'altered', 'alteration_id', 'subj_diff', 'verb_diff', 'dobj_diff', 'pobj_diff',\n",
    "              'subj_struct', 'verb_struct', 'dobj_struct', 'pobj_struct', 'same_struct']\n",
    "    df = pd.DataFrame(stats, columns=header)\n",
    "    return df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_sent_relations' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-fa1182ff9c90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mex_sens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'The man is breathing'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"a group of people gave the boy a bike in summer\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mrels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_sent_relations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mex_sens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'ARG0'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'V'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ARG1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m':'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mrels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'batch_sent_relations' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "\n",
    "ex_sens = ['The man is breathing', \"a group of people gave the boy a bike in summer\"]\n",
    "rels = batch_sent_relations(ex_sens)\n",
    "print(rels)\n",
    "for arg in ['ARG0', 'V', 'ARG1']:\n",
    "    print(arg, ':', rels[0][arg]==rels[1][arg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 6.00 GiB total capacity; 497.50 MiB already allocated; 3.93 GiB free; 544.00 MiB reserved in total by PyTorch)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-67fb25ddea9b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_stat_data_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_alterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfer_bn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen_bn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-44355c0d27fc>\u001b[0m in \u001b[0;36m_get_stat_data_frame\u001b[1;34m(model, n_samples, n_alterations, batch_size)\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;31m# Generating n_samples sentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     text, samples, _ = model.get_sentences(n_samples=batch_size, gen_len=model.h_params.max_len-1,\n\u001b[1;32m---> 96\u001b[1;33m                                                 sample_w=False, vary_z=True, complete=None)\n\u001b[0m\u001b[0;32m     97\u001b[0m     \u001b[0morig_rels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshallow_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Generating original sentences\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Experiments\\GLUE_BENCH\\disentanglement_transformer\\models.py\u001b[0m in \u001b[0;36mget_sentences\u001b[1;34m(self, n_samples, gen_len, sample_w, vary_z, complete, contains, max_tries)\u001b[0m\n\u001b[0;32m    592\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m                 self.gen_bn({'x_prev': x_prev, **{k: v.expand(v.shape[0], i + 1, v.shape[-1])\n\u001b[1;32m--> 594\u001b[1;33m                                                  for k, v in z_input.items()}})\n\u001b[0m\u001b[0;32m    595\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msample_w\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m                     \u001b[0msamples_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerated_v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpost_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'logits'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Experiments\\GLUE_BENCH\\components\\bayesnets.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs, n_iw, target, eval, prev_states, force_iw, complete, lens, plant_posteriors)\u001b[0m\n\u001b[0;32m    164\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapproximator\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlv\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprev_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprev_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlv\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m                     lv(self.approximator[lv], lv_conditions, gt_samples=gt_lv, complete=(lv in self.child) or complete,\n\u001b[1;32m--> 166\u001b[1;33m                        lens=this_len)\n\u001b[0m\u001b[0;32m    167\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrep_net\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m                         \u001b[0mlv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprev_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapproximator\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlv\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Experiments\\GLUE_BENCH\\components\\latent_variables.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, link_approximator, inputs, prior, gt_samples, complete, lens)\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sequential_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlink_approximator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgt_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlink_approximator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgt_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomplete\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcomplete\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Experiments\\GLUE_BENCH\\components\\latent_variables.py\u001b[0m in \u001b[0;36m_forward\u001b[1;34m(self, link_approximator, inputs, prior, gt_samples, complete, lens)\u001b[0m\n\u001b[0;32m    268\u001b[0m                       torch.cat([v for k, v in inputs.items() if k not in link_approximator.residual['conditions']],\n\u001b[0;32m    269\u001b[0m                                 dim=-1))\n\u001b[1;32m--> 270\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpost_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlink_approximator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    271\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcomplete\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpost_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpost_log_probas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposterior_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpost_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Experiments\\GLUE_BENCH\\components\\links.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, z_prev, lens)\u001b[0m\n\u001b[0;32m    753\u001b[0m                 \u001b[0mz_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'logits'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msbn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'logits'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 755\u001b[1;33m                 \u001b[0mz_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'logits'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'logits'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    756\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'loc'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mz_params\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatchnorm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m             \u001b[0mout_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mz_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 6.00 GiB total capacity; 497.50 MiB already allocated; 3.93 GiB free; 544.00 MiB reserved in total by PyTorch)"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "df = _get_stat_data_frame(model, n_samples=2000, n_alterations=1, batch_size=100)\n",
    "model.infer_bn.clear_values()\n",
    "model.gen_bn.clear_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\nDisentanglement score 0.4224999999999999\nDisentanglement score 0.39999999999999997\nDisentanglement score 0.3625000000000001\nDisentanglement score 0.4574999999999999\nDisentanglement score 0.43500000000000005\n0.4154999999999999 0.0323805497173843\n               subj_diff  verb_diff  dobj_diff  pobj_diff\nalteration_id                                            \n0                 0.4305     0.8855     0.1885     0.2410\n1                 0.4125     0.6960     0.2260     0.3030\n2                 0.4045     0.6700     0.2180     0.2705\n3                 0.6635     0.7305     0.2045     0.2800\nDisentanglement score 0.4189999999999999\nNumber of fixed structure pairs: 4558\n               subj_diff  verb_diff  dobj_diff  pobj_diff\nalteration_id                                            \n0               0.436563   0.830170   0.331668   0.402597\n1               0.407807   0.584718   0.335548   0.438538\n2               0.415776   0.552999   0.314708   0.394412\n3               0.666373   0.607394   0.321303   0.428697\nsubj_diff 3 0.2298098028731832\nverb_diff 0 0.22277546397264703\ndobj_diff 1 0.00387984108914341\npobj_diff 1 0.00984102288147487\nDisentanglement score 0.4663061308164485\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "scores = []\n",
    "div_by = 5\n",
    "for i in range(div_by):\n",
    "    grouped_diff = df[int(i*len(df)/div_by):int((i+1)*len(df)/div_by)].groupby('alteration_id').mean()\n",
    "    [['subj_diff', 'verb_diff', 'dobj_diff', 'pobj_diff']]\n",
    "    # print(grouped_diff)#\n",
    "    disent_score = 0\n",
    "    for lab in ['subj_diff', 'verb_diff', 'dobj_diff', 'pobj_diff']:\n",
    "        top2 = np.array(grouped_diff.nlargest(2, lab)[lab])\n",
    "        diff = top2[0]-top2[1]\n",
    "        # print(lab, diff)\n",
    "        disent_score += diff\n",
    "    print(\"Disentanglement score\", disent_score)\n",
    "    scores.append(disent_score)\n",
    "print(np.mean(scores), np.std(scores))\n",
    "grouped_diff = df.groupby('alteration_id').mean()[['subj_diff', 'verb_diff', 'dobj_diff', 'pobj_diff']]\n",
    "print(grouped_diff)#\n",
    "disent_score = 0\n",
    "for lab in ['subj_diff', 'verb_diff', 'dobj_diff', 'pobj_diff']:\n",
    "    top2 = np.array(grouped_diff.nlargest(2, lab)[lab])\n",
    "    diff = top2[0]-top2[1]\n",
    "    # print(lab, diff)\n",
    "    disent_score += diff\n",
    "print(\"Disentanglement score\", disent_score)\n",
    "\n",
    "df_fix = df[df['same_struct']==True]\n",
    "print(\"Number of fixed structure pairs:\", len(df_fix))\n",
    "grouped_diff = df_fix.groupby('alteration_id').mean()[['subj_diff', 'verb_diff', 'dobj_diff', 'pobj_diff']]\n",
    "print(grouped_diff)#\n",
    "disent_score = 0\n",
    "for lab in ['subj_diff', 'verb_diff', 'dobj_diff', 'pobj_diff']:#, 'arg_star_diff']:\n",
    "    highest_idx = grouped_diff[lab].argmax()\n",
    "    top2 = np.array(grouped_diff.nlargest(2, lab)[lab])\n",
    "    diff = top2[0]-top2[1]\n",
    "    print(lab, highest_idx, diff)\n",
    "    disent_score += diff\n",
    "    #grouped_diff = grouped_diff.drop(highest_idx)\n",
    "print(\"Disentanglement score\", disent_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 4, 2, 18)\n"
     ]
    }
   ],
   "source": [
    "from components.links import CoattentiveTransformerLink, ConditionalCoattentiveTransformerLink\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def get_attention_weights(sentences, lvs):\n",
    "    # Encoding sentences\n",
    "    encoded = []\n",
    "    lens = []\n",
    "    for sen in sentences:\n",
    "        sen_enc = [data.vocab.stoi[w] for w in sen.split(' ')]\n",
    "        lens.append(min(len(sen_enc), MAX_LEN))\n",
    "        if len(sen_enc)>=MAX_LEN:\n",
    "            sen_enc = sen_enc[:MAX_LEN]\n",
    "        else:\n",
    "            sen_enc = sen_enc+[data.vocab.stoi['<pad>']]*(MAX_LEN-len(sen_enc))\n",
    "        encoded.append(sen_enc)\n",
    "    encoded = torch.Tensor(encoded).to(DEVICE).long()\n",
    "    lens = torch.Tensor(lens).to(DEVICE).long()\n",
    "    CoattentiveTransformerLink.get_att, ConditionalCoattentiveTransformerLink.get_att = True, True\n",
    "    model.infer_bn({'x': encoded}, lens=lens)\n",
    "    CoattentiveTransformerLink.get_att, ConditionalCoattentiveTransformerLink.get_att = False, False\n",
    "    all_att_weights = []\n",
    "    for i in range(len(h_params.n_latents)):\n",
    "        trans_mod = model.infer_bn.approximator[model.infer_bn.name_to_v['z{}'.format(i+1)]]\n",
    "        all_att_weights.append(trans_mod.att_vals)\n",
    "    att_weights = []\n",
    "    for lv in lvs:\n",
    "        var_att_weights = []\n",
    "        lv_layer = sum([lv > sum(h_params.n_latents[:i+1]) for i in range(len(h_params.n_latents))])\n",
    "        rank = lv - sum(h_params.n_latents[:lv_layer])\n",
    "        for layer_att_vals in all_att_weights[lv_layer]:\n",
    "            soft_att_vals = layer_att_vals\n",
    "            att_out = torch.cat([soft_att_vals[:, rank, :MAX_LEN], soft_att_vals[:, rank, MAX_LEN:].sum(-1).unsqueeze(-1)]\n",
    "                                , -1)\n",
    "            if lv_layer==2:\n",
    "                att_out[..., -1] *= 0\n",
    "            var_att_weights.append(att_out.cpu().detach().numpy())\n",
    "        att_weights.append(var_att_weights)\n",
    "    return np.transpose(np.array(att_weights), (2, 0, 1, 3))\n",
    "\n",
    "def display_attention(sentence, att_weights, variables):\n",
    "    toked = sentence.split(' ')\n",
    "    toked += ['<pad>']*(MAX_LEN-len(toked))+['<latent>']\n",
    "    index = []\n",
    "    for i in range(len(variables)):\n",
    "        for j in range(len(att_weights[i])):\n",
    "            index.append(str(variables[i])+'_'+str(j))\n",
    "    att_weights = att_weights.reshape(-1, MAX_LEN+1)\n",
    "    data = pd.DataFrame(att_weights#[:, :len(toked)].reshape((len(att_weights), len(toked)))\n",
    "                        , columns=toked, index=index)\n",
    "    sns_plot = plt.figure(figsize=(15, 2.8))\n",
    "    ax = plt.axes()\n",
    "    g = sns.heatmap(data, annot=True, yticklabels=True, ax=ax)\n",
    "    ax.set_title('')\n",
    "    for tick in g.get_xticklabels():\n",
    "        tick.set_color('black')\n",
    "    for tick in g.get_yticklabels():\n",
    "        tick.set_color('black')\n",
    "    g.set_ylim([0, len(index)])\n",
    "    #g.set_xticklabels(data.axes[1], rotation=55, ha=\"center\", labelcolor='white')\n",
    "    # g.get_figure()\n",
    "    # plt.show()\n",
    "    return sns_plot\n",
    "\n",
    "\n",
    "sentences = ['a man is standing .',\n",
    "             'two little girls are standing .',\n",
    "             \"a girl is holding a toy\", \n",
    "              \"a girl with a white hat is holding a toy .\", \n",
    "              \"a girl is playing in a street\", \"a girl with a black coat is playing in a street .\", \n",
    "             \"a girl and a boy are playing in the street .\",\n",
    "             'a group of people are sitting around a table .']\n",
    "vars = [0, 1, 2, 3]\n",
    "att_w = get_attention_weights(sentences, vars)\n",
    "print(att_w.shape)\n",
    "\n",
    "# for sen_idx in range(len(sentences)):\n",
    "#     sns_plot = display_attention(sentences[sen_idx], att_w[sen_idx], vars)\n",
    "#     sns_plot.savefig(\"att_{}.png\".format(sen_idx)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_att_and_rel_idx(text_in):\n",
    "    max_len = text_in.shape[-1]\n",
    "    text_sents = [' '.join([model.index[model.generated_v].itos[w] \n",
    "                            for w in s]).replace(' <pad>', '').replace(' <eos>', '')\n",
    "                  for s in text_in]\n",
    "    # Getting relations' positions\n",
    "    rel_idx = [out['idx'] for out in shallow_dependencies(text_sents)]\n",
    "    # Getting layer wise attention values\n",
    "    \n",
    "    CoattentiveTransformerLink.get_att, ConditionalCoattentiveTransformerLink.get_att = True, True\n",
    "    model.infer_bn({'x': text_in})\n",
    "    CoattentiveTransformerLink.get_att, ConditionalCoattentiveTransformerLink.get_att = False, False\n",
    "    all_att_weights = []\n",
    "    for i in range(len(h_params.n_latents)):\n",
    "        trans_mod = model.infer_bn.approximator[model.infer_bn.name_to_v['z{}'.format(i+1)]]\n",
    "        all_att_weights.append(trans_mod.att_vals)\n",
    "    att_weights = []\n",
    "    for lv in range(sum(model.h_params.n_latents)):\n",
    "        var_att_weights = []\n",
    "        lv_layer = sum([lv > sum(h_params.n_latents[:i+1]) for i in range(len(h_params.n_latents))])\n",
    "        rank = lv - sum(h_params.n_latents[:lv_layer])\n",
    "        for layer_att_vals in all_att_weights[lv_layer]:\n",
    "            soft_att_vals = layer_att_vals\n",
    "            att_out = torch.cat([soft_att_vals[:, rank,\n",
    "                                 :max_len], soft_att_vals[:, rank, max_len:].sum(-1).unsqueeze(-1)]\n",
    "                                , -1)\n",
    "            if lv_layer==2:\n",
    "                att_out[..., -1] *= 0\n",
    "            var_att_weights.append(att_out.cpu().detach().numpy())\n",
    "        att_weights.append(var_att_weights)\n",
    "    # att_vals shape:[sent, lv, layer, tok]\n",
    "    att_vals = np.transpose(np.array(att_weights), (2, 0, 1, 3)).mean(-2)\n",
    "    att_maxes = att_vals.argmax(-1).tolist()\n",
    "    return rel_idx, att_maxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   0%|          | 0/334 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   1%|          | 3/334 [00:00<00:15, 21.96it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   2%|▏         | 6/334 [00:00<00:14, 22.86it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   3%|▎         | 9/334 [00:00<00:15, 21.65it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   3%|▎         | 11/334 [00:00<00:16, 19.83it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   4%|▍         | 13/334 [00:00<00:17, 18.83it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   4%|▍         | 15/334 [00:00<00:17, 18.34it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   5%|▌         | 17/334 [00:00<00:17, 17.63it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   6%|▌         | 20/334 [00:01<00:17, 18.16it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   7%|▋         | 22/334 [00:01<00:16, 18.59it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   7%|▋         | 25/334 [00:01<00:15, 19.79it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   8%|▊         | 28/334 [00:01<00:14, 20.60it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   9%|▉         | 31/334 [00:01<00:14, 20.65it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  10%|█         | 34/334 [00:01<00:14, 20.38it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  11%|█         | 37/334 [00:01<00:15, 19.12it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  12%|█▏        | 39/334 [00:01<00:15, 19.01it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  12%|█▏        | 41/334 [00:02<00:15, 18.77it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  13%|█▎        | 43/334 [00:02<00:15, 19.08it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  13%|█▎        | 45/334 [00:02<00:15, 19.20it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  14%|█▍        | 47/334 [00:02<00:14, 19.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  15%|█▍        | 49/334 [00:02<00:14, 19.19it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  15%|█▌        | 51/334 [00:02<00:15, 18.63it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  16%|█▌        | 53/334 [00:02<00:15, 17.68it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  16%|█▋        | 55/334 [00:02<00:16, 17.20it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  17%|█▋        | 57/334 [00:02<00:16, 17.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  18%|█▊        | 59/334 [00:03<00:15, 17.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  18%|█▊        | 61/334 [00:03<00:15, 17.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  19%|█▉        | 63/334 [00:03<00:15, 16.98it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  19%|█▉        | 65/334 [00:03<00:15, 17.21it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  20%|██        | 67/334 [00:03<00:16, 16.59it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  21%|██        | 69/334 [00:03<00:15, 16.96it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  21%|██▏       | 71/334 [00:03<00:15, 16.93it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  22%|██▏       | 74/334 [00:03<00:14, 17.85it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  23%|██▎       | 76/334 [00:04<00:14, 18.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  24%|██▎       | 79/334 [00:04<00:13, 18.95it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  24%|██▍       | 81/334 [00:04<00:13, 18.88it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  25%|██▌       | 84/334 [00:04<00:13, 19.22it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  26%|██▌       | 86/334 [00:04<00:13, 19.02it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  26%|██▋       | 88/334 [00:04<00:12, 19.21it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  27%|██▋       | 90/334 [00:04<00:13, 18.43it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  28%|██▊       | 92/334 [00:04<00:13, 18.17it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  28%|██▊       | 94/334 [00:05<00:13, 18.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  29%|██▉       | 97/334 [00:05<00:12, 18.86it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  30%|██▉       | 100/334 [00:05<00:11, 19.85it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  31%|███       | 103/334 [00:05<00:11, 19.60it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  32%|███▏      | 106/334 [00:05<00:11, 20.21it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  33%|███▎      | 109/334 [00:05<00:10, 20.75it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  34%|███▎      | 112/334 [00:05<00:10, 21.14it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  34%|███▍      | 115/334 [00:06<00:10, 21.24it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  35%|███▌      | 118/334 [00:06<00:10, 21.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  36%|███▌      | 121/334 [00:06<00:09, 22.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  37%|███▋      | 124/334 [00:06<00:09, 22.98it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  38%|███▊      | 127/334 [00:06<00:08, 23.57it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  39%|███▉      | 130/334 [00:06<00:08, 24.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  40%|███▉      | 133/334 [00:06<00:08, 24.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  41%|████      | 136/334 [00:06<00:08, 24.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  42%|████▏     | 139/334 [00:06<00:07, 25.19it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  43%|████▎     | 142/334 [00:07<00:07, 25.28it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  43%|████▎     | 145/334 [00:07<00:07, 25.54it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  44%|████▍     | 148/334 [00:07<00:07, 25.79it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  45%|████▌     | 151/334 [00:07<00:07, 25.86it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  46%|████▌     | 154/334 [00:07<00:06, 26.08it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  47%|████▋     | 157/334 [00:07<00:06, 25.90it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  48%|████▊     | 160/334 [00:07<00:06, 25.98it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  49%|████▉     | 163/334 [00:07<00:06, 25.97it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  50%|████▉     | 166/334 [00:08<00:06, 26.29it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  51%|█████     | 169/334 [00:08<00:06, 26.19it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  51%|█████▏    | 172/334 [00:08<00:06, 25.89it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  52%|█████▏    | 175/334 [00:08<00:06, 25.51it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  53%|█████▎    | 178/334 [00:08<00:06, 25.57it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  54%|█████▍    | 181/334 [00:08<00:05, 25.87it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  55%|█████▌    | 184/334 [00:08<00:05, 25.89it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  56%|█████▌    | 187/334 [00:08<00:05, 25.44it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  57%|█████▋    | 190/334 [00:08<00:05, 25.08it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  58%|█████▊    | 193/334 [00:09<00:05, 25.07it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  59%|█████▊    | 196/334 [00:09<00:05, 25.45it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  60%|█████▉    | 199/334 [00:09<00:05, 25.46it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  60%|██████    | 202/334 [00:09<00:05, 26.00it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  61%|██████▏   | 205/334 [00:09<00:04, 26.25it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  62%|██████▏   | 208/334 [00:09<00:04, 26.49it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  63%|██████▎   | 211/334 [00:09<00:04, 26.60it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  64%|██████▍   | 214/334 [00:09<00:04, 26.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  65%|██████▍   | 217/334 [00:09<00:04, 26.60it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  66%|██████▌   | 220/334 [00:10<00:04, 26.75it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  67%|██████▋   | 223/334 [00:10<00:04, 26.71it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  68%|██████▊   | 226/334 [00:10<00:04, 26.61it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  69%|██████▊   | 229/334 [00:10<00:03, 26.47it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  69%|██████▉   | 232/334 [00:10<00:03, 26.31it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  70%|███████   | 235/334 [00:10<00:03, 26.26it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  71%|███████▏  | 238/334 [00:10<00:03, 26.44it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  72%|███████▏  | 241/334 [00:10<00:03, 26.15it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  73%|███████▎  | 244/334 [00:11<00:03, 26.22it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  74%|███████▍  | 247/334 [00:11<00:03, 26.55it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  75%|███████▍  | 250/334 [00:11<00:03, 26.43it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  76%|███████▌  | 253/334 [00:11<00:03, 26.21it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  77%|███████▋  | 256/334 [00:11<00:02, 26.54it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  78%|███████▊  | 259/334 [00:11<00:02, 26.29it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  78%|███████▊  | 262/334 [00:11<00:02, 25.98it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  79%|███████▉  | 265/334 [00:11<00:02, 25.96it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  80%|████████  | 268/334 [00:11<00:02, 25.56it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  81%|████████  | 271/334 [00:12<00:02, 25.73it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  82%|████████▏ | 274/334 [00:12<00:02, 25.93it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  83%|████████▎ | 277/334 [00:12<00:02, 26.00it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  84%|████████▍ | 280/334 [00:12<00:02, 25.84it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  85%|████████▍ | 283/334 [00:12<00:01, 26.07it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  86%|████████▌ | 286/334 [00:12<00:01, 26.32it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  87%|████████▋ | 289/334 [00:12<00:01, 26.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  87%|████████▋ | 292/334 [00:12<00:01, 26.26it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  88%|████████▊ | 295/334 [00:12<00:01, 26.23it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  89%|████████▉ | 298/334 [00:13<00:01, 26.35it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  90%|█████████ | 301/334 [00:13<00:01, 26.15it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  91%|█████████ | 304/334 [00:13<00:01, 26.22it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  92%|█████████▏| 307/334 [00:13<00:01, 26.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  93%|█████████▎| 310/334 [00:13<00:00, 26.47it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  94%|█████████▎| 313/334 [00:13<00:00, 26.59it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  95%|█████████▍| 316/334 [00:13<00:00, 26.60it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  96%|█████████▌| 319/334 [00:13<00:00, 26.46it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  96%|█████████▋| 322/334 [00:13<00:00, 26.58it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  97%|█████████▋| 325/334 [00:14<00:00, 26.59it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  98%|█████████▊| 328/334 [00:14<00:00, 26.25it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  99%|█████████▉| 331/334 [00:14<00:00, 26.02it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy: 100%|██████████| 334/334 [00:14<00:00, 27.09it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy: 100%|██████████| 334/334 [00:14<00:00, 23.16it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rel_idx, att_maxes = [], []\n",
    "for i, batch in enumerate(tqdm(data.val_iter, desc=\"Getting model relationship accuracy\")):\n",
    "    rel_idx_i, att_maxes_i = get_att_and_rel_idx(batch.text[..., 1:])\n",
    "    rel_idx.extend(rel_idx_i)\n",
    "    att_maxes.extend(att_maxes_i)\n",
    "    \n",
    "    \n",
    "data.reinit_iterator('valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   0%|          | 0/417 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   1%|          | 3/417 [00:00<00:17, 23.69it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   1%|▏         | 6/417 [00:00<00:16, 24.67it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   2%|▏         | 9/417 [00:00<00:16, 24.49it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   3%|▎         | 12/417 [00:00<00:16, 24.78it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   3%|▎         | 14/417 [00:00<00:18, 22.00it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   4%|▍         | 17/417 [00:00<00:17, 22.84it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   5%|▍         | 20/417 [00:00<00:16, 23.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   6%|▌         | 23/417 [00:00<00:16, 23.67it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   6%|▌         | 26/417 [00:01<00:16, 23.35it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   7%|▋         | 29/417 [00:01<00:16, 23.89it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   8%|▊         | 32/417 [00:01<00:15, 24.12it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   8%|▊         | 35/417 [00:01<00:15, 24.57it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   9%|▉         | 38/417 [00:01<00:15, 24.30it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  10%|▉         | 41/417 [00:01<00:16, 23.27it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  11%|█         | 44/417 [00:01<00:16, 23.07it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  11%|█▏        | 47/417 [00:01<00:15, 23.47it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  12%|█▏        | 50/417 [00:02<00:15, 24.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  13%|█▎        | 53/417 [00:02<00:15, 24.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  13%|█▎        | 56/417 [00:02<00:14, 24.29it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  14%|█▍        | 59/417 [00:02<00:14, 24.28it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  15%|█▍        | 62/417 [00:02<00:15, 23.64it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  16%|█▌        | 65/417 [00:02<00:14, 24.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  16%|█▋        | 68/417 [00:02<00:14, 23.33it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  17%|█▋        | 71/417 [00:02<00:15, 23.00it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  18%|█▊        | 74/417 [00:03<00:15, 21.70it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  18%|█▊        | 77/417 [00:03<00:16, 21.09it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  19%|█▉        | 80/417 [00:03<00:15, 21.30it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  20%|█▉        | 83/417 [00:03<00:15, 21.63it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  21%|██        | 86/417 [00:03<00:14, 22.21it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  21%|██▏       | 89/417 [00:03<00:15, 21.84it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  22%|██▏       | 92/417 [00:03<00:14, 22.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  23%|██▎       | 95/417 [00:04<00:14, 22.80it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  24%|██▎       | 98/417 [00:04<00:14, 22.24it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  24%|██▍       | 101/417 [00:04<00:14, 22.50it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  25%|██▍       | 104/417 [00:04<00:13, 23.21it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  26%|██▌       | 107/417 [00:04<00:12, 23.85it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  26%|██▋       | 110/417 [00:04<00:12, 24.68it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  27%|██▋       | 113/417 [00:04<00:12, 25.17it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  28%|██▊       | 116/417 [00:04<00:11, 25.46it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  29%|██▊       | 119/417 [00:05<00:11, 25.53it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  29%|██▉       | 122/417 [00:05<00:11, 25.20it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  30%|██▉       | 125/417 [00:05<00:11, 25.03it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  31%|███       | 128/417 [00:05<00:11, 25.30it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  31%|███▏      | 131/417 [00:05<00:11, 25.10it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  32%|███▏      | 134/417 [00:05<00:10, 25.73it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  33%|███▎      | 137/417 [00:05<00:10, 26.27it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  34%|███▎      | 140/417 [00:05<00:10, 26.30it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  34%|███▍      | 143/417 [00:05<00:10, 26.33it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  35%|███▌      | 146/417 [00:06<00:10, 26.70it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  36%|███▌      | 149/417 [00:06<00:09, 26.89it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  36%|███▋      | 152/417 [00:06<00:09, 26.81it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  37%|███▋      | 155/417 [00:06<00:09, 27.19it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  38%|███▊      | 158/417 [00:06<00:09, 27.46it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  39%|███▊      | 161/417 [00:06<00:09, 27.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  39%|███▉      | 164/417 [00:06<00:09, 27.63it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  40%|████      | 167/417 [00:06<00:09, 27.54it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  41%|████      | 170/417 [00:06<00:09, 27.18it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  41%|████▏     | 173/417 [00:07<00:08, 27.23it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  42%|████▏     | 176/417 [00:07<00:08, 27.27it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  43%|████▎     | 179/417 [00:07<00:08, 27.44it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  44%|████▎     | 182/417 [00:07<00:08, 27.71it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  44%|████▍     | 185/417 [00:07<00:08, 27.99it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  45%|████▌     | 188/417 [00:07<00:08, 28.18it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  46%|████▌     | 191/417 [00:07<00:08, 27.93it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  47%|████▋     | 194/417 [00:07<00:08, 27.52it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  47%|████▋     | 197/417 [00:07<00:07, 27.70it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  48%|████▊     | 200/417 [00:08<00:07, 27.82it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  49%|████▊     | 203/417 [00:08<00:07, 27.91it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  49%|████▉     | 206/417 [00:08<00:07, 28.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  50%|█████     | 209/417 [00:08<00:07, 28.07it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  51%|█████     | 212/417 [00:08<00:07, 28.00it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  52%|█████▏    | 215/417 [00:08<00:07, 27.65it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  52%|█████▏    | 218/417 [00:08<00:07, 27.63it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  53%|█████▎    | 221/417 [00:08<00:07, 27.17it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  54%|█████▎    | 224/417 [00:08<00:07, 27.45it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  54%|█████▍    | 227/417 [00:09<00:06, 27.80it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  55%|█████▌    | 230/417 [00:09<00:06, 27.81it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  56%|█████▌    | 233/417 [00:09<00:06, 28.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  57%|█████▋    | 236/417 [00:09<00:06, 27.83it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  57%|█████▋    | 239/417 [00:09<00:06, 27.91it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  58%|█████▊    | 242/417 [00:09<00:06, 28.13it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  59%|█████▉    | 245/417 [00:09<00:06, 28.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  59%|█████▉    | 248/417 [00:09<00:05, 28.22it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  60%|██████    | 251/417 [00:09<00:05, 28.02it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  61%|██████    | 254/417 [00:09<00:05, 28.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  62%|██████▏   | 257/417 [00:10<00:05, 27.84it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  62%|██████▏   | 260/417 [00:10<00:05, 27.76it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  63%|██████▎   | 263/417 [00:10<00:05, 27.95it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  64%|██████▍   | 266/417 [00:10<00:05, 27.84it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  65%|██████▍   | 269/417 [00:10<00:05, 28.08it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  65%|██████▌   | 272/417 [00:10<00:05, 27.93it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  66%|██████▌   | 275/417 [00:10<00:05, 28.07it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  67%|██████▋   | 278/417 [00:10<00:04, 28.24it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  67%|██████▋   | 281/417 [00:10<00:04, 27.96it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  68%|██████▊   | 284/417 [00:11<00:04, 27.93it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  69%|██████▉   | 287/417 [00:11<00:04, 27.91it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  70%|██████▉   | 290/417 [00:11<00:04, 27.97it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  70%|███████   | 293/417 [00:11<00:04, 27.46it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  71%|███████   | 296/417 [00:11<00:04, 26.98it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  72%|███████▏  | 299/417 [00:11<00:04, 26.73it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  72%|███████▏  | 302/417 [00:11<00:04, 26.98it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  73%|███████▎  | 305/417 [00:11<00:04, 27.46it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  74%|███████▍  | 308/417 [00:11<00:03, 27.43it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  75%|███████▍  | 311/417 [00:12<00:03, 27.33it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  75%|███████▌  | 314/417 [00:12<00:03, 27.18it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  76%|███████▌  | 317/417 [00:12<00:03, 27.23it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  77%|███████▋  | 320/417 [00:12<00:03, 27.34it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  77%|███████▋  | 323/417 [00:12<00:03, 27.57it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  78%|███████▊  | 326/417 [00:12<00:03, 27.73it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  79%|███████▉  | 329/417 [00:12<00:03, 27.84it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  80%|███████▉  | 332/417 [00:12<00:03, 27.92it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  80%|████████  | 335/417 [00:12<00:02, 27.90it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  81%|████████  | 338/417 [00:13<00:02, 27.81it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  82%|████████▏ | 341/417 [00:13<00:02, 27.67it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  82%|████████▏ | 344/417 [00:13<00:02, 27.65it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  83%|████████▎ | 347/417 [00:13<00:02, 27.79it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  84%|████████▍ | 350/417 [00:13<00:02, 27.88it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  85%|████████▍ | 353/417 [00:13<00:02, 26.83it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  85%|████████▌ | 356/417 [00:13<00:02, 27.13it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  86%|████████▌ | 359/417 [00:13<00:02, 27.16it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  87%|████████▋ | 362/417 [00:13<00:02, 27.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  88%|████████▊ | 365/417 [00:14<00:01, 27.43it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  88%|████████▊ | 368/417 [00:14<00:01, 27.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  89%|████████▉ | 371/417 [00:14<00:01, 27.62it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  90%|████████▉ | 374/417 [00:14<00:01, 26.44it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  90%|█████████ | 377/417 [00:14<00:01, 26.78it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  91%|█████████ | 380/417 [00:14<00:01, 27.02it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  92%|█████████▏| 383/417 [00:14<00:01, 26.76it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  93%|█████████▎| 386/417 [00:14<00:01, 26.72it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  93%|█████████▎| 389/417 [00:14<00:01, 26.27it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  94%|█████████▍| 392/417 [00:15<00:00, 26.51it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  95%|█████████▍| 395/417 [00:15<00:00, 26.83it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  95%|█████████▌| 398/417 [00:15<00:00, 26.84it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  96%|█████████▌| 401/417 [00:15<00:00, 27.13it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  97%|█████████▋| 404/417 [00:15<00:00, 26.90it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  98%|█████████▊| 407/417 [00:15<00:00, 27.26it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  98%|█████████▊| 410/417 [00:15<00:00, 27.58it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  99%|█████████▉| 413/417 [00:15<00:00, 27.21it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy: 100%|█████████▉| 416/417 [00:15<00:00, 27.55it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy: 100%|██████████| 417/417 [00:15<00:00, 26.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Final Model Scores =======\n{'subj': 0.8359802134860713, 'verb': 0.32040775534679194, 'dobj': 0.4634146341463415, 'pobj': 0.5984567901234568}\n{'subj': 0.6894038010934652, 'verb': 0.18149110533679794, 'dobj': 0.11022514071294559, 'pobj': 0.3253086419753086}\n{'subj': 2, 'verb': 1, 'dobj': 0, 'pobj': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_encoder_disentanglement_score(data_iter):\n",
    "    rel_idx, att_maxes = [], []\n",
    "    for i, batch in enumerate(tqdm(data_iter, desc=\"Getting model relationship accuracy\")):\n",
    "        rel_idx_i, att_maxes_i = get_att_and_rel_idx(batch.text[..., 1:])\n",
    "        rel_idx.extend(rel_idx_i)\n",
    "        att_maxes.extend(att_maxes_i)\n",
    "        \n",
    "    lv_scores = []\n",
    "    for lv in range(sum(h_params.n_latents)):\n",
    "        found = {'subj':[], 'verb':[], 'dobj':[], 'pobj':[]}\n",
    "        for att, rel_pos in zip(att_maxes, rel_idx):\n",
    "            for k in found.keys():\n",
    "                if len(rel_pos[k]):\n",
    "                    found[k].append(att[lv] in rel_pos[k])\n",
    "        lv_scores.append(found)\n",
    "    enc_att_scores = {'subj':[], 'verb':[], 'dobj':[], 'pobj':[]}\n",
    "    for lv in range(sum(h_params.n_latents)):\n",
    "        for k, v in lv_scores[lv].items():\n",
    "            enc_att_scores[k].append(np.mean(v))\n",
    "    print(\"==== Final Model Scores =======\")\n",
    "    enc_max_score, enc_disent_score, enc_disent_vars = {}, {}, {}\n",
    "    for k, v in enc_att_scores.items():\n",
    "        sort_idx = np.argsort(v) \n",
    "        enc_disent_vars[k], enc_disent_score[k], enc_max_score[k] =\\\n",
    "            sort_idx[-1], v[sort_idx[-1]] - v[sort_idx[-2]], v[sort_idx[-1]]\n",
    "    print(enc_max_score)\n",
    "    print(enc_disent_score)\n",
    "    print(enc_disent_vars)\n",
    "    return enc_att_scores, enc_max_score, enc_disent_score, enc_disent_vars\n",
    "    # # ================= BASELINE ================= \n",
    "    # baseline = []\n",
    "    # for k in ['subj', 'verb', 'dobj', 'pobj']:\n",
    "    #     all_pos = []\n",
    "    #     for pos in rel_idx:\n",
    "    #         all_pos.extend(pos[k])\n",
    "    #     baseline.append(np.median(all_pos))\n",
    "    # baseline_scores = []\n",
    "    # for lv in range(4):\n",
    "    #     found = {'subj':[], 'verb':[], 'dobj':[], 'pobj':[]}\n",
    "    #     for rel_pos in rel_idx:\n",
    "    #         for k in found.keys():\n",
    "    #             if len(rel_pos[k]):\n",
    "    #                 found[k].append(baseline[lv] in rel_pos[k])\n",
    "    #     baseline_scores.append(found)\n",
    "    # \n",
    "    # baseline_enc_att_scores = {'subj':[], 'verb':[], 'dobj':[], 'pobj':[]}\n",
    "    # for lv in range(sum(h_params.n_latents)):\n",
    "    #     for k, v in baseline_scores[lv].items():\n",
    "    #         baseline_enc_att_scores[k].append(np.mean(v))\n",
    "    # print(\"==== Final baseline Scores =======\")\n",
    "    # enc_max_score, enc_disent_score, enc_disent_vars = {}, {}, {}\n",
    "    # for k, v in baseline_enc_att_scores.items():\n",
    "    #     sort_idx = np.argsort(v) \n",
    "    #     enc_disent_vars[k], enc_disent_score[k], enc_max_score[k] =\\\n",
    "    #         sort_idx[-1], v[sort_idx[-1]] - v[sort_idx[-2]], v[sort_idx[-1]]\n",
    "    # print(enc_max_score)\n",
    "    # print(enc_disent_score)\n",
    "    # print(enc_disent_vars)\n",
    "\n",
    "def show_df_hm2(df):\n",
    "    snsplt = sns.heatmap(df, cmap ='Reds', linewidths = 0.20, annot = False)\n",
    "    b, t = plt.ylim() # discover the values for bottom and top\n",
    "    b += 0.5 # Add 0.5 to the bottom\n",
    "    t -= 0.5 # Subtract 0.5 from the top\n",
    "    plt.ylim(b, t) # update the ylim(bottom, top) values\n",
    "    return snsplt.get_figure()\n",
    "enc_att, enc_max, enc_disent, enc_disent_idx = \\\n",
    "    get_encoder_disentanglement_score(data.val_iter)\n",
    "data.reinit_iterator('valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       subj      verb      dobj      pobj\n0  0.036709  0.058165  0.463415  0.598457\n1  0.146576  0.320408  0.353189  0.244753\n2  0.835980  0.138917  0.000938  0.001852\n3  0.004686  0.004997  0.182458  0.273148\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def show_df_hm2(df):\n",
    "    snsplt = sns.heatmap(df, cmap ='Reds', linewidths = 0.20, annot = True)\n",
    "    b, t = plt.ylim() # discover the values for bottom and top\n",
    "    b += 0.5 # Add 0.5 to the bottom\n",
    "    t -= 0.5 # Subtract 0.5 from the top\n",
    "    plt.ylim(b, t) # update the ylim(bottom, top) values\n",
    "    return snsplt.get_figure()\n",
    "att_df = pd.DataFrame(enc_att)\n",
    "print(att_df)\n",
    "sns_plot = show_df_hm2(att_df)\n",
    "sns_plot.savefig(\"enc_att.eps\", dpi=100, format='eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 2.0, 5.0, 7.0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# for i, batch in enumerate(data.val_iter):\n",
    "#     text_sents = [' '.join([model.index[model.generated_v].itos[w] \n",
    "#                             for w in s]).replace(' <pad>', '').replace(' <eos>', '')\n",
    "#                   for s in batch.text[:, 1:]]\n",
    "#     # Getting relations' positions\n",
    "#     rel_idx = shallow_dependencies(text_sents)\n",
    "#     for r, t in zip(rel_idx, text_sents):\n",
    "#         # if r['idx']['verb'][0] > 10:\n",
    "#         print(r, t)\n",
    "#     if i>10: break\n",
    "lv_scores = []\n",
    "for lv in range(sum(h_params.n_latents)):\n",
    "    found = {'subj':[], 'verb':[], 'dobj':[], 'pobj':[]}\n",
    "    for att, rel_pos in zip(att_maxes, rel_idx):\n",
    "        for k in found.keys():\n",
    "            if len(rel_pos[k]):\n",
    "                found[k].append(att[lv] in rel_pos[k])\n",
    "    lv_scores.append(found)\n",
    "\n",
    "baseline = []\n",
    "for k in ['subj', 'verb', 'dobj', 'pobj']:\n",
    "    all_pos = []\n",
    "    for pos in rel_idx:\n",
    "        all_pos.extend(pos[k])\n",
    "    baseline.append(np.median(all_pos))\n",
    "print(baseline)\n",
    "\n",
    "baseline_scores = []\n",
    "for lv in range(4):\n",
    "    found = {'subj':[], 'verb':[], 'dobj':[], 'pobj':[]}\n",
    "    for rel_pos in rel_idx:\n",
    "        for k in found.keys():\n",
    "            if len(rel_pos[k]):\n",
    "                found[k].append(baseline[lv] in rel_pos[k])\n",
    "    baseline_scores.append(found)\n",
    "            \n",
    "\n",
    "# found = {'subj':[], 'verb':[], 'dobj':[], 'pobj':[]}\n",
    "# for att, rel_pos in zip(att_maxes, rel_idx):\n",
    "#     for k in found.keys():\n",
    "#         if len(rel_pos[k]):\n",
    "#             found[k].append(att[lv][layer] in rel_pos[k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========our Model ========\nScores for lv 0, layer 0:\n{'subj': 0.38920863309352516, 'verb': 0.27425, 'dobj': 0.10831426392067124, 'pobj': 0.061317183951551855}\nScores for lv 1, layer 0:\n{'subj': 0.11726618705035971, 'verb': 0.12575, 'dobj': 0.38367658276125094, 'pobj': 0.23164269492808479}\nScores for lv 2, layer 0:\n{'subj': 0.0, 'verb': 0.0, 'dobj': 0.006864988558352402, 'pobj': 0.008327024981074944}\nScores for lv 3, layer 0:\n{'subj': 0.05, 'verb': 0.0335, 'dobj': 0.3531655225019069, 'pobj': 0.36260408781226344}\nScores for lv 4, layer 0:\n{'subj': 0.002158273381294964, 'verb': 0.00375, 'dobj': 0.10907704042715484, 'pobj': 0.16654049962149886}\nScores for lv 5, layer 0:\n{'subj': 0.02194244604316547, 'verb': 0.03, 'dobj': 0.2387490465293669, 'pobj': 0.33459500378501134}\nScores for lv 6, layer 0:\n{'subj': 0.007553956834532374, 'verb': 0.0145, 'dobj': 0.18916857360793288, 'pobj': 0.24072672218016655}\nScores for lv 7, layer 0:\n{'subj': 0.7607913669064749, 'verb': 0.09475, 'dobj': 0.004576659038901602, 'pobj': 0.0}\n==== Final Scores =======\n{'subj': 0.7607913669064749, 'verb': 0.27425, 'dobj': 0.38367658276125094, 'pobj': 0.36260408781226344}\n{'subj': 0.3715827338129497, 'verb': 0.1485, 'dobj': 0.030511060259344025, 'pobj': 0.028009084027252107}\n{'subj': 7, 'verb': 0, 'dobj': 1, 'pobj': 3}\n========our baseline ========\nScores for lv 0, layer 0:\n{'subj': 0.4068345323741007, 'verb': 0.31575, 'dobj': 0.09153318077803203, 'pobj': 0.03557910673732021}\nScores for lv 1, layer 0:\n{'subj': 0.19784172661870503, 'verb': 0.24375, 'dobj': 0.2959572845156369, 'pobj': 0.10749432248296745}\nScores for lv 2, layer 0:\n{'subj': 0.06510791366906475, 'verb': 0.03725, 'dobj': 0.34553775743707094, 'pobj': 0.32248296744890237}\nScores for lv 3, layer 0:\n{'subj': 0.024820143884892086, 'verb': 0.02775, 'dobj': 0.2372234935163997, 'pobj': 0.3209689629068887}\n==== Final Scores =======\n{'subj': 0.4068345323741007, 'verb': 0.31575, 'dobj': 0.34553775743707094, 'pobj': 0.32248296744890237}\n{'subj': 0.2089928057553957, 'verb': 0.07199999999999998, 'dobj': 0.04958047292143403, 'pobj': 0.0015140045420136694} 0.33208728321884334\n{'subj': 0, 'verb': 0, 'dobj': 2, 'pobj': 2}\n"
     ]
    }
   ],
   "source": [
    "print(\"========our Model ========\")\n",
    "Enc_att_scores = {'subj':[], 'verb':[], 'dobj':[], 'pobj':[]}\n",
    "for lv in range(sum(h_params.n_latents)):\n",
    "    for layer in range(1):#h_params.encoder_l):\n",
    "        print(\"Scores for lv {}, layer {}:\".format(lv, layer))\n",
    "        print({k:np.mean(v)for k, v in lv_scores[lv].items()})\n",
    "        for k, v in lv_scores[lv].items():\n",
    "            Enc_att_scores[k].append(np.mean(v))\n",
    "print(\"==== Final Scores =======\")\n",
    "enc_max_score = {}\n",
    "enc_disent_score = {}\n",
    "enc_disent_vars = {}\n",
    "for k, v in Enc_att_scores.items():\n",
    "    sort_idx = np.argsort(v) \n",
    "    enc_disent_vars[k] = sort_idx[-1]\n",
    "    enc_disent_score[k] = v[sort_idx[-1]] - v[sort_idx[-2]]\n",
    "    enc_max_score[k] = v[sort_idx[-1]]\n",
    "print(enc_max_score)\n",
    "print(enc_disent_score)\n",
    "print(enc_disent_vars)\n",
    "print(\"========our baseline ========\")\n",
    "\n",
    "baseline_enc_att_scores = {'subj':[], 'verb':[], 'dobj':[], 'pobj':[]}\n",
    "for lv in range(len(baseline_scores)):\n",
    "    for layer in range(1):#h_params.encoder_l):\n",
    "        print(\"Scores for lv {}, layer {}:\".format(lv, layer))\n",
    "        print({k:np.mean(v)for k, v in baseline_scores[lv].items()})\n",
    "        for k, v in baseline_scores[lv].items():\n",
    "            baseline_enc_att_scores[k].append(np.mean(v))\n",
    "print(\"==== Final Scores =======\")\n",
    "enc_max_score = {}\n",
    "enc_disent_score = {}\n",
    "enc_disent_vars = {}\n",
    "for k, v in baseline_enc_att_scores.items():\n",
    "    sort_idx = np.argsort(v) \n",
    "    enc_disent_vars[k] = sort_idx[-1]\n",
    "    enc_disent_score[k] = v[sort_idx[-1]] - v[sort_idx[-2]]\n",
    "    enc_max_score[k] = v[sort_idx[-1]]\n",
    "print(enc_max_score)\n",
    "print(enc_disent_score, sum(enc_disent_score.values()))\n",
    "print(enc_disent_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 'll never be going back\n(\"i 'll never be going back\", 'nsubj aux neg aux ROOT advmod')\ngreat food, great service\n('great food , service', 'amod ROOT punct appos')\ndecoration is outdated .\n('decoration is outdated .', 'nsubjpass auxpass ROOT punct')\nthe guy in front of us was ordering in spanish .\n('guy was ordering in .', 'nsubj aux ROOT prep punct')\nworst service ever .\n('worst service ever .', 'amod ROOT advmod punct')\nthe worst experience i have ever had at an enterprise location .\n('the worst experience had .', 'det amod ROOT relcl punct')\nwaitresses are slow\n('waitresses are slow', 'nsubj ROOT acomp')\nthis place is awful !\n('place is awful !', 'nsubj ROOT acomp punct')\n"
     ]
    }
   ],
   "source": [
    "ex_sents = [\"i 'll never be going back\", \"great food, great service\",\n",
    "            \"decoration is outdated .\",\n",
    "            \"the guy in front of us was ordering in spanish .\",\n",
    "            \"worst service ever .\", \"the worst experience i have ever had at an enterprise location .\",\n",
    "            'waitresses are slow', \"this place is awful !\"]\n",
    "import itertools\n",
    "def get_children(tok, depth):\n",
    "    if depth == 0:\n",
    "        return list(tok.children)\n",
    "    else:\n",
    "        return list(tok.children) + \\\n",
    "               list(itertools.chain.from_iterable([get_children(c, depth-1) for c in tok.children]))\n",
    "\n",
    "def truncated_template(sents, depth):\n",
    "    docs = nlp.pipe(sents)\n",
    "    templates = []\n",
    "    for doc in docs:\n",
    "        children = None\n",
    "        for i, tok in enumerate(doc):\n",
    "            if tok.dep_ =='ROOT':\n",
    "                children = [tok]+get_children(tok, depth)\n",
    "        if children is not None:\n",
    "            sort_dict_lex = {c.i:c.text for c in children}\n",
    "            sort_dict_syn = {c.i:c.dep_ for c in children}\n",
    "            templates.append({'lex': ' '.join([sort_dict_lex[i] for i in sorted(sort_dict_lex.keys())]),\n",
    "                              'syn': ' '.join([sort_dict_syn[i] for i in sorted(sort_dict_syn.keys())])})\n",
    "        else: \n",
    "            raise NotImplementedError(\"This sentence has no ROOT: \".format(' '.join([t.text for t in doc])))\n",
    "\n",
    "    \n",
    "docs = nlp.pipe(ex_sents)\n",
    "for doc in docs:\n",
    "    print(doc.text)\n",
    "    print(truncated_template(doc, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'det'), ('construction', 'compound'), ('workers', 'nsubj'), ('are', 'aux'), ('using', 'ROOT'), ('a', 'det'), ('pick', 'amod'), ('axes', 'dobj'), ('to', 'aux'), ('poke', 'xcomp'), ('each', 'det'), ('other', 'dobj'), ('.', 'punct')]\n[{'text': {'subj': 'it', 'verb': 'trade', 'dobj': '', 'pobj': 'the counter'}, 'idx': {'subj': [0], 'verb': [2], 'dobj': [], 'pobj': [4, 5]}}, {'text': {'subj': '', 'verb': '', 'dobj': '', 'pobj': ''}, 'idx': {'subj': [], 'verb': [], 'dobj': [], 'pobj': []}}, {'text': {'subj': 'it', 'verb': '', 'dobj': '', 'pobj': ''}, 'idx': {'subj': [5], 'verb': [], 'dobj': [], 'pobj': []}}, {'text': {'subj': 'the scroll', 'verb': '', 'dobj': 'writing on both sides', 'pobj': 'both sides'}, 'idx': {'subj': [0, 1], 'verb': [], 'dobj': [3, 4, 5, 6], 'pobj': [5, 6]}}, {'text': {'subj': '', 'verb': 'try', 'dobj': 'it', 'pobj': ''}, 'idx': {'subj': [], 'verb': [0], 'dobj': [4], 'pobj': []}}, {'text': {'subj': '', 'verb': 'pulled', 'dobj': '', 'pobj': 'the < unk > of an apartment building'}, 'idx': {'subj': [], 'verb': [6], 'dobj': [], 'pobj': [8, 9, 10, 11, 12, 13, 14, 15]}}, {'text': {'subj': 'that', 'verb': '', 'dobj': 'what', 'pobj': ''}, 'idx': {'subj': [0], 'verb': [], 'dobj': [2], 'pobj': []}}, {'text': {'subj': 'it', 'verb': 'matches', 'dobj': 'her voice', 'pobj': ''}, 'idx': {'subj': [1], 'verb': [2], 'dobj': [3, 4], 'pobj': []}}, {'text': {'subj': 'the man who was healed', 'verb': '', 'dobj': '', 'pobj': ''}, 'idx': {'subj': [0, 1, 2, 3, 4], 'verb': [], 'dobj': [], 'pobj': []}}, {'text': {'subj': 'you', 'verb': 'come', 'dobj': '', 'pobj': 'ten'}, 'idx': {'subj': [0], 'verb': [1], 'dobj': [], 'pobj': [4]}}, {'text': {'subj': 'he', 'verb': 'shouted', 'dobj': 'me', 'pobj': ''}, 'idx': {'subj': [0], 'verb': [1], 'dobj': [8], 'pobj': []}}, {'text': {'subj': '', 'verb': '', 'dobj': '', 'pobj': 'this < unk'}, 'idx': {'subj': [], 'verb': [], 'dobj': [], 'pobj': [7, 8, 9]}}]\nOntonotes &  it will trade over the counter under the symbol cray . &  it& trade& & the counter &  nsubj aux ROOT prep prep punct& it will trade over under . \\\\ \\hline\nOntonotes &  -- &  & & &  &  ROOT& -- \\\\ \\hline\nOntonotes &  there is a risk that it could become sort of a secondary brand . &  it& & &  &  expl ROOT attr punct& there is risk . \\\\ \\hline\nOntonotes &  the scroll had writing on both sides and was kept closed with seven seals &  the scroll& & writing on both sides& both sides &  nsubj ROOT dobj cc conj& scroll had writing and kept \\\\ \\hline\nOntonotes &  try hard to enter it . &  & try& it&  &  ROOT advmod xcomp punct& try hard enter . \\\\ \\hline\nOntonotes &  a woman and child were also pulled from the <unk> of an apartment building &  & pulled& & the < unk > of an apartment building &  nsubjpass auxpass advmod ROOT prep& woman were also pulled from \\\\ \\hline\nOntonotes &  that was what they said . &  that& & what&  &  nsubj ROOT ccomp punct& that was said . \\\\ \\hline\nOntonotes &  -lrb- it matches her voice . -rrb- &  it& matches& her voice&  &  npadvmod nsubj ROOT dobj punct punct& -lrb- it matches voice . -rrb- \\\\ \\hline\nOntonotes &  the man who was healed was more than 40 years old . &  the man who was healed& & &  &  nsubj ROOT acomp punct& man was old . \\\\ \\hline\nOntonotes &  you come in at ten , &  you& come& & ten &  nsubj ROOT prt prep punct& you come in at , \\\\ \\hline\nOntonotes &  he shouted , `` lord , save me ! '' &  he& shouted& me&  &  nsubj ROOT punct punct xcomp punct punct& he shouted , ` save ! '' \\\\ \\hline\nOntonotes &  there are at least three defects in this <unk> of yours . &  & & & this < unk &  expl ROOT attr punct punct& there are defects > . \\\\ \\hline\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch import device\n",
    "from data_prep import OntoGenData as Data\n",
    "import spacy\n",
    "from disentanglement_transformer.models import shallow_dependencies, truncated_template\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "DEVICE = device(\"cuda:0\")\n",
    "data = Data(16, 128, 100, DEVICE, pretrained=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'det'), ('construction', 'compound'), ('workers', 'nsubj'), ('are', 'aux'), ('using', 'ROOT'), ('a', 'det'), ('pick', 'amod'), ('axes', 'dobj'), ('to', 'aux'), ('poke', 'xcomp'), ('each', 'det'), ('other', 'dobj'), ('.', 'punct')]\n[{'text': {'subj': 'it', 'verb': 'trade', 'dobj': '', 'pobj': 'the counter'}, 'idx': {'subj': [0], 'verb': [2], 'dobj': [], 'pobj': [4, 5]}}, {'text': {'subj': '', 'verb': '', 'dobj': '', 'pobj': ''}, 'idx': {'subj': [], 'verb': [], 'dobj': [], 'pobj': []}}, {'text': {'subj': 'it', 'verb': '', 'dobj': '', 'pobj': ''}, 'idx': {'subj': [5], 'verb': [], 'dobj': [], 'pobj': []}}, {'text': {'subj': 'the scroll', 'verb': '', 'dobj': 'writing on both sides', 'pobj': 'both sides'}, 'idx': {'subj': [0, 1], 'verb': [], 'dobj': [3, 4, 5, 6], 'pobj': [5, 6]}}, {'text': {'subj': '', 'verb': 'try', 'dobj': 'it', 'pobj': ''}, 'idx': {'subj': [], 'verb': [0], 'dobj': [4], 'pobj': []}}, {'text': {'subj': '', 'verb': 'pulled', 'dobj': '', 'pobj': 'the < unk > of an apartment building'}, 'idx': {'subj': [], 'verb': [6], 'dobj': [], 'pobj': [8, 9, 10, 11, 12, 13, 14, 15]}}, {'text': {'subj': 'that', 'verb': '', 'dobj': 'what', 'pobj': ''}, 'idx': {'subj': [0], 'verb': [], 'dobj': [2], 'pobj': []}}, {'text': {'subj': 'it', 'verb': 'matches', 'dobj': 'her voice', 'pobj': ''}, 'idx': {'subj': [1], 'verb': [2], 'dobj': [3, 4], 'pobj': []}}, {'text': {'subj': 'the man who was healed', 'verb': '', 'dobj': '', 'pobj': ''}, 'idx': {'subj': [0, 1, 2, 3, 4], 'verb': [], 'dobj': [], 'pobj': []}}, {'text': {'subj': 'you', 'verb': 'come', 'dobj': '', 'pobj': 'ten'}, 'idx': {'subj': [0], 'verb': [1], 'dobj': [], 'pobj': [4]}}, {'text': {'subj': 'he', 'verb': 'shouted', 'dobj': 'me', 'pobj': ''}, 'idx': {'subj': [0], 'verb': [1], 'dobj': [8], 'pobj': []}}, {'text': {'subj': '', 'verb': '', 'dobj': '', 'pobj': 'this < unk'}, 'idx': {'subj': [], 'verb': [], 'dobj': [], 'pobj': [7, 8, 9]}}]\nOntonotes &  it will trade over the counter under the symbol cray . &  it& trade& & the counter &  nsubj aux ROOT prep prep punct& it will trade over under . \\\\ \\hline\nOntonotes &  -- &  & & &  &  ROOT& -- \\\\ \\hline\nOntonotes &  there is a risk that it could become sort of a secondary brand . &  it& & &  &  expl ROOT attr punct& there is risk . \\\\ \\hline\nOntonotes &  the scroll had writing on both sides and was kept closed with seven seals &  the scroll& & writing on both sides& both sides &  nsubj ROOT dobj cc conj& scroll had writing and kept \\\\ \\hline\nOntonotes &  try hard to enter it . &  & try& it&  &  ROOT advmod xcomp punct& try hard enter . \\\\ \\hline\nOntonotes &  a woman and child were also pulled from the <unk> of an apartment building &  & pulled& & the < unk > of an apartment building &  nsubjpass auxpass advmod ROOT prep& woman were also pulled from \\\\ \\hline\nOntonotes &  that was what they said . &  that& & what&  &  nsubj ROOT ccomp punct& that was said . \\\\ \\hline\nOntonotes &  -lrb- it matches her voice . -rrb- &  it& matches& her voice&  &  npadvmod nsubj ROOT dobj punct punct& -lrb- it matches voice . -rrb- \\\\ \\hline\nOntonotes &  the man who was healed was more than 40 years old . &  the man who was healed& & &  &  nsubj ROOT acomp punct& man was old . \\\\ \\hline\nOntonotes &  you come in at ten , &  you& come& & ten &  nsubj ROOT prt prep punct& you come in at , \\\\ \\hline\nOntonotes &  he shouted , `` lord , save me ! '' &  he& shouted& me&  &  nsubj ROOT punct punct xcomp punct punct& he shouted , ` save ! '' \\\\ \\hline\nOntonotes &  there are at least three defects in this <unk> of yours . &  & & & this < unk &  expl ROOT attr punct punct& there are defects > . \\\\ \\hline\n"
     ]
    }
   ],
   "source": [
    "data.reinit_iterator(\"train\")\n",
    "tr_iter = iter(data.train_iter)\n",
    "batch = next(tr_iter)\n",
    "\n",
    "sents = [' '.join([data.vocab.itos[t]\n",
    "                   for t in text_i]).replace('<eos>', '').replace('<pad>', '').replace('<go>', '').strip()\n",
    "         for text_i in batch.text[:12]]\n",
    "print(sents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'det'), ('construction', 'compound'), ('workers', 'nsubj'), ('are', 'aux'), ('using', 'ROOT'), ('a', 'det'), ('pick', 'amod'), ('axes', 'dobj'), ('to', 'aux'), ('poke', 'xcomp'), ('each', 'det'), ('other', 'dobj'), ('.', 'punct')]\n[{'text': {'subj': 'it', 'verb': 'trade', 'dobj': '', 'pobj': 'the counter'}, 'idx': {'subj': [0], 'verb': [2], 'dobj': [], 'pobj': [4, 5]}}, {'text': {'subj': '', 'verb': '', 'dobj': '', 'pobj': ''}, 'idx': {'subj': [], 'verb': [], 'dobj': [], 'pobj': []}}, {'text': {'subj': 'it', 'verb': '', 'dobj': '', 'pobj': ''}, 'idx': {'subj': [5], 'verb': [], 'dobj': [], 'pobj': []}}, {'text': {'subj': 'the scroll', 'verb': '', 'dobj': 'writing on both sides', 'pobj': 'both sides'}, 'idx': {'subj': [0, 1], 'verb': [], 'dobj': [3, 4, 5, 6], 'pobj': [5, 6]}}, {'text': {'subj': '', 'verb': 'try', 'dobj': 'it', 'pobj': ''}, 'idx': {'subj': [], 'verb': [0], 'dobj': [4], 'pobj': []}}, {'text': {'subj': '', 'verb': 'pulled', 'dobj': '', 'pobj': 'the < unk > of an apartment building'}, 'idx': {'subj': [], 'verb': [6], 'dobj': [], 'pobj': [8, 9, 10, 11, 12, 13, 14, 15]}}, {'text': {'subj': 'that', 'verb': '', 'dobj': 'what', 'pobj': ''}, 'idx': {'subj': [0], 'verb': [], 'dobj': [2], 'pobj': []}}, {'text': {'subj': 'it', 'verb': 'matches', 'dobj': 'her voice', 'pobj': ''}, 'idx': {'subj': [1], 'verb': [2], 'dobj': [3, 4], 'pobj': []}}, {'text': {'subj': 'the man who was healed', 'verb': '', 'dobj': '', 'pobj': ''}, 'idx': {'subj': [0, 1, 2, 3, 4], 'verb': [], 'dobj': [], 'pobj': []}}, {'text': {'subj': 'you', 'verb': 'come', 'dobj': '', 'pobj': 'ten'}, 'idx': {'subj': [0], 'verb': [1], 'dobj': [], 'pobj': [4]}}, {'text': {'subj': 'he', 'verb': 'shouted', 'dobj': 'me', 'pobj': ''}, 'idx': {'subj': [0], 'verb': [1], 'dobj': [8], 'pobj': []}}, {'text': {'subj': '', 'verb': '', 'dobj': '', 'pobj': 'this < unk'}, 'idx': {'subj': [], 'verb': [], 'dobj': [], 'pobj': [7, 8, 9]}}]\nOntonotes &  it will trade over the counter under the symbol cray . &  it& trade& & the counter &  nsubj aux ROOT prep prep punct& it will trade over under . \\\\ \\hline\nOntonotes &  -- &  & & &  &  ROOT& -- \\\\ \\hline\nOntonotes &  there is a risk that it could become sort of a secondary brand . &  it& & &  &  expl ROOT attr punct& there is risk . \\\\ \\hline\nOntonotes &  the scroll had writing on both sides and was kept closed with seven seals &  the scroll& & writing on both sides& both sides &  nsubj ROOT dobj cc conj& scroll had writing and kept \\\\ \\hline\nOntonotes &  try hard to enter it . &  & try& it&  &  ROOT advmod xcomp punct& try hard enter . \\\\ \\hline\nOntonotes &  a woman and child were also pulled from the <unk> of an apartment building &  & pulled& & the < unk > of an apartment building &  nsubjpass auxpass advmod ROOT prep& woman were also pulled from \\\\ \\hline\nOntonotes &  that was what they said . &  that& & what&  &  nsubj ROOT ccomp punct& that was said . \\\\ \\hline\nOntonotes &  -lrb- it matches her voice . -rrb- &  it& matches& her voice&  &  npadvmod nsubj ROOT dobj punct punct& -lrb- it matches voice . -rrb- \\\\ \\hline\nOntonotes &  the man who was healed was more than 40 years old . &  the man who was healed& & &  &  nsubj ROOT acomp punct& man was old . \\\\ \\hline\nOntonotes &  you come in at ten , &  you& come& & ten &  nsubj ROOT prt prep punct& you come in at , \\\\ \\hline\nOntonotes &  he shouted , `` lord , save me ! '' &  he& shouted& me&  &  nsubj ROOT punct punct xcomp punct punct& he shouted , ` save ! '' \\\\ \\hline\nOntonotes &  there are at least three defects in this <unk> of yours . &  & & & this < unk &  expl ROOT attr punct punct& there are defects > . \\\\ \\hline\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ex_sents = ['i was originally told it would take _num_ mins .',\n",
    "            'slow , over priced , i \\'ll go elsewhere next time .',\n",
    "            'we will not be back',\n",
    "            'terrible .',\n",
    "            'at this point they were open and would be for another hour .',\n",
    "            'people are outside playing baseball .',\n",
    "            'two dogs pull on opposite ends of a rope .',\n",
    "            'a lady lays at a beach .',\n",
    "            'the construction workers are using a pick axes to poke each other .',\n",
    "            'people are running through the streets while people watch .',\n",
    "            'someone prepares food into bowls ']\n",
    "ex_sents = sents\n",
    "orig = [\"Ontonotes\"]*12#+['Yelp']*5+['SNLI']*5\n",
    "print([(tok.text, tok.dep_) for tok in nlp(\n",
    "            'the construction workers are using a pick axes to poke each other .')])\n",
    "\n",
    "\n",
    "roles, temptypes = ['subj', 'verb', 'dobj', 'pobj'], ['syn', 'lex']\n",
    "deps = shallow_dependencies(ex_sents)\n",
    "temps = truncated_template(ex_sents)\n",
    "print(deps)\n",
    "for i in range(len(ex_sents)):\n",
    "    print(orig[i], '& ',ex_sents[i], '& ', '& '.join([deps[i]['text'][k] for k in roles]), '& ',\n",
    "          '& '.join([temps[i][k] for k in temptypes]), '\\\\\\\\ \\hline')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' i have been here for years and have been excellent ', \" it 's not worth the money \", ' i love this place ', \" i 've always had the new york style and the staff \", ' so , they have a great selection of food ']\n"
     ]
    }
   ],
   "source": [
    "text, samples, params = model.get_sentences(5, gen_len=16, sample_w=False, vary_z=True, complete=None, contains=None, max_tries=100)\n",
    "\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (10) must match the existing size (20) at non-singleton dimension 0.  Target sizes: [10, 96].  Tensor sizes: [20, 96]",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-2941810c7ad6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mvar_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0malt_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malt_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_alternative_sentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomplete\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m':'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malt_text\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Experiments\\GLUE_BENCH\\disentanglement_qkv\\models.py\u001b[0m in \u001b[0;36m_get_alternative_sentences\u001b[1;34m(self, prev_latent_vals, params, var_z_ids, n_samples, gen_len, complete)\u001b[0m\n\u001b[0;32m    580\u001b[0m         \u001b[0mz_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'z{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0morig_zs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_zs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhas_struct\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 582\u001b[1;33m             \u001b[0mz_input\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'zs'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0morig_zst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    583\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhas_zg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m             \u001b[0mz_input\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'zg'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0morig_zg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The expanded size of the tensor (10) must match the existing size (20) at non-singleton dimension 0.  Target sizes: [10, 96].  Tensor sizes: [20, 96]"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "var_ids = [2]\n",
    "alt_text, alt_params = model._get_alternative_sentences(samples, None, var_ids, 2, 16, complete=None,)\n",
    "for i in range(len(text)):\n",
    "    print(text[i], ':', alt_text[i::len(text)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==varying content==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " i have been here for years and have been excellent  : [' i would not recommend this place to anyone and the service ', \" i 'm not sure why they are a great job \"]\n it 's not worth the money  : [\" unfortunately , i wo n't be back \", ' the service was not very good ']\n i love this place  : [' i love this place ', ' i love this place ']\n i 've always had the new york style and the staff  : [\" i 'm very happy with the service and you 'll be \", ' they are always a smile and you can find a smile ']\n so , they have a great selection of food  : [' they are very reasonable and the food is great ', ' the atmosphere is great and the food is great ']\n==varying structure==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " i have been here for years and have been excellent  : [\" i have been here for _num_ years and it 's a great experience \", ' i have been here for years and they have a great meal and service ', ' i love this place ', \" i have been here for years and it 's always good \"]\n it 's not worth the money  : [' not a good experience ', \" i 'm not sure why it 's great \", \" it 's a great place to go \", \" i do n't even know how to make a good experience \"]\n i love this place  : [' this was really great ', ' this place was fantastic ', ' i have been here for years and it was really great ', ' i love this place ']\n i 've always had the new york style and the staff  : [\" i 've always seen the new york style \", ' i have been here many times and the new store is ', ' the new york style is always great ', ' i love the new york style and new store ']\n so , they have a great selection of food  : [' i would not recommend this place period ', \" they have so much more than that , but it 's worth it \", ' i highly recommend them , they are so much ', ' i would highly recommend them too ']\n"
     ]
    }
   ],
   "source": [
    "def _get_alternative_sentences(mdl, prev_latent_vals, params, var_z_ids, n_samples, gen_len, complete=None):\n",
    "        h_params = mdl.h_params\n",
    "        has_struct = mdl.h_params.graph_generator in (get_qkv_graph, get_hqkv_graph)\n",
    "        has_zg = mdl.h_params.graph_generator = get_hqkv_graph\n",
    "\n",
    "        n_orig_sentences = prev_latent_vals['z1'].shape[0]\n",
    "        go_symbol = torch.ones([n_samples * n_orig_sentences]).long() * \\\n",
    "                    mdl.index[mdl.generated_v].stoi['<go>']\n",
    "        go_symbol = go_symbol.to(mdl.h_params.device).unsqueeze(-1)\n",
    "        x_prev = go_symbol\n",
    "        if complete is not None:\n",
    "            for token in complete.split(' '):\n",
    "                x_prev = torch.cat(\n",
    "                    [x_prev, torch.ones([n_samples * n_orig_sentences, 1]).long().to(mdl.h_params.device) * \\\n",
    "                     mdl.index[mdl.generated_v].stoi[token]], dim=1)\n",
    "            gen_len = gen_len - len(complete.split(' '))\n",
    "\n",
    "        orig_zs = [prev_latent_vals['z{}'.format(i+1)].repeat(n_samples, 1) for i in range(len(h_params.n_latents))]\n",
    "        zs = [mdl.gen_bn.name_to_v['z{}'.format(i+1)] for i in range(len(h_params.n_latents))]\n",
    "        gen_input = {**{'z{}'.format(i+1): orig_zs[i].unsqueeze(1) for i in range(len(orig_zs))},\n",
    "                     'x_prev': torch.zeros((n_samples * n_orig_sentences, 1, mdl.generated_v.size)).to(\n",
    "                         mdl.h_params.device)}\n",
    "        if has_struct:\n",
    "            orig_zst = prev_latent_vals['zs'].repeat(n_samples, 1)\n",
    "            zst = mdl.gen_bn.name_to_v['zs']\n",
    "            gen_input['zs'] = orig_zst.unsqueeze(1)\n",
    "        if has_zg:\n",
    "            orig_zg = prev_latent_vals['zg'].repeat(n_samples, 1)\n",
    "            zg = mdl.gen_bn.name_to_v['zg']\n",
    "            # gen_input['zg'] = zg.prior_sample((n_samples * n_orig_sentences,))[0]\n",
    "            gen_input['zg'] = orig_zg.unsqueeze(1)\n",
    "        mdl.gen_bn(gen_input)\n",
    "        if has_zg:\n",
    "            z1_sample = zs[0].posterior_sample(mdl.gen_bn.name_to_v['z1'].post_params)[0].squeeze(1)\n",
    "            if has_struct:\n",
    "                zst_sample = zst.posterior_sample(mdl.gen_bn.name_to_v['zs'].post_params)[0].squeeze(1)\n",
    "        else:\n",
    "            z1_sample = zs[0].prior_sample((n_samples * n_orig_sentences,))[0]\n",
    "            if has_struct:\n",
    "                zst_sample = zst.prior_sample((n_samples * n_orig_sentences,))[0]\n",
    "        zs_sample = [z1_sample] +\\\n",
    "                    [z.post_samples.squeeze(1) for z in zs[1:]]\n",
    "\n",
    "        for id in var_z_ids:\n",
    "            # id == sum(h_params.n_latents) means its zst\n",
    "            if id == sum(h_params.n_latents) and has_struct:\n",
    "                orig_zst = zst_sample\n",
    "            else:\n",
    "                assert id < sum(h_params.n_latents)\n",
    "                z_number = sum([id > sum(h_params.n_latents[:i + 1]) for i in range(len(h_params.n_latents))])\n",
    "                z_index = id - sum(h_params.n_latents[:z_number])\n",
    "                start, end = int(h_params.z_size / max(h_params.n_latents) * z_index), int(\n",
    "                    h_params.z_size / max(h_params.n_latents) * (z_index + 1))\n",
    "                source, destination = zs_sample[z_number], orig_zs[z_number]\n",
    "                destination[:, start:end] = source[:, start:end]\n",
    "\n",
    "        z_input = {'z{}'.format(i+1): orig_zs[i].unsqueeze(1) for i in range(len(orig_zs))}\n",
    "        if has_struct:\n",
    "            z_input['zs'] = orig_zst.unsqueeze(1)\n",
    "        if has_zg:\n",
    "            z_input['zg'] = orig_zg.unsqueeze(1)\n",
    "\n",
    "        # Normal Autoregressive generation\n",
    "        for i in range(gen_len):\n",
    "            mdl.gen_bn({'x_prev': x_prev, **{k: v.expand(v.shape[0], i + 1, v.shape[-1])\n",
    "                                             for k, v in z_input.items()}})\n",
    "            samples_i = mdl.generated_v.post_params['logits']\n",
    "\n",
    "            x_prev = torch.cat([x_prev, torch.argmax(samples_i, dim=-1)[..., -1].unsqueeze(-1)],\n",
    "                               dim=-1)\n",
    "\n",
    "        text = mdl.decode_to_text2(x_prev, mdl.h_params.vocab_size, mdl.index[mdl.generated_v])\n",
    "        samples = {'z{}'.format(i+1): zs_sample[i].tolist() for i in range(len(orig_zs))}\n",
    "        if has_struct:\n",
    "            samples['zs'] = zst_sample.tolist()\n",
    "        if has_zg:\n",
    "            samples['zg'] = orig_zg.tolist()\n",
    "        return text, samples\n",
    "print(\"==varying content==\")\n",
    "var_ids = list(range(8))\n",
    "alt_text, alt_params = _get_alternative_sentences(model, samples, None, var_ids, 2, 16, complete=None)\n",
    "for i in range(len(text)):\n",
    "    print(text[i], ':', alt_text[i::len(text)])\n",
    "print(\"==varying structure==\")\n",
    "var_ids = [8]\n",
    "alt_text, alt_params = _get_alternative_sentences(model, samples, None, var_ids, 4, 16, complete=None)\n",
    "for i in range(len(text)):\n",
    "    print(text[i], ':', alt_text[i::len(text)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " no server , he was very friendly and helpful  : [' great service and great food ', ' great service ']\n the staff is friendly and helpful and friendly  : [' our food was very good and loved it and it disappeared ', ' the staff is friendly and helpful and so hard to pay her cafe ']\n the place is way more than the food is good  : [' the food was delicious and the service was great ', ' the food was delicious and the service was great ']\n amazing service , what a good time  : [' amazing people ', ' the food is always good , service is great ']\n i 've been here twice and we 're quite happy with my experience  : [' the food is always fresh and delicious ', ' i will definitely be back again ']\n"
     ]
    }
   ],
   "source": [
    "var_ids = [3, 4, 5, 6, 7]\n",
    "alt_text, alt_params = model._get_alternative_sentences(samples, None, var_ids, 2, 16, complete=None)\n",
    "for i in range(len(text)):\n",
    "    print(text[i], ':', alt_text[i::len(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' i have been here for years and have been excellent ', \" it 's not worth the money \", ' i love this place ', \" i 've always had the new york style and the staff \", ' so , they have a great selection of food ']\nz_from:   i have been here for years and have been excellent  |z_to:   it 's not worth the money  |result:   i 'm not sure why i do n't even have any\nz_from:   i have been here for years and have been excellent  |z_to:   i love this place  |result:   i have been here for years and it was really cool\nz_from:   i have been here for years and have been excellent  |z_to:   i 've always had the new york style and the staff  |result:   i have been here many times and the new store\nz_from:   i have been here for years and have been excellent  |z_to:   so , they have a great selection of food  |result:   i would n't recommend this place , so they are unbelievable\nz_from:   it 's not worth the money  |z_to:   i have been here for years and have been excellent  |result:   i have been here for years and it 's a great place\nz_from:   it 's not worth the money  |z_to:   i love this place  |result:   this was the worst place i 've ever been to\nz_from:   it 's not worth the money  |z_to:   i 've always had the new york style and the staff  |result:   the new york style is always good\nz_from:   it 's not worth the money  |z_to:   so , they have a great selection of food  |result:   so , they have no idea\nz_from:   i love this place  |z_to:   i have been here for years and have been excellent  |result:   i love this place\nz_from:   i love this place  |z_to:   it 's not worth the money  |result:   i love this place\nz_from:   i love this place  |z_to:   i 've always had the new york style and the staff  |result:   i love this store\nz_from:   i love this place  |z_to:   so , they have a great selection of food  |result:   i highly recommend them\nz_from:   i 've always had the new york style and the staff  |z_to:   i have been here for years and have been excellent  |result:   i have been here for years and it was wonderful\nz_from:   i 've always had the new york style and the staff  |z_to:   it 's not worth the money  |result:   i do n't even know how to make a money\nz_from:   i 've always had the new york style and the staff  |z_to:   i love this place  |result:   i love this place and the place was so clean\nz_from:   i 've always had the new york style and the staff  |z_to:   so , they have a great selection of food  |result:   i highly recommend them , they have so much better\nz_from:   so , they have a great selection of food  |z_to:   i have been here for years and have been excellent  |result:   i have been here for _num_ years and it 's a great experience\nz_from:   so , they have a great selection of food  |z_to:   it 's not worth the money  |result:   but , i do n't know how to make it\nz_from:   so , they have a great selection of food  |z_to:   i love this place  |result:   i love this place and the food is great\nz_from:   so , they have a great selection of food  |z_to:   i 've always had the new york style and the staff  |result:   the new york style is amazing\n"
     ]
    }
   ],
   "source": [
    "def decode_to_text(x_hat_params, vocab_size, vocab_index):\n",
    "    # It is assumed that this function is used at test time for display purposes\n",
    "    # Getting the argmax from the one hot if it's not done\n",
    "    while x_hat_params.shape[-1] == vocab_size and x_hat_params.ndim > 3:\n",
    "        x_hat_params = x_hat_params.mean(0)\n",
    "    while x_hat_params.ndim > 2 and x_hat_params.shape[-1] != self.h_params.vocab_size:\n",
    "        x_hat_params = x_hat_params[0]\n",
    "    if x_hat_params.shape[-1] == vocab_size:\n",
    "        x_hat_params = torch.argmax(x_hat_params, dim=-1)\n",
    "    assert x_hat_params.ndim == 2, \"Mis-shaped generated sequence: {}\".format(x_hat_params.shape)\n",
    "    \n",
    "    samples = [' '.join([vocab_index.itos[w]\n",
    "                         for w in sen]).split('<eos>')[0].split(' !')[0].split(' .')[0].replace('<go>', '').replace('</go>', '')\n",
    "               .replace('<pad>', '_').replace('_unk', '<?>')\n",
    "               for sen in x_hat_params]\n",
    "\n",
    "    return samples\n",
    "\n",
    "\n",
    "def swap_latents(mdl, prev_latent_vals, var_z_ids, gen_len, complete=None, no_unk=True):\n",
    "            has_struct = 'zs' in mdl.gen_bn.name_to_v\n",
    "            has_zg = 'zg' in mdl.gen_bn.name_to_v\n",
    "            \n",
    "            \n",
    "            n_orig_sentences = prev_latent_vals['z1'].shape[0]\n",
    "            n_samples = n_orig_sentences\n",
    "            go_symbol = torch.ones([n_samples * n_orig_sentences]).long() * \\\n",
    "                        mdl.index[mdl.generated_v].stoi['<go>']\n",
    "            go_symbol = go_symbol.to(mdl.h_params.device).unsqueeze(-1)\n",
    "            x_prev = go_symbol\n",
    "            if complete is not None:\n",
    "                for token in complete.split(' '):\n",
    "                    x_prev = torch.cat([x_prev, torch.ones([n_samples * n_orig_sentences, 1]).long().to(mdl.h_params.device) * \\\n",
    "                        mdl.index[mdl.generated_v].stoi[token]], dim=1)\n",
    "                gen_len = gen_len - len(complete.split(' '))\n",
    "            temp = 1.\n",
    "            orig_z = prev_latent_vals['z1'].unsqueeze(1).repeat(1, n_samples, 1)\n",
    "            z_sample = orig_z.reshape(n_samples*n_orig_sentences, -1)\n",
    "            orig_z = orig_z.transpose(0, 1).reshape(n_samples*n_orig_sentences, -1)\n",
    "            if has_struct:\n",
    "                orig_zst = prev_latent_vals['zs'].unsqueeze(1).repeat(1, n_samples, 1)\n",
    "                zst_sample = orig_zst.reshape(n_samples*n_orig_sentences, -1)\n",
    "                orig_zst = orig_zst.transpose(0, 1).reshape(n_samples*n_orig_sentences, -1)\n",
    "            if has_zg:\n",
    "                orig_zg = prev_latent_vals['zg'].unsqueeze(1).repeat(1, n_samples, 1)\n",
    "                orig_zg = orig_zg.transpose(0, 1).reshape(n_samples*n_orig_sentences, -1)\n",
    "            \n",
    "\n",
    "            for id in var_z_ids:\n",
    "                if id < sum(h_params.n_latents):\n",
    "                    z_number = sum([id> sum(h_params.n_latents[:i+1]) for i in range(len(h_params.n_latents))])\n",
    "                    z_index = id - sum(h_params.n_latents[:z_number])\n",
    "                    start, end = int(h_params.z_size/max(h_params.n_latents)*z_index),\\\n",
    "                                 int(h_params.z_size/max(h_params.n_latents)*(z_index+1))\n",
    "                    source, destination = [z_sample][z_number], [orig_z][z_number]\n",
    "                    destination[:, start:end] = source[:, start:end]\n",
    "                elif id == sum(h_params.n_latents) and has_struct:\n",
    "                    orig_zst = zst_sample\n",
    "                else:\n",
    "                    raise IndexError(\"You gave a too high z_id for swapping with this model\")\n",
    "                    \n",
    "            z_input = {'z1': orig_z.unsqueeze(1), **({'zs':orig_zst.unsqueeze(1)} if has_struct else {}), \n",
    "                       **({'zg':orig_zg.unsqueeze(1)} if has_zg else {})}\n",
    "            \n",
    "            # Normal Autoregressive generation\n",
    "            for i in range(gen_len):\n",
    "                mdl.gen_bn({'x_prev': x_prev, **{k: v.expand(v.shape[0], i+1, v.shape[-1])\n",
    "                                                  for k, v in z_input.items()}})\n",
    "                samples_i = mdl.generated_v.post_params['logits']\n",
    "                if no_unk:\n",
    "                    annul_vector = 1-F.one_hot(torch.tensor([data.vocab.stoi['<unk>']]).to(DEVICE), h_params.vocab_size)\n",
    "                    samples_i *= annul_vector\n",
    "                \n",
    "                x_prev = torch.cat([x_prev, torch.argmax(samples_i,     dim=-1)[..., -1].unsqueeze(-1)],\n",
    "                                   dim=-1)\n",
    "            \n",
    "            text = decode_to_text(x_prev, mdl.h_params.vocab_size, mdl.index[mdl.generated_v])\n",
    "            return text, {'z1': orig_z}\n",
    "sw_zs = [8]\n",
    "sw_text, sw_samples = swap_latents(model, samples, sw_zs, 16, complete=None, no_unk=True)\n",
    "print(text)\n",
    "for i in range(len(text)):\n",
    "    for j in range(len(text)):\n",
    "        if i!=j:\n",
    "            print(\"z_from: \", text[i], \"|z_to: \", text[j], \"|result: \", sw_text[len(text)*i+j])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' negative thing is any of the place ', ' i love their iced tea ', ' lunch sandwich was good ', ' i will definitely be back for visiting business with your needs ', \" the _num_ other of the store is n't as good as it was \"]\n"
     ]
    }
   ],
   "source": [
    "text, samples, params = model.get_sentences(5, gen_len=16, sample_w=False, vary_z=True, complete=None, \n",
    "                                            contains=None, max_tries=100)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Changing Structure=======\n-->  negative thing is any of the place  |<  seriously >< i would recommend any day to the servers for sure >< some open dry either >< the food is amazing but the workers are made for the water  >\n-->  i love their iced tea  |<  the service is consistently excellent as well >< the service is consistently excellent as well >< watch out >< watch the spot if you get a great price  >\n-->  lunch sandwich was good  |<  i love this place >< rare my sandwich at $ _num_ >< i will definitely be back >< the food is always good and the employees are knowledgeable  >\n-->  i will definitely be back for visiting business with your needs  |<  i love the bruschetta and queso were all delicious >< they were very nice and extremely knowledgable and looked very welcoming and fun >< great food >< i love the bruschetta and the food is great  >\n-->  the _num_ other of the store is n't as good as it was  |<  love this place >< great authentic indian food >< thank you to the top notch staff >< let 's tell you that i will be back  >\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Changing Content=======\n-->  negative thing is any of the place  |<  great atmosphere , great food >< great food at a reasonable price >< friendly , outstanding , and the food is great >< slow and speedy service  >\n-->  i love their iced tea  |<  i absolutely recommend their safeway >< i am very satisfied with dave >< the food is horrible and cool >< i enjoy getting the mini food  >\n-->  lunch sandwich was good  |<  great woman and the staff is always friendly and helpful >< owner is very friendly and helpful >< wish i could give this place zero stars >< sure it was a great experience  >\n-->  i will definitely be back for visiting business with your needs  |<  i would recommend this place and i would recommend your dentist >< i would recommend this place to anyone i like taking your business >< i will definitely be back to see any great care >< the food is great and the selection is the best around  >\n-->  the _num_ other of the store is n't as good as it was  |<  i used to have the best `` grocery '' i 've ever had >< they have a cool patio , lots of regulars and lots of animals >< i was here for a family trip for a quick lunch and lunch >< i was here for the first time and she was very sweet and sweet  >\n"
     ]
    }
   ],
   "source": [
    "alt_text, alt_samples = model._get_alternative_sentences(samples, params, [8], 4, 16, complete=None)\n",
    "print(\"====== Changing Structure=======\")\n",
    "for i in range(len(text)):\n",
    "    print(\"-->\", text[i], '|<', '><'.join(alt_text[i::len(text)]), '>')\n",
    "alt_text, alt_samples = model._get_alternative_sentences(samples, params, list(range(8)), 4, 16, complete=None)\n",
    "print(\"====== Changing Content=======\")\n",
    "for i in range(len(text)):\n",
    "    print(\"-->\", text[i], '|<', '><'.join(alt_text[i::len(text)]), '>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
