{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b97e0f2e99bae8b9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset csv (C:\\Users\\ghazy\\.cache\\huggingface\\datasets\\csv\\default-b97e0f2e99bae8b9\\0.0.0\\2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words:  10000 , On device:  cuda\nLoss Type:  VAE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstruction net size: 09.71 M\nprior net sizes:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model at step 84766\nUnsupervised training examples:  493056\nUnsupervised val examples:  384.0\nNumber of parameters:  28.47 M\nInference parameters:  20.04 M\nGeneration parameters:  09.71 M\nEmbedding parameters:  01.28 M\n"
     ]
    }
   ],
   "source": [
    "# This file will implement the main training loop for a model\n",
    "from time import time\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "from torch import device\n",
    "import torch\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "\n",
    "from disentanglement_final.data_prep import NLIGenData2, OntoGenData, HuggingYelp2, ParaNMTCuratedData\n",
    "from disentanglement_final.models import DisentanglementTransformerVAE, LaggingDisentanglementTransformerVAE\n",
    "from disentanglement_final.h_params import DefaultTransformerHParams as HParams\n",
    "from disentanglement_final.graphs import *\n",
    "from components.criteria import *\n",
    "parser = argparse.ArgumentParser()\n",
    "from torch.nn import MultiheadAttention\n",
    "# Training and Optimization\n",
    "k, kz, klstm = 2, 4, 2\n",
    "parser.add_argument(\"--test_name\", default='unnamed', type=str)\n",
    "parser.add_argument(\"--data\", default='nli', choices=[\"nli\", \"ontonotes\", \"yelp\", 'paranmt'], type=str)\n",
    "parser.add_argument(\"--csv_out\", default='disentqkv.csv', type=str)\n",
    "parser.add_argument(\"--max_len\", default=17, type=int)\n",
    "parser.add_argument(\"--batch_size\", default=128, type=int)\n",
    "parser.add_argument(\"--grad_accu\", default=1, type=int)\n",
    "parser.add_argument(\"--n_epochs\", default=20, type=int)\n",
    "parser.add_argument(\"--test_freq\", default=32, type=int)\n",
    "parser.add_argument(\"--complete_test_freq\", default=160, type=int)\n",
    "parser.add_argument(\"--generation_weight\", default=1, type=float)\n",
    "parser.add_argument(\"--device\", default='cuda:0', choices=[\"cuda:0\", \"cuda:1\", \"cuda:2\", \"cpu\"], type=str)\n",
    "parser.add_argument(\"--embedding_dim\", default=128, type=int)#################\"\n",
    "parser.add_argument(\"--pretrained_embeddings\", default=False, type=bool)#################\"\n",
    "parser.add_argument(\"--z_size\", default=96*kz, type=int)#################\"\n",
    "parser.add_argument(\"--z_emb_dim\", default=192*k, type=int)#################\"\n",
    "parser.add_argument(\"--n_keys\", default=4, type=int)#################\"\n",
    "parser.add_argument(\"--n_latents\", default=[16, 16, 16], nargs='+', type=int)#################\"\n",
    "parser.add_argument(\"--text_rep_l\", default=3, type=int)\n",
    "parser.add_argument(\"--text_rep_h\", default=192*k, type=int)\n",
    "parser.add_argument(\"--encoder_h\", default=192*k, type=int)#################\"\n",
    "parser.add_argument(\"--encoder_l\", default=2, type=int)#################\"\n",
    "parser.add_argument(\"--decoder_h\", default=int(192*k), type=int)\n",
    "parser.add_argument(\"--decoder_l\", default=2, type=int)#################\"\n",
    "parser.add_argument(\"--highway\", default=False, type=bool)\n",
    "parser.add_argument(\"--markovian\", default=True, type=bool)\n",
    "parser.add_argument('--minimal_enc', dest='minimal_enc', action='store_true')\n",
    "parser.add_argument('--no-minimal_enc', dest='minimal_enc', action='store_false')\n",
    "parser.set_defaults(minimal_enc=False)\n",
    "parser.add_argument(\"--losses\", default='VAE', choices=[\"VAE\", \"IWAE\" \"LagVAE\"], type=str)\n",
    "parser.add_argument(\"--graph\", default='Normal', choices=[\"Vanilla\", \"IndepInfer\", \"QKV\", \"HQKV\"], type=str)\n",
    "parser.add_argument(\"--training_iw_samples\", default=1, type=int)\n",
    "parser.add_argument(\"--testing_iw_samples\", default=5, type=int)\n",
    "parser.add_argument(\"--test_prior_samples\", default=10, type=int)\n",
    "parser.add_argument(\"--anneal_kl0\", default=3000, type=int)\n",
    "parser.add_argument(\"--anneal_kl1\", default=6000, type=int)\n",
    "parser.add_argument(\"--anneal_kl_type\", default=\"linear\", choices=[\"linear\", \"sigmoid\"], type=str)\n",
    "parser.add_argument(\"--grad_clip\", default=5., type=float)\n",
    "parser.add_argument(\"--kl_th\", default=0/(768*k/2), type=float or None)\n",
    "parser.add_argument(\"--max_elbo1\", default=6.0, type=float)\n",
    "parser.add_argument(\"--max_elbo2\", default=4.0, type=float)\n",
    "parser.add_argument(\"--max_elbo_choice\", default=10, type=int)\n",
    "parser.add_argument(\"--kl_beta\", default=0.5, type=float)\n",
    "parser.add_argument(\"--kl_beta_zs\", default=0.1, type=float)\n",
    "parser.add_argument(\"--kl_beta_zg\", default=0.5/8, type=float)\n",
    "parser.add_argument(\"--dropout\", default=0.3, type=float)\n",
    "parser.add_argument(\"--word_dropout\", default=0.1, type=float)\n",
    "parser.add_argument(\"--l2_reg\", default=0, type=float)\n",
    "parser.add_argument(\"--lr\", default=2e-4, type=float)\n",
    "parser.add_argument(\"--lr_reduction\", default=4., type=float)\n",
    "parser.add_argument(\"--wait_epochs\", default=1, type=float)\n",
    "parser.add_argument(\"--save_all\", default=True, type=bool)\n",
    "\n",
    "flags, _ = parser.parse_known_args()\n",
    "\n",
    "# Manual Settings, Deactivate before pushing\n",
    "if True:\n",
    "    flags.batch_size = 128\n",
    "    flags.grad_accu = 1\n",
    "    flags.max_len = 20\n",
    "    flags.test_name = \"nliLM/HQKVParanmtMini5\"\n",
    "    flags.data = \"paranmt\"\n",
    "    flags.n_latents = [16]\n",
    "    flags.n_keys = 16\n",
    "    flags.graph =\"QKV\"  # \"Vanilla\"\n",
    "    # flags.losses = \"LagVAE\"\n",
    "    flags.kl_beta = 0.4\n",
    "    flags.kl_beta_zg = 0.1\n",
    "    flags.kl_beta_zs = 0.1\n",
    "    # flags.anneal_kl0, flags.anneal_kl1 = 3900, 6900\n",
    "    flags.word_dropout = 0.4\n",
    "    flags.anneal_kl_type = \"sigmoid\"\n",
    "\n",
    "    # flags.anneal_kl0 = 0\n",
    "    flags.max_elbo_choice = 6\n",
    "    # flags.z_size = 16\n",
    "    # flags.encoder_h = 256\n",
    "    # flags.decoder_h = 256\n",
    "\n",
    "\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "GRAPH = {\"Vanilla\": get_vanilla_graph,\n",
    "         \"IndepInfer\": get_structured_auto_regressive_indep_graph,\n",
    "         \"QKV\": get_qkv_graph,\n",
    "         \"HQKV\": get_hqkv_graph}[flags.graph]\n",
    "if flags.graph == \"NormalLSTM\":\n",
    "    flags.encoder_h = int(flags.encoder_h/k*klstm)\n",
    "if flags.graph == \"Vanilla\":\n",
    "    flags.n_latents = [flags.z_size]\n",
    "if flags.losses == \"LagVAE\":\n",
    "    flags.anneal_kl0 = 0\n",
    "    flags.anneal_kl1 = 0\n",
    "Data = {\"nli\": NLIGenData2, \"ontonotes\": OntoGenData, \"yelp\": HuggingYelp2, \"paranmt\": ParaNMTCuratedData}[flags.data]\n",
    "MAX_LEN = flags.max_len\n",
    "BATCH_SIZE = flags.batch_size\n",
    "GRAD_ACCU = flags.grad_accu\n",
    "N_EPOCHS = flags.n_epochs\n",
    "TEST_FREQ = flags.test_freq\n",
    "COMPLETE_TEST_FREQ = flags.complete_test_freq\n",
    "DEVICE = device(flags.device)\n",
    "# This prevents illegal memory access on multigpu machines (unresolved issue on torch's github)\n",
    "if flags.device.startswith('cuda'):\n",
    "    torch.cuda.set_device(int(flags.device[-1]))\n",
    "LOSSES = {'IWAE': [IWLBo],\n",
    "          'VAE': [ELBo],\n",
    "          'LagVAE': [ELBo]}[flags.losses]\n",
    "\n",
    "ANNEAL_KL = [flags.anneal_kl0*flags.grad_accu, flags.anneal_kl1*flags.grad_accu]\n",
    "LOSS_PARAMS = [1]\n",
    "if flags.grad_accu > 1:\n",
    "    LOSS_PARAMS = [w/flags.grad_accu for w in LOSS_PARAMS]\n",
    "\n",
    "\n",
    "data = Data(MAX_LEN, BATCH_SIZE, N_EPOCHS, DEVICE, pretrained=flags.pretrained_embeddings)\n",
    "h_params = HParams(len(data.vocab.itos), len(data.tags.itos) if flags.data == 'yelp' else None, MAX_LEN, BATCH_SIZE, N_EPOCHS,\n",
    "                   device=DEVICE, vocab_ignore_index=data.vocab.stoi['<pad>'], decoder_h=flags.decoder_h,\n",
    "                   decoder_l=flags.decoder_l, encoder_h=flags.encoder_h, encoder_l=flags.encoder_l,\n",
    "                   text_rep_h=flags.text_rep_h, text_rep_l=flags.text_rep_l,\n",
    "                   test_name=flags.test_name, grad_accumulation_steps=GRAD_ACCU,\n",
    "                   optimizer_kwargs={'lr': flags.lr, #'weight_decay': flags.l2_reg, 't0':100, 'lambd':0.},\n",
    "                                     'weight_decay': flags.l2_reg, 'betas': (0.9, 0.99)},\n",
    "                   is_weighted=[], graph_generator=GRAPH,\n",
    "                   z_size=flags.z_size, embedding_dim=flags.embedding_dim, anneal_kl=ANNEAL_KL,\n",
    "                   grad_clip=flags.grad_clip*flags.grad_accu, kl_th=flags.kl_th, highway=flags.highway,\n",
    "                   losses=LOSSES, dropout=flags.dropout, training_iw_samples=flags.training_iw_samples,\n",
    "                   testing_iw_samples=flags.testing_iw_samples, loss_params=LOSS_PARAMS, optimizer=optim.AdamW,\n",
    "                   markovian=flags.markovian, word_dropout=flags.word_dropout, contiguous_lm=False,\n",
    "                   test_prior_samples=flags.test_prior_samples, n_latents=flags.n_latents, n_keys=flags.n_keys,\n",
    "                   max_elbo=[flags.max_elbo_choice, flags.max_elbo1],  # max_elbo is paper's beta\n",
    "                   z_emb_dim=flags.z_emb_dim, minimal_enc=flags.minimal_enc, kl_beta=flags.kl_beta,\n",
    "                   kl_beta_zs=flags.kl_beta_zs, kl_beta_zg=flags.kl_beta_zg, anneal_kl_type=flags.anneal_kl_type)\n",
    "val_iterator = iter(data.val_iter)\n",
    "print(\"Words: \", len(data.vocab.itos), \", On device: \", DEVICE.type)\n",
    "print(\"Loss Type: \", flags.losses)\n",
    "if flags.losses == 'LagVAE':\n",
    "    model = LaggingDisentanglementTransformerVAE(data.vocab, data.tags, h_params, wvs=data.wvs, dataset=flags.data,\n",
    "                                                 enc_iter=data.enc_train_iter)\n",
    "else:\n",
    "    model = DisentanglementTransformerVAE(data.vocab, data.tags, h_params, wvs=data.wvs, dataset=flags.data)\n",
    "if DEVICE.type == 'cuda':\n",
    "    model.cuda(DEVICE)\n",
    "\n",
    "total_unsupervised_train_samples = len(data.train_iter)*BATCH_SIZE\n",
    "total_unsupervised_val_samples = len(data.val_iter)*(BATCH_SIZE/data.divide_bs)\n",
    "print(\"Unsupervised training examples: \", total_unsupervised_train_samples)\n",
    "print(\"Unsupervised val examples: \", total_unsupervised_val_samples)\n",
    "current_time = time()\n",
    "#print(model)\n",
    "number_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Number of parameters: \", \"{0:05.2f} M\".format(number_parameters/1e6))\n",
    "number_parameters = sum(p.numel() for p in model.infer_bn.parameters() if p.requires_grad)\n",
    "print(\"Inference parameters: \", \"{0:05.2f} M\".format(number_parameters/1e6))\n",
    "number_parameters = sum(p.numel() for p in model.gen_bn.parameters() if p.requires_grad)\n",
    "print(\"Generation parameters: \", \"{0:05.2f} M\".format(number_parameters/1e6))\n",
    "number_parameters = sum(p.numel() for p in model.word_embeddings.parameters() if p.requires_grad)\n",
    "print(\"Embedding parameters: \", \"{0:05.2f} M\".format(number_parameters/1e6))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_metric\n",
    "from tqdm import tqdm\n",
    "\n",
    "bleu_score = load_metric(\"bleu\").compute\n",
    "\n",
    "def get_paraphrase_bleu(model, iterator):\n",
    "    with torch.no_grad():\n",
    "        orig, para, orig_mod, para_mod, rec = [], [], [], [], []\n",
    "        zs_infer, z_infer, x_gen = model.infer_bn.name_to_v['zs'], \\\n",
    "                                      {'z{}'.format(i+1):model.infer_bn.name_to_v['z{}'.format(i+1)]\n",
    "                                       for i in range(len(model.h_params.n_latents))}, \\\n",
    "                                   model.gen_bn.name_to_v['x']\n",
    "\n",
    "        go_symbol = torch.ones((1, 1)).long() * model.index[model.generated_v].stoi['<go>']\n",
    "        go_symbol = go_symbol.to(model.h_params.device)\n",
    "        temp = 1.\n",
    "        for i, batch in enumerate(tqdm(iterator, desc=\"Getting Model Paraphrase Bleu stats\")):\n",
    "            if batch.text.shape[1] < 2: continue\n",
    "            # if i > 1: break\n",
    "\n",
    "            # get source and target sentence latent variable values\n",
    "            model.infer_bn({'x': batch.text[..., 1:]})\n",
    "            orig_zs, orig_z = zs_infer.post_params['loc'][..., 0, :], \\\n",
    "                                       {k: v.post_params['loc'][..., 0, :] for k, v in z_infer.items()}\n",
    "            model.infer_bn({'x': batch.para[..., 1:]})\n",
    "            para_zs, para_z = zs_infer.post_params['loc'][..., 0, :],\\\n",
    "                                       {k: v.post_params['loc'][..., 0, :] for k, v in z_infer.items()}\n",
    "\n",
    "            # generate source and target reconstructions with the latent variable swap\n",
    "            # Inputs: 1) original sentence to be reconstructed,\n",
    "            #         2) original sentence with the paraphrase's structure\n",
    "            #         3) paraphrase with the original sentence's content\n",
    "            z_input = {'zs': torch.cat([orig_zs, orig_zs, para_zs]).unsqueeze(1),\n",
    "                       **{k: torch.cat([orig_z[k], para_z[k], orig_z[k]]).unsqueeze(1) for k in para_z.keys()}}\n",
    "            x_prev = go_symbol.repeat((para_zs.shape[0]*3, 1))\n",
    "            for i in range(model.h_params.max_len):\n",
    "                model.gen_bn({'x_prev': x_prev, **{k: v.expand(v.shape[0], i + 1, v.shape[-1])\n",
    "                                                  for k, v in z_input.items()}}, target=x_gen)\n",
    "                samples_i = model.generated_v.post_params['logits']\n",
    "                x_prev = torch.cat([x_prev, torch.argmax(samples_i, dim=-1)[..., -1].unsqueeze(-1)],\n",
    "                                   dim=-1)\n",
    "\n",
    "            # store original sentences, the 2 resulting \"paraphrases\", and the reconstruction of the original\n",
    "            text = model.decode_to_text2(x_prev, model.h_params.vocab_size,\n",
    "                                         model.index[model.generated_v])\n",
    "            rec_i, para_mod_i, orig_mod_i = text[:int(len(text)/3)], text[int(len(text)/3):int(len(text)*2/3)], \\\n",
    "                                            text[int(len(text)*2/3):]\n",
    "            orig_i = model.decode_to_text2(batch.text[..., 1:], model.h_params.vocab_size, \n",
    "                                          model.index[model.generated_v])\n",
    "            para_i = model.decode_to_text2(batch.para[..., 1:], model.h_params.vocab_size,\n",
    "                                          model.index[model.generated_v])\n",
    "            orig.extend([[o.split()] for o in orig_i])\n",
    "            para.extend([[p.split()] for p in para_i])\n",
    "            orig_mod.extend([o.split() for o in orig_mod_i])\n",
    "            para_mod.extend([p.split() for p in para_mod_i])\n",
    "            rec.extend([r.split() for r in rec_i])\n",
    "        # for o, r, pm, om in zip(orig, rec, para_mod, orig_mod):\n",
    "        #     print([' '.join(o[0]), '|||',  ' '.join(r), '|||',  ' '.join(pm), '|||',  ' '.join(om)])\n",
    "        # Calculate the 3 bleu scores\n",
    "        orig_mod_bleu = bleu_score(predictions=orig_mod, references=para)['bleu']\n",
    "        para_mod_bleu = bleu_score(predictions=para_mod, references=orig)['bleu']\n",
    "        rec_bleu = bleu_score(predictions=rec, references=orig)['bleu']\n",
    "\n",
    "        return orig_mod_bleu, para_mod_bleu, rec_bleu\n",
    "    \n",
    "def get_reconstructions(model, sens, beam_size=1, mask_unk=False):\n",
    "    with torch.no_grad():\n",
    "        zs_infer, z_infer, x_gen = model.infer_bn.name_to_v['zs'], \\\n",
    "                                      {'z{}'.format(i+1):model.infer_bn.name_to_v['z{}'.format(i+1)]\n",
    "                                       for i in range(len(model.h_params.n_latents))}, \\\n",
    "                                   model.gen_bn.name_to_v['x']\n",
    "\n",
    "        go_symbol = torch.ones((1, 1)).long() * model.index[model.generated_v].stoi['<go>']\n",
    "        go_symbol = go_symbol.to(model.h_params.device)\n",
    "        \n",
    "        model.infer_bn({'x': sens})\n",
    "        orig_zs, orig_z = zs_infer.post_params['loc'][..., 0, :], \\\n",
    "                                   {k: v.post_params['loc'][..., 0, :] for k, v in z_infer.items()}\n",
    "        \n",
    "        z_input = {'zs': torch.cat([orig_zs]).unsqueeze(1),\n",
    "                   **{k: torch.cat([orig_z[k]]).unsqueeze(1) for k in orig_z.keys()}}\n",
    "        x_prev = go_symbol.repeat((orig_zs.shape[0], 1))\n",
    "        # for i in range(model.h_params.max_len):\n",
    "        #     model.gen_bn({'x_prev': x_prev, **{k: v.expand(v.shape[0], i + 1, v.shape[-1])\n",
    "        #                                       for k, v in z_input.items()}}, target=x_gen)\n",
    "        #     samples_i = model.generated_v.post_params['logits']\n",
    "        #     x_prev = torch.cat([x_prev, torch.argmax(samples_i, dim=-1)[..., -1].unsqueeze(-1)],\n",
    "        #                        dim=-1)\n",
    "        x_prev = generate_from_z(model, z_input, x_prev,\n",
    "                                 only_z_sampling=True, temp=1.0, mask_unk=mask_unk,\n",
    "                                 beam_size=beam_size)\n",
    "        if beam_size>1:\n",
    "            x_prev = x_prev[0:int(x_prev.shape[0]/beam_size)]\n",
    "\n",
    "        rec = model.decode_to_text2(x_prev, model.h_params.vocab_size,\n",
    "                                     model.index[model.generated_v])\n",
    "        orig = model.decode_to_text2(sens, model.h_params.vocab_size, \n",
    "                                          model.index[model.generated_v])\n",
    "\n",
    "        return rec, orig\n",
    "    \n",
    "\n",
    "def generate_from_z(model, z_input, x_prev, gen_len=None, only_z_sampling=True, temp=1.0, \n",
    "                    mask_unk=True):\n",
    "    unk_mask = torch.ones(x_prev.shape[0], 1, \n",
    "                          model.h_params.vocab_size).long().to(model.h_params.device)\n",
    "    if mask_unk:\n",
    "        unk_mask[..., model.index[model.generated_v].stoi['<unk>']] = 0\n",
    "\n",
    "    for i in range(gen_len or model.h_params.max_len):\n",
    "        model.gen_bn({'x_prev': x_prev, **{k: v.expand(v.shape[0], i + 1, v.shape[-1])\n",
    "                                          for k, v in z_input.items()}})\n",
    "        unk_mask_i = unk_mask.expand(unk_mask.shape[0], i + 1, unk_mask.shape[-1])\n",
    "        if only_z_sampling:\n",
    "            samples_i = model.generated_v.post_params['logits']\n",
    "        else:\n",
    "            samples_i = model.generated_v.posterior(logits=model.generated_v.post_params['logits'],\n",
    "                                                   temperature=temp).rsample()\n",
    "        x_prev = torch.cat([x_prev, \n",
    "                            torch.argmax(samples_i*unk_mask_i, dim=-1)[..., -1].unsqueeze(-1)],\n",
    "                           dim=-1)\n",
    "    return x_prev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 20])\n"
     ]
    }
   ],
   "source": [
    "sens = next(iter(data.val_iter))\n",
    "print(sens.text.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n you ' re afraid you ' re gon na get caught  ===>  i ' m not gon ' re gon na get out -> i ' m gon na do n ' t get any chance -> i ' m gon na i ' m gon na get out \n our theme today will be mental health  ===>  the <unk> <unk> , will be to <unk> -> the <unk> of <unk> will be to change -> the <unk> of <unk> will be to change \n it is not gon na be very easy  ===>  it ' s not gon na be easy easy -> it ' s not gon na be easy easy -> it ' s not gon na be easy easy \n looks like you got our <unk> on your religion  ===>  i ' m not going to your own your ass -> i ' m gon na make your own your ass -> i ' m gon na make your own your ass \n did she open the door and leave no prints here  ===>  what ' s the <unk> of the <unk> to leave -> you ' ve got the <unk> on here to the doors -> you ' ve got the <unk> on here to the doors \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def generate_from_z(model, z_input, x_prev, gen_len=None, only_z_sampling=True, temp=1.0, \n",
    "                    mask_unk=True, beam_size=1):\n",
    "    eos_idx = (model.index[model.generated_v].stoi[\"?\"], \n",
    "               model.index[model.generated_v].stoi[\"!\"], \n",
    "               model.index[model.generated_v].stoi[\".\"],\n",
    "               model.index[model.generated_v].stoi[\"<eos>\"])\n",
    "    unk_mask = torch.ones(x_prev.shape[0], 1, \n",
    "                          model.h_params.vocab_size).long().to(model.h_params.device)\n",
    "    if mask_unk:\n",
    "        unk_mask[..., model.index[model.generated_v].stoi['<unk>']] = 0\n",
    "    ended = [False]*x_prev.shape[0]\n",
    "    seq_scores = torch.tensor([[0.0]*x_prev.shape[0]]*beam_size).to(x_prev.device)\n",
    "    if beam_size > 1:\n",
    "        z_input = {k: v.unsqueeze(0).expand(beam_size, v.shape[0], 1, v.shape[-1])\n",
    "                                              for k, v in z_input.items()}\n",
    "        x_prev = x_prev.unsqueeze(0).expand(beam_size, *x_prev.shape)\n",
    "        unk_mask = unk_mask.unsqueeze(0).expand(beam_size, *unk_mask.shape)\n",
    "\n",
    "    for i in range(gen_len or model.h_params.max_len):\n",
    "        if beam_size == 1:\n",
    "            z_i = {k: v.expand(v.shape[0], i + 1, v.shape[-1])\n",
    "                                              for k, v in z_input.items()}\n",
    "        else:\n",
    "            z_i = {k: v.expand(beam_size, v.shape[1], i + 1, v.shape[-1])\n",
    "                                              for k, v in z_input.items()}\n",
    "        model.gen_bn({'x_prev': x_prev, **z_i})\n",
    "        unk_mask_i = unk_mask.expand(*unk_mask.shape[:-2], i + 1, unk_mask.shape[-1])\n",
    "        if only_z_sampling:\n",
    "            samples_i = model.generated_v.post_params['logits']\n",
    "        else:\n",
    "            samples_i = model.generated_v.posterior(logits=model.generated_v.post_params['logits'],\n",
    "                                                   temperature=temp).rsample()\n",
    "        if beam_size == 1:\n",
    "            best_toks = torch.argmax(samples_i*unk_mask_i, dim=-1)\n",
    "            x_prev = torch.cat([x_prev, \n",
    "                                best_toks[..., -1].unsqueeze(-1)],\n",
    "                               dim=-1)\n",
    "        else:\n",
    "            next_xprev = torch.zeros((x_prev.shape[0], \n",
    "                                      x_prev.shape[1], x_prev.shape[2]+1)).long().to(x_prev.device)\n",
    "            for j in range(x_prev.shape[1]):\n",
    "                if any([idx in eos_idx for idx in x_prev[0, j]]) or ended[j]:\n",
    "                    next_xprev[:, j] = torch.cat([x_prev[:, j], \n",
    "                                                  x_prev[:, j, -1:]*0+eos_idx[-1]], dim=-1)\n",
    "                    ended[j] = True\n",
    "                    continue\n",
    "                if i==0:\n",
    "                    sample_ij = samples_i[0, j, -1].reshape(-1)*unk_mask_i[0, j, -1].reshape(-1)\n",
    "                else:\n",
    "                    sample_ij = (samples_i[:, j, -1]+seq_scores[:, j].unsqueeze(-1)).reshape(-1)\\\n",
    "                                *unk_mask_i[:, j, -1].reshape(-1)\n",
    "                                \n",
    "                tk = torch.topk(sample_ij, \n",
    "                                k=beam_size, dim=-1)\n",
    "                vocab_size = model.h_params.vocab_size\n",
    "                b_idx, w_idx = tk.indices.floor_divide(vocab_size), tk.indices % vocab_size\n",
    "                seq_scores[:, j] = seq_scores[b_idx, j]+tk.values\n",
    "                # print(j, \"-->\", b_idx, w_idx)\n",
    "                next_xprev[:, j] = torch.cat([x_prev[b_idx, j], w_idx.unsqueeze(-1)], dim=-1)\n",
    "            x_prev = next_xprev\n",
    "        # print(seq_scores)\n",
    "                    \n",
    "            # best_toks = torch.topk(samples_i*unk_mask_i, k=beam_size, dim=-1).indices\n",
    "    if beam_size>1:\n",
    "        x_prev = x_prev.view(x_prev.shape[0]*x_prev.shape[1], x_prev.shape[2])\n",
    "    return x_prev\n",
    "start, end, bs =40, 45, 5 \n",
    "rec1, orig = get_reconstructions(model, sens.text[start:end], beam_size=1, mask_unk=False)\n",
    "rec2, orig = get_reconstructions(model, sens.text[start:end], beam_size=5, mask_unk=False)\n",
    "rec3, orig = get_reconstructions(model, sens.text[start:end], beam_size=100, mask_unk=False)\n",
    "print(\"=========================\")\n",
    "for i in range(len(orig)):\n",
    "    print(orig[i], '===>', '->'.join([rec1[i], rec2[i], rec3[i]]))\n",
    "    # print(\"--------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 2., 0.],\n         [1., 2., 0.]],\n\n        [[0., 1., 1.],\n         [0., 0., 1.]]])\n1.6666666666666667 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a = torch.Tensor([[[1, 2, 3], [4, 5, 6]],\n",
    "                 [[0, 4, -5], [3, 9, 1]]])\n",
    "print(a%3)\n",
    "print(5/3, 5//3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
