{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Instanciating the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\ghazy\\AppData\\Local\\Temp\\tmp8n244ixv\\config.json as plain json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 443259  examples. statistics:\n -words: 8.881732801815643+-3.6417630572571147(quantiles(0.5, 0.7, 0.9, 0.95, 0.99:9.0,11.0,14.0,15.0,15.0)\nDataset has 4000  examples. statistics:\n -words: 8.9255+-3.668371539252806(quantiles(0.5, 0.7, 0.9, 0.95, 0.99:9.0,11.0,14.0,15.0,15.0)\nDataset has 1000  examples. statistics:\n -words: 10.325+-2.8399603870476784(quantiles(0.5, 0.7, 0.9, 0.95, 0.99:10.0,12.0,14.0,15.0,15.0)\ndata loading took 5.414997816085815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words:  9600 , On device:  cuda\nLoss Type:  VAE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model at step 69260\nUnsupervised training examples:  443264\nUnsupervised val examples:  42752\nNumber of parameters:  23.33 M\nInference parameters:  03.52 M\nGeneration parameters:  21.04 M\nEmbedding parameters:  01.23 M\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "from torch import device\n",
    "import torch\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "\n",
    "from data_prep import NLIGenData2, OntoGenData, HuggingYelp2\n",
    "from disentanglement_qkv.models import DisentanglementTransformerVAE, LaggingDisentanglementTransformerVAE\n",
    "from disentanglement_qkv.h_params import DefaultTransformerHParams as HParams\n",
    "from disentanglement_qkv.graphs import *\n",
    "from components.criteria import *\n",
    "parser = argparse.ArgumentParser()\n",
    "from torch.nn import MultiheadAttention\n",
    "# Training and Optimization\n",
    "k, kz, klstm = 1, 8, 2\n",
    "parser.add_argument(\"--test_name\", default='unnamed', type=str)\n",
    "parser.add_argument(\"--data\", default='nli', choices=[\"nli\", \"ontonotes\", \"yelp\"], type=str)\n",
    "parser.add_argument(\"--csv_out\", default='disentFinal.csv', type=str)\n",
    "parser.add_argument(\"--max_len\", default=17, type=int)\n",
    "parser.add_argument(\"--batch_size\", default=128, type=int)\n",
    "parser.add_argument(\"--grad_accu\", default=1, type=int)\n",
    "parser.add_argument(\"--n_epochs\", default=20, type=int)\n",
    "parser.add_argument(\"--test_freq\", default=32, type=int)\n",
    "parser.add_argument(\"--complete_test_freq\", default=160, type=int)\n",
    "parser.add_argument(\"--generation_weight\", default=1, type=float)\n",
    "parser.add_argument(\"--device\", default='cuda:0', choices=[\"cuda:0\", \"cuda:1\", \"cuda:2\", \"cpu\"], type=str)\n",
    "parser.add_argument(\"--embedding_dim\", default=128, type=int)#################\"\n",
    "parser.add_argument(\"--pretrained_embeddings\", default=False, type=bool)#################\"\n",
    "parser.add_argument(\"--z_size\", default=96*kz, type=int)#################\"\n",
    "parser.add_argument(\"--z_emb_dim\", default=192*k, type=int)#################\"\n",
    "parser.add_argument(\"--n_latents\", default=[16, 16, 16], nargs='+', type=int)#################\"\n",
    "parser.add_argument(\"--text_rep_l\", default=3, type=int)\n",
    "parser.add_argument(\"--text_rep_h\", default=192*k, type=int)\n",
    "parser.add_argument(\"--encoder_h\", default=192*k, type=int)#################\"\n",
    "parser.add_argument(\"--encoder_l\", default=2, type=int)#################\"\n",
    "parser.add_argument(\"--decoder_h\", default=192*k, type=int)\n",
    "parser.add_argument(\"--decoder_l\", default=2, type=int)#################\"\n",
    "parser.add_argument(\"--highway\", default=False, type=bool)\n",
    "parser.add_argument(\"--markovian\", default=True, type=bool)\n",
    "parser.add_argument('--minimal_enc', dest='minimal_enc', action='store_true')\n",
    "parser.add_argument('--no-minimal_enc', dest='minimal_enc', action='store_false')\n",
    "parser.set_defaults(minimal_enc=False)\n",
    "parser.add_argument(\"--losses\", default='VAE', choices=[\"VAE\", \"IWAE\" \"LagVAE\"], type=str)\n",
    "parser.add_argument(\"--graph\", default='Normal', choices=[\"Vanilla\", \"Discrete\", \"IndepInfer\", \"Normal\", \"NormalConGen\",\n",
    "                                                          \"NormalSimplePrior\", \"Normal2\",  \"NormalLSTM\"], type=str)\n",
    "parser.add_argument(\"--training_iw_samples\", default=1, type=int)\n",
    "parser.add_argument(\"--testing_iw_samples\", default=5, type=int)\n",
    "parser.add_argument(\"--test_prior_samples\", default=10, type=int)\n",
    "parser.add_argument(\"--anneal_kl0\", default=3000, type=int)\n",
    "parser.add_argument(\"--anneal_kl1\", default=6000, type=int)\n",
    "parser.add_argument(\"--grad_clip\", default=5., type=float)\n",
    "parser.add_argument(\"--kl_th\", default=0/(768*k/2), type=float or None)\n",
    "parser.add_argument(\"--max_elbo1\", default=6.0, type=float)\n",
    "parser.add_argument(\"--max_elbo2\", default=4.0, type=float)\n",
    "parser.add_argument(\"--max_elbo_choice\", default=10, type=int)\n",
    "parser.add_argument(\"--kl_beta\", default=0.4, type=float)\n",
    "parser.add_argument(\"--dropout\", default=0.3, type=float)\n",
    "parser.add_argument(\"--word_dropout\", default=0.1, type=float)\n",
    "parser.add_argument(\"--l2_reg\", default=0, type=float)\n",
    "parser.add_argument(\"--lr\", default=2e-4, type=float)\n",
    "parser.add_argument(\"--lr_reduction\", default=4., type=float)\n",
    "parser.add_argument(\"--wait_epochs\", default=1, type=float)\n",
    "parser.add_argument(\"--save_all\", default=True, type=bool)\n",
    "\n",
    "flags, _ = parser.parse_known_args()\n",
    "\n",
    "# Manual Settings, Deactivate before pushing\n",
    "if True:\n",
    "    flags.batch_size = 128\n",
    "    flags.grad_accu = 1\n",
    "    flags.max_len = 17\n",
    "    flags.test_name = \"nliLM/YelpWide_beta0.4.5\"\n",
    "    flags.data = \"yelp\"\n",
    "    flags.n_latents = [8]\n",
    "    flags.graph =\"IndepInfer\"  # \"Vanilla\"\n",
    "    # flags.losses = \"LagVAE\"\n",
    "    # flags.kl_beta = 0.5\n",
    "    # flags.z_size = 16\n",
    "    # flags.encoder_h = 256\n",
    "    # flags.decoder_h = 256\n",
    "\n",
    "\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "GRAPH = {\"Vanilla\": get_vanilla_graph,\n",
    "         \"Discrete\": get_discrete_auto_regressive_graph,\n",
    "         \"IndepInfer\": get_structured_auto_regressive_indep_graph,\n",
    "         \"Normal\": get_structured_auto_regressive_graph,\n",
    "         \"NormalConGen\": get_structured_auto_regressive_graphConGen,\n",
    "         \"Normal2\": get_structured_auto_regressive_graph2,\n",
    "         \"NormalLSTM\": get_lstm_graph,\n",
    "         \"NormalSimplePrior\": get_structured_auto_regressive_simple_prior}[flags.graph]\n",
    "if flags.graph == \"NormalLSTM\":\n",
    "    flags.encoder_h = int(flags.encoder_h/k*klstm)\n",
    "if flags.graph == \"Vanilla\":\n",
    "    flags.n_latents = [flags.z_size]\n",
    "if flags.losses == \"LagVAE\":\n",
    "    flags.anneal_kl0 = 0\n",
    "    flags.anneal_kl1 = 0\n",
    "Data = {\"nli\": NLIGenData2, \"ontonotes\": OntoGenData, \"yelp\": HuggingYelp2}[flags.data]\n",
    "MAX_LEN = flags.max_len\n",
    "BATCH_SIZE = flags.batch_size\n",
    "GRAD_ACCU = flags.grad_accu\n",
    "N_EPOCHS = flags.n_epochs\n",
    "TEST_FREQ = flags.test_freq\n",
    "COMPLETE_TEST_FREQ = flags.complete_test_freq\n",
    "DEVICE = device(flags.device)\n",
    "# This prevents illegal memory access on multigpu machines (unresolved issue on torch's github)\n",
    "if flags.device.startswith('cuda'):\n",
    "    torch.cuda.set_device(int(flags.device[-1]))\n",
    "LOSSES = {'IWAE': [IWLBo],\n",
    "          'VAE': [ELBo],\n",
    "          'LagVAE': [ELBo]}[flags.losses]\n",
    "\n",
    "ANNEAL_KL = [flags.anneal_kl0*flags.grad_accu, flags.anneal_kl1*flags.grad_accu]\n",
    "LOSS_PARAMS = [1]\n",
    "if flags.grad_accu > 1:\n",
    "    LOSS_PARAMS = [w/flags.grad_accu for w in LOSS_PARAMS]\n",
    "\n",
    "\n",
    "data = Data(MAX_LEN, BATCH_SIZE, N_EPOCHS, DEVICE, pretrained=flags.pretrained_embeddings)\n",
    "h_params = HParams(len(data.vocab.itos), len(data.tags.itos) if flags.data == 'yelp' else None, MAX_LEN, BATCH_SIZE, N_EPOCHS,\n",
    "                   device=DEVICE, vocab_ignore_index=data.vocab.stoi['<pad>'], decoder_h=flags.decoder_h,\n",
    "                   decoder_l=flags.decoder_l, encoder_h=flags.encoder_h, encoder_l=flags.encoder_l,\n",
    "                   text_rep_h=flags.text_rep_h, text_rep_l=flags.text_rep_l,\n",
    "                   test_name=flags.test_name, grad_accumulation_steps=GRAD_ACCU,\n",
    "                   optimizer_kwargs={'lr': flags.lr, #'weight_decay': flags.l2_reg, 't0':100, 'lambd':0.},\n",
    "                                     'weight_decay': flags.l2_reg, 'betas': (0.9, 0.99)},\n",
    "                   is_weighted=[], graph_generator=GRAPH,\n",
    "                   z_size=flags.z_size, embedding_dim=flags.embedding_dim, anneal_kl=ANNEAL_KL,\n",
    "                   grad_clip=flags.grad_clip*flags.grad_accu, kl_th=flags.kl_th, highway=flags.highway,\n",
    "                   losses=LOSSES, dropout=flags.dropout, training_iw_samples=flags.training_iw_samples,\n",
    "                   testing_iw_samples=flags.testing_iw_samples, loss_params=LOSS_PARAMS, optimizer=optim.AdamW,\n",
    "                   markovian=flags.markovian, word_dropout=flags.word_dropout, contiguous_lm=False,\n",
    "                   test_prior_samples=flags.test_prior_samples, n_latents=flags.n_latents,\n",
    "                   max_elbo=[flags.max_elbo_choice, flags.max_elbo1],  # max_elbo is paper's beta\n",
    "                   z_emb_dim=flags.z_emb_dim, minimal_enc=flags.minimal_enc, kl_beta=flags.kl_beta)\n",
    "val_iterator = iter(data.val_iter)\n",
    "print(\"Words: \", len(data.vocab.itos), \", On device: \", DEVICE.type)\n",
    "print(\"Loss Type: \", flags.losses)\n",
    "if flags.losses == 'LagVAE':\n",
    "    model = LaggingDisentanglementTransformerVAE(data.vocab, data.tags, h_params, wvs=data.wvs, dataset=flags.data,\n",
    "                                                 enc_iter=data.enc_train_iter)\n",
    "else:\n",
    "    model = DisentanglementTransformerVAE(data.vocab, data.tags, h_params, wvs=data.wvs, dataset=flags.data)\n",
    "if DEVICE.type == 'cuda':\n",
    "    model.cuda(DEVICE)\n",
    "\n",
    "total_unsupervised_train_samples = len(data.train_iter)*BATCH_SIZE\n",
    "total_unsupervised_val_samples = len(data.val_iter)*BATCH_SIZE\n",
    "print(\"Unsupervised training examples: \", total_unsupervised_train_samples)\n",
    "print(\"Unsupervised val examples: \", total_unsupervised_val_samples)\n",
    "current_time = time()\n",
    "#print(model)\n",
    "number_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Number of parameters: \", \"{0:05.2f} M\".format(number_parameters/1e6))\n",
    "number_parameters = sum(p.numel() for p in model.infer_bn.parameters() if p.requires_grad)\n",
    "print(\"Inference parameters: \", \"{0:05.2f} M\".format(number_parameters/1e6))\n",
    "number_parameters = sum(p.numel() for p in model.gen_bn.parameters() if p.requires_grad)\n",
    "print(\"Generation parameters: \", \"{0:05.2f} M\".format(number_parameters/1e6))\n",
    "number_parameters = sum(p.numel() for p in model.word_embeddings.parameters() if p.requires_grad)\n",
    "print(\"Embedding parameters: \", \"{0:05.2f} M\".format(number_parameters/1e6))\n",
    "min_perp = 1e20\n",
    "wait_count = 0\n",
    "loss = torch.tensor(1e20)\n",
    "mean_loss = 0\n",
    "stabilize_epochs = 0\n",
    "prev_mi = 0\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "      \n",
    "# import spacy\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "# predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/openie-model.2020.03.26.tar.gz\")\n",
    "# const_predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/elmo-constituency-parser-2020.02.10.tar.gz\")\n",
    "\n",
    "# def batch_sent_relations(sents):\n",
    "#     target = [{'sentence': sent} for sent in sents]\n",
    "#     preds = predictor.predict_batch_json(target)\n",
    "#     sent_dicts = []\n",
    "#     for pred in preds:\n",
    "#         sent_dict = {'ARG0': '', 'V': '', 'ARG1': '', 'ARG*': ''}\n",
    "#         if len(pred['verbs']):\n",
    "#             el = pred['verbs'][0]\n",
    "#             for v_i in el['description'].split('[')[1:]:\n",
    "#                 in_bracket = v_i.split(']')[0]\n",
    "#                 try:\n",
    "#                     arg_l, arg_str = in_bracket.split(':')\n",
    "#                     if arg_l in sent_dict:\n",
    "#                         sent_dict[arg_l] = arg_str\n",
    "#                     else:\n",
    "#                         sent_dict['ARG*'] = ''.join([sent_dict['ARG*'], arg_str])\n",
    "#                 except ValueError as e:\n",
    "#                     print('this raised an anomaly:', el)\n",
    "#         if sent_dict['ARG0'] == '':\n",
    "#             sent_dict['ARG0'] = sent_dict['ARG1']\n",
    "#             sent_dict['ARG1'] = ''\n",
    "#         sent_dicts.append(sent_dict)\n",
    "#     return sent_dicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "      \n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def shallow_constituents(sents, verbose=0):\n",
    "    target = [{'sentence': sent} for sent in sents]\n",
    "    preds = const_predictor.predict_batch_json(target)\n",
    "    outputs = []\n",
    "    for pred in preds:\n",
    "        root_c = pred['hierplane_tree']['root']['children']\n",
    "        subj, verb, np, pp = '', '', '', ''\n",
    "        parsing_error = False\n",
    "        try:\n",
    "            subj = [c['word'] for c in root_c if c['nodeType']=='NP'][0]\n",
    "            VP_c = [c for c in root_c if c['nodeType']=='VP'][0]['children']\n",
    "            if not any([c['nodeType'].startswith('VB') for c in VP_c]):\n",
    "                outputs.append({'subj':subj, 'verb':'', 'np':'', 'pp':''})\n",
    "                continue\n",
    "            verb = [c['word'] for c in VP_c if c['nodeType'].startswith('VB')][0]\n",
    "            \n",
    "            np = [c['word'] for c in VP_c if c['nodeType']=='NP'][0] if any([c['nodeType']=='NP' for c in VP_c]) else ''\n",
    "            pp = [c['word'] for c in VP_c if c['nodeType']=='PP'][0] if any([c['nodeType']=='PP' for c in VP_c]) else ''\n",
    "            if verbose: \n",
    "                print([[c['nodeType'],c['word']] for c in VP_c])\n",
    "            while any([c['nodeType'] == 'VP' for c in VP_c]):\n",
    "                VP_c = [c for c in VP_c if c['nodeType']=='VP'][0]['children']\n",
    "                if verbose: \n",
    "                    print([[c['nodeType'],c['word']] for c in VP_c])\n",
    "                verb += ' '+[c['word'] for c in VP_c if c['nodeType'].startswith('VB')][0]\n",
    "                if any([c['nodeType']=='NP' for c in VP_c]):\n",
    "                    for np_i in [c['word'] for c in VP_c if c['nodeType']=='NP']:\n",
    "                        np += ' '+np_i\n",
    "                if any([c['nodeType']=='PP' for c in VP_c]):\n",
    "                    for pp_i in [c['word'] for c in VP_c if c['nodeType']=='PP']:\n",
    "                        pp += ' '+pp_i\n",
    "        except IndexError:\n",
    "            parsing_error = True\n",
    "        outputs.append({'subj':subj, 'verb':verb, 'np':np, 'pp':pp, 'err': parsing_error})\n",
    "    return outputs\n",
    "\n",
    "def shallow_dependencies(sents):\n",
    "    docs = nlp.pipe(sents)\n",
    "    relations = []\n",
    "    for doc in docs:\n",
    "        subj, verb, dobj, pobj = ['', []], ['', []], ['', []], ['', []]\n",
    "        for i, tok in enumerate(doc):\n",
    "            if tok.dep_ =='ROOT':\n",
    "                verb = [tok.text, [tok.i]]\n",
    "            if tok.dep_ == 'nsubj' and subj[0] == '':\n",
    "                subj = [' '.join([toki.text for toki in tok.subtree]), [toki.i for toki in tok.subtree]]\n",
    "            if tok.dep_ == 'dobj' and dobj[0] == '':\n",
    "                dobj = [' '.join([toki.text for toki in tok.subtree]), [toki.i for toki in tok.subtree]]\n",
    "            if tok.dep_ == 'pobj' and pobj[0] == '':\n",
    "                pobj = [' '.join([toki.text for toki in tok.subtree]), [toki.i for toki in tok.subtree]]\n",
    "        relations.append({'text':{'subj': subj[0], 'verb': verb[0], 'dobj': dobj[0], 'pobj': pobj[0]},\n",
    "                         'idx':{'subj': subj[1], 'verb': verb[1], 'dobj': dobj[1], 'pobj': pobj[1]}})\n",
    "    return relations\n",
    "\n",
    "def get_sentence_statistics(orig, sen, orig_relations=None, relations=None):\n",
    "    same_struct = True\n",
    "    error = orig_relations.pop('err', None) or relations.pop('err', None)\n",
    "    for k in orig_relations.keys():\n",
    "        if (orig_relations[k] == '' and relations[k] != '') or (orig_relations[k] == '' and relations[k] != ''):\n",
    "            same_struct = False\n",
    "    def get_diff(arg):\n",
    "        if orig_relations[arg] != '' and relations[arg] != '':\n",
    "            return orig_relations[arg] != relations[arg], False\n",
    "        else: \n",
    "            return False, orig_relations[arg] != relations[arg]\n",
    "    return get_diff('subj'), get_diff('verb'), get_diff('np'), get_diff('pp'), same_struct, error\n",
    "    # return get_diff('ARG0'), get_diff('V'), get_diff('ARG1'), get_diff('ARG*'), same_struct\n",
    "\n",
    "def get_sentence_statistics2(orig, sen, orig_relations=None, relations=None):\n",
    "    orig_relations, relations = orig_relations['text'], relations['text']\n",
    "    same_struct = True\n",
    "    for k in orig_relations.keys():\n",
    "        if (orig_relations[k] == '' and relations[k] != '') or (orig_relations[k] == '' and relations[k] != ''):\n",
    "            same_struct = False\n",
    "    def get_diff(arg):\n",
    "        if orig_relations[arg] != '' and relations[arg] != '':\n",
    "            return orig_relations[arg] != relations[arg], False\n",
    "        else: \n",
    "            return False, orig_relations[arg] != relations[arg]\n",
    "    return get_diff('subj'), get_diff('verb'), get_diff('dobj'), get_diff('pobj'), same_struct\n",
    "\n",
    "\n",
    "\n",
    "def _get_stat_data_frame(model, n_samples=20, n_alterations=10, batch_size=10):\n",
    "    stats = []\n",
    "    nlatents = model.h_params.n_latents\n",
    "    # Generating n_samples sentences    \n",
    "    text, samples, _ = model.get_sentences(n_samples=batch_size, gen_len=model.h_params.max_len-1,\n",
    "                                                sample_w=False, vary_z=True, complete=None)\n",
    "    orig_rels = shallow_dependencies(text)\n",
    "    for _ in tqdm(range(int(n_samples / batch_size)), desc=\"Generating original sentences\"):\n",
    "        text_i, samples_i, _ = model.get_sentences(n_samples=batch_size, gen_len=model.h_params.max_len-1,\n",
    "                                                    sample_w=False, vary_z=True, complete=None)\n",
    "        text.extend(text_i)\n",
    "        for k in samples.keys():\n",
    "            samples[k] = torch.cat([samples[k], samples_i[k]])\n",
    "        orig_rels.extend(shallow_dependencies(text_i))\n",
    "    for i in range(int(n_samples / batch_size)):\n",
    "        for j in tqdm(range(sum(nlatents)), desc=\"Processing sample {}\".format(str(i))):\n",
    "            # Altering the sentences\n",
    "            alt_text, _ = model._get_alternative_sentences(\n",
    "                                                       prev_latent_vals={k: v[i * batch_size:(i + 1) * batch_size]\n",
    "                                                                         for k, v in samples.items()},\n",
    "                                                       params=None, var_z_ids=[j], n_samples=n_alterations,\n",
    "                                                       gen_len=model.h_params.max_len-1, complete=None)\n",
    "            alt_rels = shallow_dependencies(alt_text)\n",
    "            # Getting alteration statistics\n",
    "            for k in range(n_alterations * batch_size):\n",
    "                orig_text = text[(i * batch_size) + k % batch_size]\n",
    "                try:\n",
    "                    arg0_diff, v_diff, arg1_diff, arg_star_diff, same_struct = \\\n",
    "                        get_sentence_statistics2(orig_text, alt_text[k], orig_rels[(i * batch_size) + k % batch_size],\n",
    "                                                alt_rels[k])\n",
    "                except RecursionError or IndexError:\n",
    "                    continue\n",
    "                stats.append([orig_text, alt_text[k], j, int(arg0_diff[0]), int(v_diff[0]), \n",
    "                              int(arg1_diff[0]), int(arg_star_diff[0]), int(arg0_diff[1]), int(v_diff[1]), \n",
    "                              int(arg1_diff[1]), int(arg_star_diff[1]), same_struct])\n",
    "\n",
    "    header = ['original', 'altered', 'alteration_id', 'subj_diff', 'verb_diff', 'dobj_diff', 'pobj_diff',\n",
    "              'subj_struct', 'verb_struct', 'dobj_struct', 'pobj_struct', 'same_struct']\n",
    "    df = pd.DataFrame(stats, columns=header)\n",
    "    return df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_sent_relations' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-fa1182ff9c90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mex_sens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'The man is breathing'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"a group of people gave the boy a bike in summer\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mrels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_sent_relations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mex_sens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'ARG0'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'V'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ARG1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m':'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mrels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'batch_sent_relations' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "\n",
    "ex_sens = ['The man is breathing', \"a group of people gave the boy a bike in summer\"]\n",
    "rels = batch_sent_relations(ex_sens)\n",
    "print(rels)\n",
    "for arg in ['ARG0', 'V', 'ARG1']:\n",
    "    print(arg, ':', rels[0][arg]==rels[1][arg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 6.00 GiB total capacity; 497.50 MiB already allocated; 3.93 GiB free; 544.00 MiB reserved in total by PyTorch)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-67fb25ddea9b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_stat_data_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_alterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfer_bn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen_bn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-44355c0d27fc>\u001b[0m in \u001b[0;36m_get_stat_data_frame\u001b[1;34m(model, n_samples, n_alterations, batch_size)\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;31m# Generating n_samples sentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     text, samples, _ = model.get_sentences(n_samples=batch_size, gen_len=model.h_params.max_len-1,\n\u001b[1;32m---> 96\u001b[1;33m                                                 sample_w=False, vary_z=True, complete=None)\n\u001b[0m\u001b[0;32m     97\u001b[0m     \u001b[0morig_rels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshallow_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Generating original sentences\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Experiments\\GLUE_BENCH\\disentanglement_transformer\\models.py\u001b[0m in \u001b[0;36mget_sentences\u001b[1;34m(self, n_samples, gen_len, sample_w, vary_z, complete, contains, max_tries)\u001b[0m\n\u001b[0;32m    592\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m                 self.gen_bn({'x_prev': x_prev, **{k: v.expand(v.shape[0], i + 1, v.shape[-1])\n\u001b[1;32m--> 594\u001b[1;33m                                                  for k, v in z_input.items()}})\n\u001b[0m\u001b[0;32m    595\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msample_w\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m                     \u001b[0msamples_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerated_v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpost_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'logits'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Experiments\\GLUE_BENCH\\components\\bayesnets.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs, n_iw, target, eval, prev_states, force_iw, complete, lens, plant_posteriors)\u001b[0m\n\u001b[0;32m    164\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapproximator\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlv\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprev_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprev_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlv\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m                     lv(self.approximator[lv], lv_conditions, gt_samples=gt_lv, complete=(lv in self.child) or complete,\n\u001b[1;32m--> 166\u001b[1;33m                        lens=this_len)\n\u001b[0m\u001b[0;32m    167\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrep_net\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m                         \u001b[0mlv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprev_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapproximator\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlv\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Experiments\\GLUE_BENCH\\components\\latent_variables.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, link_approximator, inputs, prior, gt_samples, complete, lens)\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sequential_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlink_approximator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgt_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlink_approximator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgt_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomplete\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcomplete\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Experiments\\GLUE_BENCH\\components\\latent_variables.py\u001b[0m in \u001b[0;36m_forward\u001b[1;34m(self, link_approximator, inputs, prior, gt_samples, complete, lens)\u001b[0m\n\u001b[0;32m    268\u001b[0m                       torch.cat([v for k, v in inputs.items() if k not in link_approximator.residual['conditions']],\n\u001b[0;32m    269\u001b[0m                                 dim=-1))\n\u001b[1;32m--> 270\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpost_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlink_approximator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    271\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcomplete\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpost_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpost_log_probas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposterior_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpost_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Experiments\\GLUE_BENCH\\components\\links.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, z_prev, lens)\u001b[0m\n\u001b[0;32m    753\u001b[0m                 \u001b[0mz_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'logits'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msbn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'logits'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 755\u001b[1;33m                 \u001b[0mz_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'logits'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'logits'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    756\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'loc'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mz_params\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatchnorm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m             \u001b[0mout_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mz_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 6.00 GiB total capacity; 497.50 MiB already allocated; 3.93 GiB free; 544.00 MiB reserved in total by PyTorch)"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "df = _get_stat_data_frame(model, n_samples=2000, n_alterations=1, batch_size=100)\n",
    "model.infer_bn.clear_values()\n",
    "model.gen_bn.clear_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\nDisentanglement score 0.4224999999999999\nDisentanglement score 0.39999999999999997\nDisentanglement score 0.3625000000000001\nDisentanglement score 0.4574999999999999\nDisentanglement score 0.43500000000000005\n0.4154999999999999 0.0323805497173843\n               subj_diff  verb_diff  dobj_diff  pobj_diff\nalteration_id                                            \n0                 0.4305     0.8855     0.1885     0.2410\n1                 0.4125     0.6960     0.2260     0.3030\n2                 0.4045     0.6700     0.2180     0.2705\n3                 0.6635     0.7305     0.2045     0.2800\nDisentanglement score 0.4189999999999999\nNumber of fixed structure pairs: 4558\n               subj_diff  verb_diff  dobj_diff  pobj_diff\nalteration_id                                            \n0               0.436563   0.830170   0.331668   0.402597\n1               0.407807   0.584718   0.335548   0.438538\n2               0.415776   0.552999   0.314708   0.394412\n3               0.666373   0.607394   0.321303   0.428697\nsubj_diff 3 0.2298098028731832\nverb_diff 0 0.22277546397264703\ndobj_diff 1 0.00387984108914341\npobj_diff 1 0.00984102288147487\nDisentanglement score 0.4663061308164485\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "scores = []\n",
    "div_by = 5\n",
    "for i in range(div_by):\n",
    "    grouped_diff = df[int(i*len(df)/div_by):int((i+1)*len(df)/div_by)].groupby('alteration_id').mean()\n",
    "    [['subj_diff', 'verb_diff', 'dobj_diff', 'pobj_diff']]\n",
    "    # print(grouped_diff)#\n",
    "    disent_score = 0\n",
    "    for lab in ['subj_diff', 'verb_diff', 'dobj_diff', 'pobj_diff']:\n",
    "        top2 = np.array(grouped_diff.nlargest(2, lab)[lab])\n",
    "        diff = top2[0]-top2[1]\n",
    "        # print(lab, diff)\n",
    "        disent_score += diff\n",
    "    print(\"Disentanglement score\", disent_score)\n",
    "    scores.append(disent_score)\n",
    "print(np.mean(scores), np.std(scores))\n",
    "grouped_diff = df.groupby('alteration_id').mean()[['subj_diff', 'verb_diff', 'dobj_diff', 'pobj_diff']]\n",
    "print(grouped_diff)#\n",
    "disent_score = 0\n",
    "for lab in ['subj_diff', 'verb_diff', 'dobj_diff', 'pobj_diff']:\n",
    "    top2 = np.array(grouped_diff.nlargest(2, lab)[lab])\n",
    "    diff = top2[0]-top2[1]\n",
    "    # print(lab, diff)\n",
    "    disent_score += diff\n",
    "print(\"Disentanglement score\", disent_score)\n",
    "\n",
    "df_fix = df[df['same_struct']==True]\n",
    "print(\"Number of fixed structure pairs:\", len(df_fix))\n",
    "grouped_diff = df_fix.groupby('alteration_id').mean()[['subj_diff', 'verb_diff', 'dobj_diff', 'pobj_diff']]\n",
    "print(grouped_diff)#\n",
    "disent_score = 0\n",
    "for lab in ['subj_diff', 'verb_diff', 'dobj_diff', 'pobj_diff']:#, 'arg_star_diff']:\n",
    "    highest_idx = grouped_diff[lab].argmax()\n",
    "    top2 = np.array(grouped_diff.nlargest(2, lab)[lab])\n",
    "    diff = top2[0]-top2[1]\n",
    "    print(lab, highest_idx, diff)\n",
    "    disent_score += diff\n",
    "    #grouped_diff = grouped_diff.drop(highest_idx)\n",
    "print(\"Disentanglement score\", disent_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 4, 2, 18)\n"
     ]
    }
   ],
   "source": [
    "from components.links import CoattentiveTransformerLink, ConditionalCoattentiveTransformerLink\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def get_attention_weights(sentences, lvs):\n",
    "    # Encoding sentences\n",
    "    encoded = []\n",
    "    lens = []\n",
    "    for sen in sentences:\n",
    "        sen_enc = [data.vocab.stoi[w] for w in sen.split(' ')]\n",
    "        lens.append(min(len(sen_enc), MAX_LEN))\n",
    "        if len(sen_enc)>=MAX_LEN:\n",
    "            sen_enc = sen_enc[:MAX_LEN]\n",
    "        else:\n",
    "            sen_enc = sen_enc+[data.vocab.stoi['<pad>']]*(MAX_LEN-len(sen_enc))\n",
    "        encoded.append(sen_enc)\n",
    "    encoded = torch.Tensor(encoded).to(DEVICE).long()\n",
    "    lens = torch.Tensor(lens).to(DEVICE).long()\n",
    "    CoattentiveTransformerLink.get_att, ConditionalCoattentiveTransformerLink.get_att = True, True\n",
    "    model.infer_bn({'x': encoded}, lens=lens)\n",
    "    CoattentiveTransformerLink.get_att, ConditionalCoattentiveTransformerLink.get_att = False, False\n",
    "    all_att_weights = []\n",
    "    for i in range(len(h_params.n_latents)):\n",
    "        trans_mod = model.infer_bn.approximator[model.infer_bn.name_to_v['z{}'.format(i+1)]]\n",
    "        all_att_weights.append(trans_mod.att_vals)\n",
    "    att_weights = []\n",
    "    for lv in lvs:\n",
    "        var_att_weights = []\n",
    "        lv_layer = sum([lv > sum(h_params.n_latents[:i+1]) for i in range(len(h_params.n_latents))])\n",
    "        rank = lv - sum(h_params.n_latents[:lv_layer])\n",
    "        for layer_att_vals in all_att_weights[lv_layer]:\n",
    "            soft_att_vals = layer_att_vals\n",
    "            att_out = torch.cat([soft_att_vals[:, rank, :MAX_LEN], soft_att_vals[:, rank, MAX_LEN:].sum(-1).unsqueeze(-1)]\n",
    "                                , -1)\n",
    "            if lv_layer==2:\n",
    "                att_out[..., -1] *= 0\n",
    "            var_att_weights.append(att_out.cpu().detach().numpy())\n",
    "        att_weights.append(var_att_weights)\n",
    "    return np.transpose(np.array(att_weights), (2, 0, 1, 3))\n",
    "\n",
    "def display_attention(sentence, att_weights, variables):\n",
    "    toked = sentence.split(' ')\n",
    "    toked += ['<pad>']*(MAX_LEN-len(toked))+['<latent>']\n",
    "    index = []\n",
    "    for i in range(len(variables)):\n",
    "        for j in range(len(att_weights[i])):\n",
    "            index.append(str(variables[i])+'_'+str(j))\n",
    "    att_weights = att_weights.reshape(-1, MAX_LEN+1)\n",
    "    data = pd.DataFrame(att_weights#[:, :len(toked)].reshape((len(att_weights), len(toked)))\n",
    "                        , columns=toked, index=index)\n",
    "    sns_plot = plt.figure(figsize=(15, 2.8))\n",
    "    ax = plt.axes()\n",
    "    g = sns.heatmap(data, annot=True, yticklabels=True, ax=ax)\n",
    "    ax.set_title('')\n",
    "    for tick in g.get_xticklabels():\n",
    "        tick.set_color('black')\n",
    "    for tick in g.get_yticklabels():\n",
    "        tick.set_color('black')\n",
    "    g.set_ylim([0, len(index)])\n",
    "    #g.set_xticklabels(data.axes[1], rotation=55, ha=\"center\", labelcolor='white')\n",
    "    # g.get_figure()\n",
    "    # plt.show()\n",
    "    return sns_plot\n",
    "\n",
    "\n",
    "sentences = ['a man is standing .',\n",
    "             'two little girls are standing .',\n",
    "             \"a girl is holding a toy\", \n",
    "              \"a girl with a white hat is holding a toy .\", \n",
    "              \"a girl is playing in a street\", \"a girl with a black coat is playing in a street .\", \n",
    "             \"a girl and a boy are playing in the street .\",\n",
    "             'a group of people are sitting around a table .']\n",
    "vars = [0, 1, 2, 3]\n",
    "att_w = get_attention_weights(sentences, vars)\n",
    "print(att_w.shape)\n",
    "\n",
    "# for sen_idx in range(len(sentences)):\n",
    "#     sns_plot = display_attention(sentences[sen_idx], att_w[sen_idx], vars)\n",
    "#     sns_plot.savefig(\"att_{}.png\".format(sen_idx)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_att_and_rel_idx(text_in):\n",
    "    max_len = text_in.shape[-1]\n",
    "    text_sents = [' '.join([model.index[model.generated_v].itos[w] \n",
    "                            for w in s]).replace(' <pad>', '').replace(' <eos>', '')\n",
    "                  for s in text_in]\n",
    "    # Getting relations' positions\n",
    "    rel_idx = [out['idx'] for out in shallow_dependencies(text_sents)]\n",
    "    # Getting layer wise attention values\n",
    "    \n",
    "    CoattentiveTransformerLink.get_att, ConditionalCoattentiveTransformerLink.get_att = True, True\n",
    "    model.infer_bn({'x': text_in})\n",
    "    CoattentiveTransformerLink.get_att, ConditionalCoattentiveTransformerLink.get_att = False, False\n",
    "    all_att_weights = []\n",
    "    for i in range(len(h_params.n_latents)):\n",
    "        trans_mod = model.infer_bn.approximator[model.infer_bn.name_to_v['z{}'.format(i+1)]]\n",
    "        all_att_weights.append(trans_mod.att_vals)\n",
    "    att_weights = []\n",
    "    for lv in range(sum(model.h_params.n_latents)):\n",
    "        var_att_weights = []\n",
    "        lv_layer = sum([lv > sum(h_params.n_latents[:i+1]) for i in range(len(h_params.n_latents))])\n",
    "        rank = lv - sum(h_params.n_latents[:lv_layer])\n",
    "        for layer_att_vals in all_att_weights[lv_layer]:\n",
    "            soft_att_vals = layer_att_vals\n",
    "            att_out = torch.cat([soft_att_vals[:, rank,\n",
    "                                 :max_len], soft_att_vals[:, rank, max_len:].sum(-1).unsqueeze(-1)]\n",
    "                                , -1)\n",
    "            if lv_layer==2:\n",
    "                att_out[..., -1] *= 0\n",
    "            var_att_weights.append(att_out.cpu().detach().numpy())\n",
    "        att_weights.append(var_att_weights)\n",
    "    # att_vals shape:[sent, lv, layer, tok]\n",
    "    att_vals = np.transpose(np.array(att_weights), (2, 0, 1, 3)).mean(-2)\n",
    "    att_maxes = att_vals.argmax(-1).tolist()\n",
    "    return rel_idx, att_maxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   0%|          | 0/334 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   1%|          | 3/334 [00:00<00:15, 21.96it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   2%|         | 6/334 [00:00<00:14, 22.86it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   3%|         | 9/334 [00:00<00:15, 21.65it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   3%|         | 11/334 [00:00<00:16, 19.83it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   4%|         | 13/334 [00:00<00:17, 18.83it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   4%|         | 15/334 [00:00<00:17, 18.34it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   5%|         | 17/334 [00:00<00:17, 17.63it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   6%|         | 20/334 [00:01<00:17, 18.16it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   7%|         | 22/334 [00:01<00:16, 18.59it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   7%|         | 25/334 [00:01<00:15, 19.79it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   8%|         | 28/334 [00:01<00:14, 20.60it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   9%|         | 31/334 [00:01<00:14, 20.65it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  10%|         | 34/334 [00:01<00:14, 20.38it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  11%|         | 37/334 [00:01<00:15, 19.12it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  12%|        | 39/334 [00:01<00:15, 19.01it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  12%|        | 41/334 [00:02<00:15, 18.77it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  13%|        | 43/334 [00:02<00:15, 19.08it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  13%|        | 45/334 [00:02<00:15, 19.20it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  14%|        | 47/334 [00:02<00:14, 19.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  15%|        | 49/334 [00:02<00:14, 19.19it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  15%|        | 51/334 [00:02<00:15, 18.63it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  16%|        | 53/334 [00:02<00:15, 17.68it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  16%|        | 55/334 [00:02<00:16, 17.20it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  17%|        | 57/334 [00:02<00:16, 17.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  18%|        | 59/334 [00:03<00:15, 17.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  18%|        | 61/334 [00:03<00:15, 17.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  19%|        | 63/334 [00:03<00:15, 16.98it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  19%|        | 65/334 [00:03<00:15, 17.21it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  20%|        | 67/334 [00:03<00:16, 16.59it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  21%|        | 69/334 [00:03<00:15, 16.96it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  21%|       | 71/334 [00:03<00:15, 16.93it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  22%|       | 74/334 [00:03<00:14, 17.85it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  23%|       | 76/334 [00:04<00:14, 18.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  24%|       | 79/334 [00:04<00:13, 18.95it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  24%|       | 81/334 [00:04<00:13, 18.88it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  25%|       | 84/334 [00:04<00:13, 19.22it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  26%|       | 86/334 [00:04<00:13, 19.02it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  26%|       | 88/334 [00:04<00:12, 19.21it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  27%|       | 90/334 [00:04<00:13, 18.43it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  28%|       | 92/334 [00:04<00:13, 18.17it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  28%|       | 94/334 [00:05<00:13, 18.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  29%|       | 97/334 [00:05<00:12, 18.86it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  30%|       | 100/334 [00:05<00:11, 19.85it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  31%|       | 103/334 [00:05<00:11, 19.60it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  32%|      | 106/334 [00:05<00:11, 20.21it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  33%|      | 109/334 [00:05<00:10, 20.75it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  34%|      | 112/334 [00:05<00:10, 21.14it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  34%|      | 115/334 [00:06<00:10, 21.24it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  35%|      | 118/334 [00:06<00:10, 21.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  36%|      | 121/334 [00:06<00:09, 22.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  37%|      | 124/334 [00:06<00:09, 22.98it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  38%|      | 127/334 [00:06<00:08, 23.57it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  39%|      | 130/334 [00:06<00:08, 24.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  40%|      | 133/334 [00:06<00:08, 24.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  41%|      | 136/334 [00:06<00:08, 24.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  42%|     | 139/334 [00:06<00:07, 25.19it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  43%|     | 142/334 [00:07<00:07, 25.28it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  43%|     | 145/334 [00:07<00:07, 25.54it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  44%|     | 148/334 [00:07<00:07, 25.79it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  45%|     | 151/334 [00:07<00:07, 25.86it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  46%|     | 154/334 [00:07<00:06, 26.08it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  47%|     | 157/334 [00:07<00:06, 25.90it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  48%|     | 160/334 [00:07<00:06, 25.98it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  49%|     | 163/334 [00:07<00:06, 25.97it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  50%|     | 166/334 [00:08<00:06, 26.29it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  51%|     | 169/334 [00:08<00:06, 26.19it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  51%|    | 172/334 [00:08<00:06, 25.89it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  52%|    | 175/334 [00:08<00:06, 25.51it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  53%|    | 178/334 [00:08<00:06, 25.57it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  54%|    | 181/334 [00:08<00:05, 25.87it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  55%|    | 184/334 [00:08<00:05, 25.89it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  56%|    | 187/334 [00:08<00:05, 25.44it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  57%|    | 190/334 [00:08<00:05, 25.08it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  58%|    | 193/334 [00:09<00:05, 25.07it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  59%|    | 196/334 [00:09<00:05, 25.45it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  60%|    | 199/334 [00:09<00:05, 25.46it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  60%|    | 202/334 [00:09<00:05, 26.00it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  61%|   | 205/334 [00:09<00:04, 26.25it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  62%|   | 208/334 [00:09<00:04, 26.49it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  63%|   | 211/334 [00:09<00:04, 26.60it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  64%|   | 214/334 [00:09<00:04, 26.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  65%|   | 217/334 [00:09<00:04, 26.60it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  66%|   | 220/334 [00:10<00:04, 26.75it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  67%|   | 223/334 [00:10<00:04, 26.71it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  68%|   | 226/334 [00:10<00:04, 26.61it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  69%|   | 229/334 [00:10<00:03, 26.47it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  69%|   | 232/334 [00:10<00:03, 26.31it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  70%|   | 235/334 [00:10<00:03, 26.26it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  71%|  | 238/334 [00:10<00:03, 26.44it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  72%|  | 241/334 [00:10<00:03, 26.15it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  73%|  | 244/334 [00:11<00:03, 26.22it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  74%|  | 247/334 [00:11<00:03, 26.55it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  75%|  | 250/334 [00:11<00:03, 26.43it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  76%|  | 253/334 [00:11<00:03, 26.21it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  77%|  | 256/334 [00:11<00:02, 26.54it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  78%|  | 259/334 [00:11<00:02, 26.29it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  78%|  | 262/334 [00:11<00:02, 25.98it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  79%|  | 265/334 [00:11<00:02, 25.96it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  80%|  | 268/334 [00:11<00:02, 25.56it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  81%|  | 271/334 [00:12<00:02, 25.73it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  82%| | 274/334 [00:12<00:02, 25.93it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  83%| | 277/334 [00:12<00:02, 26.00it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  84%| | 280/334 [00:12<00:02, 25.84it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  85%| | 283/334 [00:12<00:01, 26.07it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  86%| | 286/334 [00:12<00:01, 26.32it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  87%| | 289/334 [00:12<00:01, 26.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  87%| | 292/334 [00:12<00:01, 26.26it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  88%| | 295/334 [00:12<00:01, 26.23it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  89%| | 298/334 [00:13<00:01, 26.35it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  90%| | 301/334 [00:13<00:01, 26.15it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  91%| | 304/334 [00:13<00:01, 26.22it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  92%|| 307/334 [00:13<00:01, 26.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  93%|| 310/334 [00:13<00:00, 26.47it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  94%|| 313/334 [00:13<00:00, 26.59it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  95%|| 316/334 [00:13<00:00, 26.60it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  96%|| 319/334 [00:13<00:00, 26.46it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  96%|| 322/334 [00:13<00:00, 26.58it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  97%|| 325/334 [00:14<00:00, 26.59it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  98%|| 328/334 [00:14<00:00, 26.25it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  99%|| 331/334 [00:14<00:00, 26.02it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy: 100%|| 334/334 [00:14<00:00, 27.09it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy: 100%|| 334/334 [00:14<00:00, 23.16it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rel_idx, att_maxes = [], []\n",
    "for i, batch in enumerate(tqdm(data.val_iter, desc=\"Getting model relationship accuracy\")):\n",
    "    rel_idx_i, att_maxes_i = get_att_and_rel_idx(batch.text[..., 1:])\n",
    "    rel_idx.extend(rel_idx_i)\n",
    "    att_maxes.extend(att_maxes_i)\n",
    "    \n",
    "    \n",
    "data.reinit_iterator('valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   0%|          | 0/417 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   1%|          | 3/417 [00:00<00:17, 23.69it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   1%|         | 6/417 [00:00<00:16, 24.67it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   2%|         | 9/417 [00:00<00:16, 24.49it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   3%|         | 12/417 [00:00<00:16, 24.78it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   3%|         | 14/417 [00:00<00:18, 22.00it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   4%|         | 17/417 [00:00<00:17, 22.84it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   5%|         | 20/417 [00:00<00:16, 23.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   6%|         | 23/417 [00:00<00:16, 23.67it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   6%|         | 26/417 [00:01<00:16, 23.35it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   7%|         | 29/417 [00:01<00:16, 23.89it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   8%|         | 32/417 [00:01<00:15, 24.12it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   8%|         | 35/417 [00:01<00:15, 24.57it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:   9%|         | 38/417 [00:01<00:15, 24.30it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  10%|         | 41/417 [00:01<00:16, 23.27it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  11%|         | 44/417 [00:01<00:16, 23.07it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  11%|        | 47/417 [00:01<00:15, 23.47it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  12%|        | 50/417 [00:02<00:15, 24.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  13%|        | 53/417 [00:02<00:15, 24.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  13%|        | 56/417 [00:02<00:14, 24.29it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  14%|        | 59/417 [00:02<00:14, 24.28it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  15%|        | 62/417 [00:02<00:15, 23.64it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  16%|        | 65/417 [00:02<00:14, 24.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  16%|        | 68/417 [00:02<00:14, 23.33it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  17%|        | 71/417 [00:02<00:15, 23.00it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  18%|        | 74/417 [00:03<00:15, 21.70it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  18%|        | 77/417 [00:03<00:16, 21.09it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  19%|        | 80/417 [00:03<00:15, 21.30it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  20%|        | 83/417 [00:03<00:15, 21.63it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  21%|        | 86/417 [00:03<00:14, 22.21it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  21%|       | 89/417 [00:03<00:15, 21.84it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  22%|       | 92/417 [00:03<00:14, 22.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  23%|       | 95/417 [00:04<00:14, 22.80it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  24%|       | 98/417 [00:04<00:14, 22.24it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  24%|       | 101/417 [00:04<00:14, 22.50it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  25%|       | 104/417 [00:04<00:13, 23.21it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  26%|       | 107/417 [00:04<00:12, 23.85it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  26%|       | 110/417 [00:04<00:12, 24.68it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  27%|       | 113/417 [00:04<00:12, 25.17it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  28%|       | 116/417 [00:04<00:11, 25.46it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  29%|       | 119/417 [00:05<00:11, 25.53it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  29%|       | 122/417 [00:05<00:11, 25.20it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  30%|       | 125/417 [00:05<00:11, 25.03it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  31%|       | 128/417 [00:05<00:11, 25.30it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  31%|      | 131/417 [00:05<00:11, 25.10it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  32%|      | 134/417 [00:05<00:10, 25.73it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  33%|      | 137/417 [00:05<00:10, 26.27it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  34%|      | 140/417 [00:05<00:10, 26.30it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  34%|      | 143/417 [00:05<00:10, 26.33it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  35%|      | 146/417 [00:06<00:10, 26.70it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  36%|      | 149/417 [00:06<00:09, 26.89it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  36%|      | 152/417 [00:06<00:09, 26.81it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  37%|      | 155/417 [00:06<00:09, 27.19it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  38%|      | 158/417 [00:06<00:09, 27.46it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  39%|      | 161/417 [00:06<00:09, 27.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  39%|      | 164/417 [00:06<00:09, 27.63it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  40%|      | 167/417 [00:06<00:09, 27.54it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  41%|      | 170/417 [00:06<00:09, 27.18it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  41%|     | 173/417 [00:07<00:08, 27.23it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  42%|     | 176/417 [00:07<00:08, 27.27it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  43%|     | 179/417 [00:07<00:08, 27.44it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  44%|     | 182/417 [00:07<00:08, 27.71it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  44%|     | 185/417 [00:07<00:08, 27.99it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  45%|     | 188/417 [00:07<00:08, 28.18it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  46%|     | 191/417 [00:07<00:08, 27.93it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  47%|     | 194/417 [00:07<00:08, 27.52it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  47%|     | 197/417 [00:07<00:07, 27.70it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  48%|     | 200/417 [00:08<00:07, 27.82it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  49%|     | 203/417 [00:08<00:07, 27.91it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  49%|     | 206/417 [00:08<00:07, 28.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  50%|     | 209/417 [00:08<00:07, 28.07it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  51%|     | 212/417 [00:08<00:07, 28.00it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  52%|    | 215/417 [00:08<00:07, 27.65it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  52%|    | 218/417 [00:08<00:07, 27.63it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  53%|    | 221/417 [00:08<00:07, 27.17it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  54%|    | 224/417 [00:08<00:07, 27.45it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  54%|    | 227/417 [00:09<00:06, 27.80it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  55%|    | 230/417 [00:09<00:06, 27.81it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  56%|    | 233/417 [00:09<00:06, 28.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  57%|    | 236/417 [00:09<00:06, 27.83it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  57%|    | 239/417 [00:09<00:06, 27.91it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  58%|    | 242/417 [00:09<00:06, 28.13it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  59%|    | 245/417 [00:09<00:06, 28.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  59%|    | 248/417 [00:09<00:05, 28.22it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  60%|    | 251/417 [00:09<00:05, 28.02it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  61%|    | 254/417 [00:09<00:05, 28.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  62%|   | 257/417 [00:10<00:05, 27.84it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  62%|   | 260/417 [00:10<00:05, 27.76it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  63%|   | 263/417 [00:10<00:05, 27.95it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  64%|   | 266/417 [00:10<00:05, 27.84it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  65%|   | 269/417 [00:10<00:05, 28.08it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  65%|   | 272/417 [00:10<00:05, 27.93it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  66%|   | 275/417 [00:10<00:05, 28.07it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  67%|   | 278/417 [00:10<00:04, 28.24it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  67%|   | 281/417 [00:10<00:04, 27.96it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  68%|   | 284/417 [00:11<00:04, 27.93it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  69%|   | 287/417 [00:11<00:04, 27.91it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  70%|   | 290/417 [00:11<00:04, 27.97it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  70%|   | 293/417 [00:11<00:04, 27.46it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  71%|   | 296/417 [00:11<00:04, 26.98it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  72%|  | 299/417 [00:11<00:04, 26.73it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  72%|  | 302/417 [00:11<00:04, 26.98it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  73%|  | 305/417 [00:11<00:04, 27.46it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  74%|  | 308/417 [00:11<00:03, 27.43it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  75%|  | 311/417 [00:12<00:03, 27.33it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  75%|  | 314/417 [00:12<00:03, 27.18it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  76%|  | 317/417 [00:12<00:03, 27.23it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  77%|  | 320/417 [00:12<00:03, 27.34it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  77%|  | 323/417 [00:12<00:03, 27.57it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  78%|  | 326/417 [00:12<00:03, 27.73it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  79%|  | 329/417 [00:12<00:03, 27.84it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  80%|  | 332/417 [00:12<00:03, 27.92it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  80%|  | 335/417 [00:12<00:02, 27.90it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  81%|  | 338/417 [00:13<00:02, 27.81it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  82%| | 341/417 [00:13<00:02, 27.67it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  82%| | 344/417 [00:13<00:02, 27.65it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  83%| | 347/417 [00:13<00:02, 27.79it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  84%| | 350/417 [00:13<00:02, 27.88it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  85%| | 353/417 [00:13<00:02, 26.83it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  85%| | 356/417 [00:13<00:02, 27.13it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  86%| | 359/417 [00:13<00:02, 27.16it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  87%| | 362/417 [00:13<00:02, 27.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  88%| | 365/417 [00:14<00:01, 27.43it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  88%| | 368/417 [00:14<00:01, 27.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  89%| | 371/417 [00:14<00:01, 27.62it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  90%| | 374/417 [00:14<00:01, 26.44it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  90%| | 377/417 [00:14<00:01, 26.78it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  91%| | 380/417 [00:14<00:01, 27.02it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  92%|| 383/417 [00:14<00:01, 26.76it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  93%|| 386/417 [00:14<00:01, 26.72it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  93%|| 389/417 [00:14<00:01, 26.27it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  94%|| 392/417 [00:15<00:00, 26.51it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  95%|| 395/417 [00:15<00:00, 26.83it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  95%|| 398/417 [00:15<00:00, 26.84it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  96%|| 401/417 [00:15<00:00, 27.13it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  97%|| 404/417 [00:15<00:00, 26.90it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  98%|| 407/417 [00:15<00:00, 27.26it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  98%|| 410/417 [00:15<00:00, 27.58it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy:  99%|| 413/417 [00:15<00:00, 27.21it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy: 100%|| 416/417 [00:15<00:00, 27.55it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting model relationship accuracy: 100%|| 417/417 [00:15<00:00, 26.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Final Model Scores =======\n{'subj': 0.8359802134860713, 'verb': 0.32040775534679194, 'dobj': 0.4634146341463415, 'pobj': 0.5984567901234568}\n{'subj': 0.6894038010934652, 'verb': 0.18149110533679794, 'dobj': 0.11022514071294559, 'pobj': 0.3253086419753086}\n{'subj': 2, 'verb': 1, 'dobj': 0, 'pobj': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_encoder_disentanglement_score(data_iter):\n",
    "    rel_idx, att_maxes = [], []\n",
    "    for i, batch in enumerate(tqdm(data_iter, desc=\"Getting model relationship accuracy\")):\n",
    "        rel_idx_i, att_maxes_i = get_att_and_rel_idx(batch.text[..., 1:])\n",
    "        rel_idx.extend(rel_idx_i)\n",
    "        att_maxes.extend(att_maxes_i)\n",
    "        \n",
    "    lv_scores = []\n",
    "    for lv in range(sum(h_params.n_latents)):\n",
    "        found = {'subj':[], 'verb':[], 'dobj':[], 'pobj':[]}\n",
    "        for att, rel_pos in zip(att_maxes, rel_idx):\n",
    "            for k in found.keys():\n",
    "                if len(rel_pos[k]):\n",
    "                    found[k].append(att[lv] in rel_pos[k])\n",
    "        lv_scores.append(found)\n",
    "    enc_att_scores = {'subj':[], 'verb':[], 'dobj':[], 'pobj':[]}\n",
    "    for lv in range(sum(h_params.n_latents)):\n",
    "        for k, v in lv_scores[lv].items():\n",
    "            enc_att_scores[k].append(np.mean(v))\n",
    "    print(\"==== Final Model Scores =======\")\n",
    "    enc_max_score, enc_disent_score, enc_disent_vars = {}, {}, {}\n",
    "    for k, v in enc_att_scores.items():\n",
    "        sort_idx = np.argsort(v) \n",
    "        enc_disent_vars[k], enc_disent_score[k], enc_max_score[k] =\\\n",
    "            sort_idx[-1], v[sort_idx[-1]] - v[sort_idx[-2]], v[sort_idx[-1]]\n",
    "    print(enc_max_score)\n",
    "    print(enc_disent_score)\n",
    "    print(enc_disent_vars)\n",
    "    return enc_att_scores, enc_max_score, enc_disent_score, enc_disent_vars\n",
    "    # # ================= BASELINE ================= \n",
    "    # baseline = []\n",
    "    # for k in ['subj', 'verb', 'dobj', 'pobj']:\n",
    "    #     all_pos = []\n",
    "    #     for pos in rel_idx:\n",
    "    #         all_pos.extend(pos[k])\n",
    "    #     baseline.append(np.median(all_pos))\n",
    "    # baseline_scores = []\n",
    "    # for lv in range(4):\n",
    "    #     found = {'subj':[], 'verb':[], 'dobj':[], 'pobj':[]}\n",
    "    #     for rel_pos in rel_idx:\n",
    "    #         for k in found.keys():\n",
    "    #             if len(rel_pos[k]):\n",
    "    #                 found[k].append(baseline[lv] in rel_pos[k])\n",
    "    #     baseline_scores.append(found)\n",
    "    # \n",
    "    # baseline_enc_att_scores = {'subj':[], 'verb':[], 'dobj':[], 'pobj':[]}\n",
    "    # for lv in range(sum(h_params.n_latents)):\n",
    "    #     for k, v in baseline_scores[lv].items():\n",
    "    #         baseline_enc_att_scores[k].append(np.mean(v))\n",
    "    # print(\"==== Final baseline Scores =======\")\n",
    "    # enc_max_score, enc_disent_score, enc_disent_vars = {}, {}, {}\n",
    "    # for k, v in baseline_enc_att_scores.items():\n",
    "    #     sort_idx = np.argsort(v) \n",
    "    #     enc_disent_vars[k], enc_disent_score[k], enc_max_score[k] =\\\n",
    "    #         sort_idx[-1], v[sort_idx[-1]] - v[sort_idx[-2]], v[sort_idx[-1]]\n",
    "    # print(enc_max_score)\n",
    "    # print(enc_disent_score)\n",
    "    # print(enc_disent_vars)\n",
    "\n",
    "def show_df_hm2(df):\n",
    "    snsplt = sns.heatmap(df, cmap ='Reds', linewidths = 0.20, annot = False)\n",
    "    b, t = plt.ylim() # discover the values for bottom and top\n",
    "    b += 0.5 # Add 0.5 to the bottom\n",
    "    t -= 0.5 # Subtract 0.5 from the top\n",
    "    plt.ylim(b, t) # update the ylim(bottom, top) values\n",
    "    return snsplt.get_figure()\n",
    "enc_att, enc_max, enc_disent, enc_disent_idx = \\\n",
    "    get_encoder_disentanglement_score(data.val_iter)\n",
    "data.reinit_iterator('valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       subj      verb      dobj      pobj\n0  0.036709  0.058165  0.463415  0.598457\n1  0.146576  0.320408  0.353189  0.244753\n2  0.835980  0.138917  0.000938  0.001852\n3  0.004686  0.004997  0.182458  0.273148\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def show_df_hm2(df):\n",
    "    snsplt = sns.heatmap(df, cmap ='Reds', linewidths = 0.20, annot = True)\n",
    "    b, t = plt.ylim() # discover the values for bottom and top\n",
    "    b += 0.5 # Add 0.5 to the bottom\n",
    "    t -= 0.5 # Subtract 0.5 from the top\n",
    "    plt.ylim(b, t) # update the ylim(bottom, top) values\n",
    "    return snsplt.get_figure()\n",
    "att_df = pd.DataFrame(enc_att)\n",
    "print(att_df)\n",
    "sns_plot = show_df_hm2(att_df)\n",
    "sns_plot.savefig(\"enc_att.eps\", dpi=100, format='eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 2.0, 5.0, 7.0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# for i, batch in enumerate(data.val_iter):\n",
    "#     text_sents = [' '.join([model.index[model.generated_v].itos[w] \n",
    "#                             for w in s]).replace(' <pad>', '').replace(' <eos>', '')\n",
    "#                   for s in batch.text[:, 1:]]\n",
    "#     # Getting relations' positions\n",
    "#     rel_idx = shallow_dependencies(text_sents)\n",
    "#     for r, t in zip(rel_idx, text_sents):\n",
    "#         # if r['idx']['verb'][0] > 10:\n",
    "#         print(r, t)\n",
    "#     if i>10: break\n",
    "lv_scores = []\n",
    "for lv in range(sum(h_params.n_latents)):\n",
    "    found = {'subj':[], 'verb':[], 'dobj':[], 'pobj':[]}\n",
    "    for att, rel_pos in zip(att_maxes, rel_idx):\n",
    "        for k in found.keys():\n",
    "            if len(rel_pos[k]):\n",
    "                found[k].append(att[lv] in rel_pos[k])\n",
    "    lv_scores.append(found)\n",
    "\n",
    "baseline = []\n",
    "for k in ['subj', 'verb', 'dobj', 'pobj']:\n",
    "    all_pos = []\n",
    "    for pos in rel_idx:\n",
    "        all_pos.extend(pos[k])\n",
    "    baseline.append(np.median(all_pos))\n",
    "print(baseline)\n",
    "\n",
    "baseline_scores = []\n",
    "for lv in range(4):\n",
    "    found = {'subj':[], 'verb':[], 'dobj':[], 'pobj':[]}\n",
    "    for rel_pos in rel_idx:\n",
    "        for k in found.keys():\n",
    "            if len(rel_pos[k]):\n",
    "                found[k].append(baseline[lv] in rel_pos[k])\n",
    "    baseline_scores.append(found)\n",
    "            \n",
    "\n",
    "# found = {'subj':[], 'verb':[], 'dobj':[], 'pobj':[]}\n",
    "# for att, rel_pos in zip(att_maxes, rel_idx):\n",
    "#     for k in found.keys():\n",
    "#         if len(rel_pos[k]):\n",
    "#             found[k].append(att[lv][layer] in rel_pos[k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========our Model ========\nScores for lv 0, layer 0:\n{'subj': 0.38920863309352516, 'verb': 0.27425, 'dobj': 0.10831426392067124, 'pobj': 0.061317183951551855}\nScores for lv 1, layer 0:\n{'subj': 0.11726618705035971, 'verb': 0.12575, 'dobj': 0.38367658276125094, 'pobj': 0.23164269492808479}\nScores for lv 2, layer 0:\n{'subj': 0.0, 'verb': 0.0, 'dobj': 0.006864988558352402, 'pobj': 0.008327024981074944}\nScores for lv 3, layer 0:\n{'subj': 0.05, 'verb': 0.0335, 'dobj': 0.3531655225019069, 'pobj': 0.36260408781226344}\nScores for lv 4, layer 0:\n{'subj': 0.002158273381294964, 'verb': 0.00375, 'dobj': 0.10907704042715484, 'pobj': 0.16654049962149886}\nScores for lv 5, layer 0:\n{'subj': 0.02194244604316547, 'verb': 0.03, 'dobj': 0.2387490465293669, 'pobj': 0.33459500378501134}\nScores for lv 6, layer 0:\n{'subj': 0.007553956834532374, 'verb': 0.0145, 'dobj': 0.18916857360793288, 'pobj': 0.24072672218016655}\nScores for lv 7, layer 0:\n{'subj': 0.7607913669064749, 'verb': 0.09475, 'dobj': 0.004576659038901602, 'pobj': 0.0}\n==== Final Scores =======\n{'subj': 0.7607913669064749, 'verb': 0.27425, 'dobj': 0.38367658276125094, 'pobj': 0.36260408781226344}\n{'subj': 0.3715827338129497, 'verb': 0.1485, 'dobj': 0.030511060259344025, 'pobj': 0.028009084027252107}\n{'subj': 7, 'verb': 0, 'dobj': 1, 'pobj': 3}\n========our baseline ========\nScores for lv 0, layer 0:\n{'subj': 0.4068345323741007, 'verb': 0.31575, 'dobj': 0.09153318077803203, 'pobj': 0.03557910673732021}\nScores for lv 1, layer 0:\n{'subj': 0.19784172661870503, 'verb': 0.24375, 'dobj': 0.2959572845156369, 'pobj': 0.10749432248296745}\nScores for lv 2, layer 0:\n{'subj': 0.06510791366906475, 'verb': 0.03725, 'dobj': 0.34553775743707094, 'pobj': 0.32248296744890237}\nScores for lv 3, layer 0:\n{'subj': 0.024820143884892086, 'verb': 0.02775, 'dobj': 0.2372234935163997, 'pobj': 0.3209689629068887}\n==== Final Scores =======\n{'subj': 0.4068345323741007, 'verb': 0.31575, 'dobj': 0.34553775743707094, 'pobj': 0.32248296744890237}\n{'subj': 0.2089928057553957, 'verb': 0.07199999999999998, 'dobj': 0.04958047292143403, 'pobj': 0.0015140045420136694} 0.33208728321884334\n{'subj': 0, 'verb': 0, 'dobj': 2, 'pobj': 2}\n"
     ]
    }
   ],
   "source": [
    "print(\"========our Model ========\")\n",
    "Enc_att_scores = {'subj':[], 'verb':[], 'dobj':[], 'pobj':[]}\n",
    "for lv in range(sum(h_params.n_latents)):\n",
    "    for layer in range(1):#h_params.encoder_l):\n",
    "        print(\"Scores for lv {}, layer {}:\".format(lv, layer))\n",
    "        print({k:np.mean(v)for k, v in lv_scores[lv].items()})\n",
    "        for k, v in lv_scores[lv].items():\n",
    "            Enc_att_scores[k].append(np.mean(v))\n",
    "print(\"==== Final Scores =======\")\n",
    "enc_max_score = {}\n",
    "enc_disent_score = {}\n",
    "enc_disent_vars = {}\n",
    "for k, v in Enc_att_scores.items():\n",
    "    sort_idx = np.argsort(v) \n",
    "    enc_disent_vars[k] = sort_idx[-1]\n",
    "    enc_disent_score[k] = v[sort_idx[-1]] - v[sort_idx[-2]]\n",
    "    enc_max_score[k] = v[sort_idx[-1]]\n",
    "print(enc_max_score)\n",
    "print(enc_disent_score)\n",
    "print(enc_disent_vars)\n",
    "print(\"========our baseline ========\")\n",
    "\n",
    "baseline_enc_att_scores = {'subj':[], 'verb':[], 'dobj':[], 'pobj':[]}\n",
    "for lv in range(len(baseline_scores)):\n",
    "    for layer in range(1):#h_params.encoder_l):\n",
    "        print(\"Scores for lv {}, layer {}:\".format(lv, layer))\n",
    "        print({k:np.mean(v)for k, v in baseline_scores[lv].items()})\n",
    "        for k, v in baseline_scores[lv].items():\n",
    "            baseline_enc_att_scores[k].append(np.mean(v))\n",
    "print(\"==== Final Scores =======\")\n",
    "enc_max_score = {}\n",
    "enc_disent_score = {}\n",
    "enc_disent_vars = {}\n",
    "for k, v in baseline_enc_att_scores.items():\n",
    "    sort_idx = np.argsort(v) \n",
    "    enc_disent_vars[k] = sort_idx[-1]\n",
    "    enc_disent_score[k] = v[sort_idx[-1]] - v[sort_idx[-2]]\n",
    "    enc_max_score[k] = v[sort_idx[-1]]\n",
    "print(enc_max_score)\n",
    "print(enc_disent_score, sum(enc_disent_score.values()))\n",
    "print(enc_disent_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 'll never be going back\n(\"i 'll never be going back\", 'nsubj aux neg aux ROOT advmod')\ngreat food, great service\n('great food , service', 'amod ROOT punct appos')\ndecoration is outdated .\n('decoration is outdated .', 'nsubjpass auxpass ROOT punct')\nthe guy in front of us was ordering in spanish .\n('guy was ordering in .', 'nsubj aux ROOT prep punct')\nworst service ever .\n('worst service ever .', 'amod ROOT advmod punct')\nthe worst experience i have ever had at an enterprise location .\n('the worst experience had .', 'det amod ROOT relcl punct')\nwaitresses are slow\n('waitresses are slow', 'nsubj ROOT acomp')\nthis place is awful !\n('place is awful !', 'nsubj ROOT acomp punct')\n"
     ]
    }
   ],
   "source": [
    "ex_sents = [\"i 'll never be going back\", \"great food, great service\",\n",
    "            \"decoration is outdated .\",\n",
    "            \"the guy in front of us was ordering in spanish .\",\n",
    "            \"worst service ever .\", \"the worst experience i have ever had at an enterprise location .\",\n",
    "            'waitresses are slow', \"this place is awful !\"]\n",
    "import itertools\n",
    "def get_children(tok, depth):\n",
    "    if depth == 0:\n",
    "        return list(tok.children)\n",
    "    else:\n",
    "        return list(tok.children) + \\\n",
    "               list(itertools.chain.from_iterable([get_children(c, depth-1) for c in tok.children]))\n",
    "\n",
    "def truncated_template(sents, depth):\n",
    "    docs = nlp.pipe(sents)\n",
    "    templates = []\n",
    "    for doc in docs:\n",
    "        children = None\n",
    "        for i, tok in enumerate(doc):\n",
    "            if tok.dep_ =='ROOT':\n",
    "                children = [tok]+get_children(tok, depth)\n",
    "        if children is not None:\n",
    "            sort_dict_lex = {c.i:c.text for c in children}\n",
    "            sort_dict_syn = {c.i:c.dep_ for c in children}\n",
    "            templates.append({'lex': ' '.join([sort_dict_lex[i] for i in sorted(sort_dict_lex.keys())]),\n",
    "                              'syn': ' '.join([sort_dict_syn[i] for i in sorted(sort_dict_syn.keys())])})\n",
    "        else: \n",
    "            raise NotImplementedError(\"This sentence has no ROOT: \".format(' '.join([t.text for t in doc])))\n",
    "\n",
    "    \n",
    "docs = nlp.pipe(ex_sents)\n",
    "for doc in docs:\n",
    "    print(doc.text)\n",
    "    print(truncated_template(doc, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'det'), ('construction', 'compound'), ('workers', 'nsubj'), ('are', 'aux'), ('using', 'ROOT'), ('a', 'det'), ('pick', 'amod'), ('axes', 'dobj'), ('to', 'aux'), ('poke', 'xcomp'), ('each', 'det'), ('other', 'dobj'), ('.', 'punct')]\n[{'text': {'subj': 'it', 'verb': 'told', 'dobj': '_ num _ mins', 'pobj': ''}, 'idx': {'subj': [4], 'verb': [3], 'dobj': [7, 8, 9, 10], 'pobj': []}}, {'text': {'subj': 'i', 'verb': 'go', 'dobj': '', 'pobj': ''}, 'idx': {'subj': [5], 'verb': [7], 'dobj': [], 'pobj': []}}, {'text': {'subj': 'we', 'verb': '', 'dobj': '', 'pobj': ''}, 'idx': {'subj': [0], 'verb': [], 'dobj': [], 'pobj': []}}, {'text': {'subj': '', 'verb': '', 'dobj': '', 'pobj': ''}, 'idx': {'subj': [], 'verb': [], 'dobj': [], 'pobj': []}}, {'text': {'subj': 'they', 'verb': '', 'dobj': '', 'pobj': 'this point'}, 'idx': {'subj': [3], 'verb': [], 'dobj': [], 'pobj': [1, 2]}}, {'text': {'subj': 'people', 'verb': '', 'dobj': 'baseball', 'pobj': ''}, 'idx': {'subj': [0], 'verb': [], 'dobj': [4], 'pobj': []}}, {'text': {'subj': 'two dogs', 'verb': 'pull', 'dobj': 'opposite ends of a rope', 'pobj': 'a rope'}, 'idx': {'subj': [0, 1], 'verb': [2], 'dobj': [4, 5, 6, 7, 8], 'pobj': [7, 8]}}, {'text': {'subj': 'a lady', 'verb': 'lays', 'dobj': '', 'pobj': 'a beach'}, 'idx': {'subj': [0, 1], 'verb': [2], 'dobj': [], 'pobj': [4, 5]}}, {'text': {'subj': 'the construction workers', 'verb': 'using', 'dobj': 'a pick axes', 'pobj': ''}, 'idx': {'subj': [0, 1, 2], 'verb': [4], 'dobj': [5, 6, 7], 'pobj': []}}, {'text': {'subj': 'people', 'verb': 'running', 'dobj': '', 'pobj': 'the streets'}, 'idx': {'subj': [0], 'verb': [2], 'dobj': [], 'pobj': [4, 5]}}, {'text': {'subj': 'someone', 'verb': 'prepares', 'dobj': 'food', 'pobj': 'bowls'}, 'idx': {'subj': [0], 'verb': [1], 'dobj': [2], 'pobj': [4]}}]\nYelp &  i was originally told it would take _num_ mins . &  it& told& _ num _ mins&  &  nsubjpass auxpass advmod ROOT ccomp punct& i was originally told take . \\\\ \\hline\nYelp &  slow , over priced , i 'll go elsewhere next time . &  i& go& &  &  advmod punct advcl punct nsubj aux ROOT advmod npadvmod punct& slow , priced , i 'll go elsewhere time . \\\\ \\hline\nYelp &  we will not be back &  we& & &  &  nsubj aux neg ROOT advmod& we will not be back \\\\ \\hline\nYelp &  terrible . &  & & &  &  ROOT punct& terrible . \\\\ \\hline\nYelp &  at this point they were open and would be for another hour . &  they& & & this point &  prep nsubj ROOT acomp cc conj punct& at they were open and be . \\\\ \\hline\nSNLI &  people are outside playing baseball . &  people& & baseball&  &  nsubj ROOT advmod punct& people are outside . \\\\ \\hline\nSNLI &  two dogs pull on opposite ends of a rope . &  two dogs& pull& opposite ends of a rope& a rope &  nsubj ROOT prt dobj punct& dogs pull on ends . \\\\ \\hline\nSNLI &  a lady lays at a beach . &  a lady& lays& & a beach &  nsubj ROOT prep punct& lady lays at . \\\\ \\hline\nSNLI &  the construction workers are using a pick axes to poke each other . &  the construction workers& using& a pick axes&  &  nsubj aux ROOT dobj xcomp punct& workers are using axes poke . \\\\ \\hline\nSNLI &  people are running through the streets while people watch . &  people& running& & the streets &  nsubj aux ROOT prep advcl punct& people are running through watch . \\\\ \\hline\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-9046500d96b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mex_sents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     print(orig[i], '& ',ex_sents[i], '& ', '& '.join([deps[i]['text'][k] for k in roles]), '& ',\n\u001b[0m\u001b[0;32m     30\u001b[0m           '& '.join([temps[i][k] for k in temptypes]), '\\\\\\\\ \\hline')\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "      \n",
    "import spacy\n",
    "from disentanglement_transformer.models import shallow_dependencies, truncated_template\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "ex_sents = ['i was originally told it would take _num_ mins .',\n",
    "            'slow , over priced , i \\'ll go elsewhere next time .',\n",
    "            'we will not be back',\n",
    "            'terrible .',\n",
    "            'at this point they were open and would be for another hour .',\n",
    "            'people are outside playing baseball .',\n",
    "            'two dogs pull on opposite ends of a rope .',\n",
    "            'a lady lays at a beach .',\n",
    "            'the construction workers are using a pick axes to poke each other .',\n",
    "            'people are running through the streets while people watch .',\n",
    "            'someone prepares food into bowls ']\n",
    "orig = ['Yelp']*5+['SNLI']*5\n",
    "print([(tok.text, tok.dep_) for tok in nlp(\n",
    "            'the construction workers are using a pick axes to poke each other .')])\n",
    "\n",
    "\n",
    "roles, temptypes = ['subj', 'verb', 'dobj', 'pobj'], ['syn', 'lex']\n",
    "deps = shallow_dependencies(ex_sents)\n",
    "temps = truncated_template(ex_sents)\n",
    "print(deps)\n",
    "for i in range(len(ex_sents)):\n",
    "    print(orig[i], '& ',ex_sents[i], '& ', '& '.join([deps[i]['text'][k] for k in roles]), '& ',\n",
    "          '& '.join([temps[i][k] for k in temptypes]), '\\\\\\\\ \\hline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' no server , he was very friendly and helpful ', ' the staff is friendly and helpful and friendly ', ' the place is way more than the food is good ', ' amazing service , what a good time ', \" i 've been here twice and we 're quite happy with my experience \"]\n"
     ]
    }
   ],
   "source": [
    "text, samples, params = model.get_sentences(5, gen_len=16, sample_w=False, vary_z=True, complete=None, contains=None, max_tries=100)\n",
    "\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " no server , he was very friendly and helpful  : [' no server , no flavor ', ' no server , no flavor ']\n the staff is friendly and helpful and friendly  : [\" i would n't recommend this for a nice place \", ' wrong ']\n the place is way more than the food is good  : [' the place is way more than the lobby and they are great ', ' the place is way more well and my dogs ']\n amazing service , what a good time  : [' amazing service , what a great place to eat ', ' service is always what they do , everything ']\n i 've been here twice and we 're quite happy with my experience  : [' they are very friendly and helpful ', ' the food was delicious ']\n"
     ]
    }
   ],
   "source": [
    "var_ids = [2]\n",
    "alt_text, alt_params = model._get_alternative_sentences(samples, None, var_ids, 2, 16, complete=None)\n",
    "for i in range(len(text)):\n",
    "    print(text[i], ':', alt_text[i::len(text)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " no server , he was very friendly and helpful  : [' no server , he had to die for our slice of ed ', ' no server , he gave me , but our slice of sort of soup ']\n the staff is friendly and helpful and friendly  : [' wrong ', \" i 've been here twice for dinner \"]\n the place is way more than the food is good  : [' the place is way more than the food is good ', ' the place is way more than the food is good ']\n amazing service , what a good time  : [' amazing service , what a great place to eat ', \" i 've always enjoyed this location because this place was really positive \"]\n i 've been here twice and we 're quite happy with my experience  : [' the staff is friendly and the food is quite easy as well ', ' great food ']\n"
     ]
    }
   ],
   "source": [
    "var_ids = [2]\n",
    "alt_text, alt_params = model._get_alternative_sentences(samples, None, var_ids, 2, 16, complete=None)\n",
    "for i in range(len(text)):\n",
    "    print(text[i], ':', alt_text[i::len(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " no server , he was very friendly and helpful  : [' great service and great food ', ' great service ']\n the staff is friendly and helpful and friendly  : [' our food was very good and loved it and it disappeared ', ' the staff is friendly and helpful and so hard to pay her cafe ']\n the place is way more than the food is good  : [' the food was delicious and the service was great ', ' the food was delicious and the service was great ']\n amazing service , what a good time  : [' amazing people ', ' the food is always good , service is great ']\n i 've been here twice and we 're quite happy with my experience  : [' the food is always fresh and delicious ', ' i will definitely be back again ']\n"
     ]
    }
   ],
   "source": [
    "var_ids = [3, 4, 5, 6, 7]\n",
    "alt_text, alt_params = model._get_alternative_sentences(samples, None, var_ids, 2, 16, complete=None)\n",
    "for i in range(len(text)):\n",
    "    print(text[i], ':', alt_text[i::len(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' no server , he was very friendly and helpful ', ' the staff is friendly and helpful and friendly ', ' the place is way more than the food is good ', ' amazing service , what a good time ', \" i 've been here twice and we 're quite happy with my experience \"]\norig: \"  no server , he was very friendly and helpful  |z_orig:   no server , he was very friendly and helpful  |result:   no server , he was very friendly and helpful\norig: \"  no server , he was very friendly and helpful  |z_orig:   the staff is friendly and helpful and friendly  |result:   our waitress was very nice and attentive\norig: \"  no server , he was very friendly and helpful  |z_orig:   the place is way more than the food is good  |result:   the place is way more than the food is good\norig: \"  no server , he was very friendly and helpful  |z_orig:   amazing service , what a good time  |result:   amazing service , what a great place to eat\norig: \"  no server , he was very friendly and helpful  |z_orig:   i 've been here twice and we 're quite happy with my experience  |result:   they are very friendly and helpful\norig: \"  the staff is friendly and helpful and friendly  |z_orig:   no server , he was very friendly and helpful  |result:   no server , he was rude and so our slice of ed\norig: \"  the staff is friendly and helpful and friendly  |z_orig:   the staff is friendly and helpful and friendly  |result:   the staff is friendly and helpful and friendly\norig: \"  the staff is friendly and helpful and friendly  |z_orig:   the place is way more than the food is good  |result:   the place is way more rude and so unique\norig: \"  the staff is friendly and helpful and friendly  |z_orig:   amazing service , what a good time  |result:   service is always what they have loved and wait\norig: \"  the staff is friendly and helpful and friendly  |z_orig:   i 've been here twice and we 're quite happy with my experience  |result:   the staff is friendly and the staff is easy and helpful\norig: \"  the place is way more than the food is good  |z_orig:   no server , he was very friendly and helpful  |result:   no server , he was very friendly and helpful\norig: \"  the place is way more than the food is good  |z_orig:   the staff is friendly and helpful and friendly  |result:   our waitress was very nice and attentive\norig: \"  the place is way more than the food is good  |z_orig:   the place is way more than the food is good  |result:   the place is way more than the food is good\norig: \"  the place is way more than the food is good  |z_orig:   amazing service , what a good time  |result:   amazing service , what a great place to eat\norig: \"  the place is way more than the food is good  |z_orig:   i 've been here twice and we 're quite happy with my experience  |result:   they are very friendly and helpful\norig: \"  amazing service , what a good time  |z_orig:   no server , he was very friendly and helpful  |result:   no server , he had a good time as some sort of ed\norig: \"  amazing service , what a good time  |z_orig:   the staff is friendly and helpful and friendly  |result:   the food is always delicious\norig: \"  amazing service , what a good time  |z_orig:   the place is way more than the food is good  |result:   the place is way more good for a unique atmosphere\norig: \"  amazing service , what a good time  |z_orig:   amazing service , what a good time  |result:   amazing service , what a good time\norig: \"  amazing service , what a good time  |z_orig:   i 've been here twice and we 're quite happy with my experience  |result:   the food was delicious and the service is wonderful and they were amazing\norig: \"  i 've been here twice and we 're quite happy with my experience  |z_orig:   no server , he was very friendly and helpful  |result:   no server , he was ordered for my wife ordered sort of ed\norig: \"  i 've been here twice and we 're quite happy with my experience  |z_orig:   the staff is friendly and helpful and friendly  |result:   i will definitely be back again soon\norig: \"  i 've been here twice and we 're quite happy with my experience  |z_orig:   the place is way more than the food is good  |result:   the place is way more clean and we 're looking\norig: \"  i 've been here twice and we 're quite happy with my experience  |z_orig:   amazing service , what a good time  |result:   amazing service , what a terrible visit\norig: \"  i 've been here twice and we 're quite happy with my experience  |z_orig:   i 've been here twice and we 're quite happy with my experience  |result:   i 've been here twice and we 're quite happy with my experience\n"
     ]
    }
   ],
   "source": [
    "def decode_to_text(x_hat_params, vocab_size, vocab_index):\n",
    "    # It is assumed that this function is used at test time for display purposes\n",
    "    # Getting the argmax from the one hot if it's not done\n",
    "    while x_hat_params.shape[-1] == vocab_size and x_hat_params.ndim > 3:\n",
    "        x_hat_params = x_hat_params.mean(0)\n",
    "    while x_hat_params.ndim > 2 and x_hat_params.shape[-1] != self.h_params.vocab_size:\n",
    "        x_hat_params = x_hat_params[0]\n",
    "    if x_hat_params.shape[-1] == vocab_size:\n",
    "        x_hat_params = torch.argmax(x_hat_params, dim=-1)\n",
    "    assert x_hat_params.ndim == 2, \"Mis-shaped generated sequence: {}\".format(x_hat_params.shape)\n",
    "    \n",
    "    samples = [' '.join([vocab_index.itos[w]\n",
    "                         for w in sen]).split('<eos>')[0].split(' !')[0].split(' .')[0].replace('<go>', '').replace('</go>', '')\n",
    "               .replace('<pad>', '_').replace('_unk', '<?>')\n",
    "               for sen in x_hat_params]\n",
    "\n",
    "    return samples\n",
    "\n",
    "\n",
    "def swap_latents(mdl, prev_latent_vals, var_z_ids, gen_len, complete=None, no_unk=True):\n",
    "            n_orig_sentences = prev_latent_vals['z1'].shape[0]\n",
    "            n_samples = n_orig_sentences\n",
    "            go_symbol = torch.ones([n_samples * n_orig_sentences]).long() * \\\n",
    "                        mdl.index[mdl.generated_v].stoi['<go>']\n",
    "            go_symbol = go_symbol.to(mdl.h_params.device).unsqueeze(-1)\n",
    "            x_prev = go_symbol\n",
    "            if complete is not None:\n",
    "                for token in complete.split(' '):\n",
    "                    x_prev = torch.cat([x_prev, torch.ones([n_samples * n_orig_sentences, 1]).long().to(mdl.h_params.device) * \\\n",
    "                        mdl.index[mdl.generated_v].stoi[token]], dim=1)\n",
    "                gen_len = gen_len - len(complete.split(' '))\n",
    "            temp = 1.\n",
    "            orig_z = prev_latent_vals['z1'].unsqueeze(1).repeat(1, n_samples, 1)\n",
    "            z_sample = orig_z.reshape(n_samples*n_orig_sentences, -1)\n",
    "            orig_z = orig_z.transpose(0, 1).reshape(n_samples*n_orig_sentences, -1)\n",
    "\n",
    "            for id in var_z_ids:\n",
    "                z_number = sum([id> sum(h_params.n_latents[:i+1]) for i in range(len(h_params.n_latents))])\n",
    "                z_index = id - sum(h_params.n_latents[:z_number])\n",
    "                start, end = int(h_params.z_size/max(h_params.n_latents)*z_index),\\\n",
    "                             int(h_params.z_size/max(h_params.n_latents)*(z_index+1))\n",
    "                source, destination = [z_sample][z_number], [orig_z][z_number]\n",
    "                destination[:, start:end] = source[:, start:end]\n",
    "            \n",
    "            z_input = {'z1': orig_z.unsqueeze(1)}\n",
    "            \n",
    "            # Normal Autoregressive generation\n",
    "            for i in range(gen_len):\n",
    "                mdl.gen_bn({'x_prev': x_prev, **{k: v.expand(v.shape[0], i+1, v.shape[-1])\n",
    "                                                  for k, v in z_input.items()}})\n",
    "                samples_i = mdl.generated_v.post_params['logits']\n",
    "                if no_unk:\n",
    "                    annul_vector = 1-F.one_hot(torch.tensor([data.vocab.stoi['<unk>']]).to(DEVICE), h_params.vocab_size)\n",
    "                    samples_i *= annul_vector\n",
    "                \n",
    "                x_prev = torch.cat([x_prev, torch.argmax(samples_i,     dim=-1)[..., -1].unsqueeze(-1)],\n",
    "                                   dim=-1)\n",
    "            \n",
    "            text = decode_to_text(x_prev, mdl.h_params.vocab_size, mdl.index[mdl.generated_v])\n",
    "            return text, {'z1': orig_z}\n",
    "sw_zs = [2]\n",
    "sw_text, sw_samples = swap_latents(model, samples, sw_zs, 16, complete=None, no_unk=True)\n",
    "print(text)\n",
    "for i in range(len(text)):\n",
    "    for j in range(len(text)):\n",
    "        print(\"orig: \\\"\", text[i], \"|z_orig: \", text[j], \"|result: \", sw_text[len(text)*i+j])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
