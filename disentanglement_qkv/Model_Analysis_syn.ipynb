{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Instanciating the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 443259  examples. statistics:\n -words: 8.881732801815643+-3.6417630572571147(quantiles(0.5, 0.7, 0.9, 0.95, 0.99:9.0,11.0,14.0,15.0,15.0)\nDataset has 4000  examples. statistics:\n -words: 8.9255+-3.668371539252806(quantiles(0.5, 0.7, 0.9, 0.95, 0.99:9.0,11.0,14.0,15.0,15.0)\nDataset has 1000  examples. statistics:\n -words: 10.325+-2.8399603870476784(quantiles(0.5, 0.7, 0.9, 0.95, 0.99:10.0,12.0,14.0,15.0,15.0)\ndata loading took 5.219523668289185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words:  9600 , On device:  cuda\nLoss Type:  VAE\nreconstruction net size: 25.54 M\nprior net sizes:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model at step 27704\nUnsupervised training examples:  443264\nUnsupervised val examples:  42752\nNumber of parameters:  30.71 M\nInference parameters:  06.02 M\nGeneration parameters:  25.91 M\nEmbedding parameters:  01.23 M\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "from torch import device\n",
    "import torch\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "\n",
    "from data_prep import NLIGenData2, OntoGenData, HuggingYelp2\n",
    "from disentanglement_qkv.models import DisentanglementTransformerVAE, LaggingDisentanglementTransformerVAE\n",
    "from disentanglement_qkv.h_params import DefaultTransformerHParams as HParams\n",
    "from disentanglement_qkv.graphs import *\n",
    "from components.criteria import *\n",
    "parser = argparse.ArgumentParser()\n",
    "from torch.nn import MultiheadAttention\n",
    "# Training and Optimization\n",
    "k, kz, klstm = 1, 8, 2\n",
    "# k, kz, klstm = 2, 4, 2\n",
    "parser.add_argument(\"--test_name\", default='unnamed', type=str)\n",
    "parser.add_argument(\"--data\", default='nli', choices=[\"nli\", \"ontonotes\", \"yelp\"], type=str)\n",
    "parser.add_argument(\"--csv_out\", default='disentqkv.csv', type=str)\n",
    "parser.add_argument(\"--max_len\", default=17, type=int)\n",
    "parser.add_argument(\"--batch_size\", default=128, type=int)\n",
    "parser.add_argument(\"--grad_accu\", default=1, type=int)\n",
    "parser.add_argument(\"--n_epochs\", default=20, type=int)\n",
    "parser.add_argument(\"--test_freq\", default=32, type=int)\n",
    "parser.add_argument(\"--complete_test_freq\", default=160, type=int)\n",
    "parser.add_argument(\"--generation_weight\", default=1, type=float)\n",
    "parser.add_argument(\"--device\", default='cuda:0', choices=[\"cuda:0\", \"cuda:1\", \"cuda:2\", \"cpu\"], type=str)\n",
    "parser.add_argument(\"--embedding_dim\", default=128, type=int)#################\"\n",
    "parser.add_argument(\"--pretrained_embeddings\", default=False, type=bool)#################\"\n",
    "parser.add_argument(\"--z_size\", default=96*kz, type=int)#################\"\n",
    "parser.add_argument(\"--z_emb_dim\", default=192*k, type=int)#################\"\n",
    "parser.add_argument(\"--n_keys\", default=4, type=int)#################\"\n",
    "parser.add_argument(\"--n_latents\", default=[16, 16, 16], nargs='+', type=int)#################\"\n",
    "parser.add_argument(\"--text_rep_l\", default=3, type=int)\n",
    "parser.add_argument(\"--text_rep_h\", default=192*k, type=int)\n",
    "parser.add_argument(\"--encoder_h\", default=192*k, type=int)#################\"\n",
    "parser.add_argument(\"--encoder_l\", default=2, type=int)#################\"\n",
    "parser.add_argument(\"--decoder_h\", default=192*k, type=int)\n",
    "parser.add_argument(\"--decoder_l\", default=2, type=int)#################\"\n",
    "parser.add_argument(\"--highway\", default=False, type=bool)\n",
    "parser.add_argument(\"--markovian\", default=True, type=bool)\n",
    "parser.add_argument('--minimal_enc', dest='minimal_enc', action='store_true')\n",
    "parser.add_argument('--no-minimal_enc', dest='minimal_enc', action='store_false')\n",
    "parser.set_defaults(minimal_enc=False)\n",
    "parser.add_argument(\"--losses\", default='VAE', choices=[\"VAE\", \"IWAE\" \"LagVAE\"], type=str)\n",
    "parser.add_argument(\"--graph\", default='Normal', choices=[\"Vanilla\", \"IndepInfer\", \"QKV\", \"HQKV\"], type=str)\n",
    "parser.add_argument(\"--training_iw_samples\", default=1, type=int)\n",
    "parser.add_argument(\"--testing_iw_samples\", default=5, type=int)\n",
    "parser.add_argument(\"--test_prior_samples\", default=10, type=int)\n",
    "parser.add_argument(\"--anneal_kl0\", default=3000, type=int)\n",
    "parser.add_argument(\"--anneal_kl1\", default=6000, type=int)\n",
    "parser.add_argument(\"--grad_clip\", default=5., type=float)\n",
    "parser.add_argument(\"--kl_th\", default=0/(768*k/2), type=float or None)\n",
    "parser.add_argument(\"--max_elbo1\", default=6.0, type=float)\n",
    "parser.add_argument(\"--max_elbo2\", default=4.0, type=float)\n",
    "parser.add_argument(\"--max_elbo_choice\", default=10, type=int)\n",
    "parser.add_argument(\"--kl_beta\", default=0.4, type=float)\n",
    "parser.add_argument(\"--dropout\", default=0.3, type=float)\n",
    "parser.add_argument(\"--word_dropout\", default=0.1, type=float)\n",
    "parser.add_argument(\"--l2_reg\", default=0, type=float)\n",
    "parser.add_argument(\"--lr\", default=2e-4, type=float)\n",
    "parser.add_argument(\"--lr_reduction\", default=4., type=float)\n",
    "parser.add_argument(\"--wait_epochs\", default=1, type=float)\n",
    "parser.add_argument(\"--save_all\", default=True, type=bool)\n",
    "\n",
    "flags, _ = parser.parse_known_args()\n",
    "\n",
    "# Manual Settings, Deactivate before pushing\n",
    "if True:\n",
    "    flags.batch_size = 128\n",
    "    flags.grad_accu = 1\n",
    "    flags.max_len = 17\n",
    "    # flags.test_name = \"nliLM/YelpQKV_beta0.5.0.3.1.16\"#\"nliLM/HQKVTest2\"\n",
    "    flags.test_name = \"nliLM/HQKVTest2\"\n",
    "    flags.data = \"yelp\"\n",
    "    flags.n_latents = [8]\n",
    "    # flags.n_latents = [16]\n",
    "    flags.graph = \"HQKV\"  \n",
    "    # flags.graph =\"QKV\"\n",
    "    # flags.losses = \"LagVAE\"\n",
    "    flags.kl_beta = 0.5\n",
    "    \n",
    "    # flags.anneal_kl0 = 0\n",
    "    flags.max_elbo_choice = 6\n",
    "    # flags.z_size = 16\n",
    "    # flags.encoder_h = 256\n",
    "    # flags.decoder_h = 256\n",
    "    \n",
    "\n",
    "\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "GRAPH = {\"Vanilla\": get_vanilla_graph,\n",
    "         \"IndepInfer\": get_structured_auto_regressive_indep_graph,\n",
    "         \"QKV\": get_qkv_graph2,\n",
    "         \"HQKV\": get_hqkv_graph_old}[flags.graph]\n",
    "if flags.graph == \"NormalLSTM\":\n",
    "    flags.encoder_h = int(flags.encoder_h/k*klstm)\n",
    "if flags.graph == \"Vanilla\":\n",
    "    flags.n_latents = [flags.z_size]\n",
    "if flags.losses == \"LagVAE\":\n",
    "    flags.anneal_kl0 = 0\n",
    "    flags.anneal_kl1 = 0\n",
    "Data = {\"nli\": NLIGenData2, \"ontonotes\": OntoGenData, \"yelp\": HuggingYelp2}[flags.data]\n",
    "MAX_LEN = flags.max_len\n",
    "BATCH_SIZE = flags.batch_size\n",
    "GRAD_ACCU = flags.grad_accu\n",
    "N_EPOCHS = flags.n_epochs\n",
    "TEST_FREQ = flags.test_freq\n",
    "COMPLETE_TEST_FREQ = flags.complete_test_freq\n",
    "DEVICE = device(flags.device)\n",
    "# This prevents illegal memory access on multigpu machines (unresolved issue on torch's github)\n",
    "if flags.device.startswith('cuda'):\n",
    "    torch.cuda.set_device(int(flags.device[-1]))\n",
    "LOSSES = {'IWAE': [IWLBo],\n",
    "          'VAE': [ELBo],\n",
    "          'LagVAE': [ELBo]}[flags.losses]\n",
    "\n",
    "ANNEAL_KL = [flags.anneal_kl0*flags.grad_accu, flags.anneal_kl1*flags.grad_accu]\n",
    "LOSS_PARAMS = [1]\n",
    "if flags.grad_accu > 1:\n",
    "    LOSS_PARAMS = [w/flags.grad_accu for w in LOSS_PARAMS]\n",
    "\n",
    "\n",
    "data = Data(MAX_LEN, BATCH_SIZE, N_EPOCHS, DEVICE, pretrained=flags.pretrained_embeddings)\n",
    "h_params = HParams(len(data.vocab.itos), len(data.tags.itos) if flags.data == 'yelp' else None, MAX_LEN, BATCH_SIZE, N_EPOCHS,\n",
    "                   device=DEVICE, vocab_ignore_index=data.vocab.stoi['<pad>'], decoder_h=flags.decoder_h,\n",
    "                   decoder_l=flags.decoder_l, encoder_h=flags.encoder_h, encoder_l=flags.encoder_l,\n",
    "                   text_rep_h=flags.text_rep_h, text_rep_l=flags.text_rep_l,\n",
    "                   test_name=flags.test_name, grad_accumulation_steps=GRAD_ACCU,\n",
    "                   optimizer_kwargs={'lr': flags.lr, #'weight_decay': flags.l2_reg, 't0':100, 'lambd':0.},\n",
    "                                     'weight_decay': flags.l2_reg, 'betas': (0.9, 0.99)},\n",
    "                   is_weighted=[], graph_generator=GRAPH,\n",
    "                   z_size=flags.z_size, embedding_dim=flags.embedding_dim, anneal_kl=ANNEAL_KL,\n",
    "                   grad_clip=flags.grad_clip*flags.grad_accu, kl_th=flags.kl_th, highway=flags.highway,\n",
    "                   losses=LOSSES, dropout=flags.dropout, training_iw_samples=flags.training_iw_samples,\n",
    "                   testing_iw_samples=flags.testing_iw_samples, loss_params=LOSS_PARAMS, optimizer=optim.AdamW,\n",
    "                   markovian=flags.markovian, word_dropout=flags.word_dropout, contiguous_lm=False,\n",
    "                   test_prior_samples=flags.test_prior_samples, n_latents=flags.n_latents, n_keys=flags.n_keys,\n",
    "                   max_elbo=[flags.max_elbo_choice, flags.max_elbo1],  # max_elbo is paper's beta\n",
    "                   z_emb_dim=flags.z_emb_dim, minimal_enc=flags.minimal_enc, kl_beta=flags.kl_beta)\n",
    "val_iterator = iter(data.val_iter)\n",
    "print(\"Words: \", len(data.vocab.itos), \", On device: \", DEVICE.type)\n",
    "print(\"Loss Type: \", flags.losses)\n",
    "if flags.losses == 'LagVAE':\n",
    "    model = LaggingDisentanglementTransformerVAE(data.vocab, data.tags, h_params, wvs=data.wvs, dataset=flags.data,\n",
    "                                                 enc_iter=data.enc_train_iter)\n",
    "else:\n",
    "    model = DisentanglementTransformerVAE(data.vocab, data.tags, h_params, wvs=data.wvs, dataset=flags.data)\n",
    "if DEVICE.type == 'cuda':\n",
    "    model.cuda(DEVICE)\n",
    "\n",
    "total_unsupervised_train_samples = len(data.train_iter)*BATCH_SIZE\n",
    "total_unsupervised_val_samples = len(data.val_iter)*BATCH_SIZE\n",
    "print(\"Unsupervised training examples: \", total_unsupervised_train_samples)\n",
    "print(\"Unsupervised val examples: \", total_unsupervised_val_samples)\n",
    "current_time = time()\n",
    "#print(model)\n",
    "number_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Number of parameters: \", \"{0:05.2f} M\".format(number_parameters/1e6))\n",
    "number_parameters = sum(p.numel() for p in model.infer_bn.parameters() if p.requires_grad)\n",
    "print(\"Inference parameters: \", \"{0:05.2f} M\".format(number_parameters/1e6))\n",
    "number_parameters = sum(p.numel() for p in model.gen_bn.parameters() if p.requires_grad)\n",
    "print(\"Generation parameters: \", \"{0:05.2f} M\".format(number_parameters/1e6))\n",
    "number_parameters = sum(p.numel() for p in model.word_embeddings.parameters() if p.requires_grad)\n",
    "print(\"Embedding parameters: \", \"{0:05.2f} M\".format(number_parameters/1e6))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\" i 've been sorry \", ' the service was great and the food was great ', \" i have been here for years and i 've had a better experience \", ' i had a great experience with this place ', ' the food is great ']\n"
     ]
    }
   ],
   "source": [
    "text, samples, params = model.get_sentences(5, gen_len=16, sample_w=False, vary_z=True, complete=None, contains=None, max_tries=100)\n",
    "\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " i 've been sorry  : [' i had a great experience with my friend at this location ', \" i 've been a regular for a year \"]\n the service was great and the food was great  : [' service was great ', ' service was great ']\n i have been here for years and i 've had a better experience  : [\" i 've been here for years , and love \", ' unfortunately , we were very disappointed ']\n i had a great experience with this place  : [\" it 's pretty good \", \" it 's a great place to go \"]\n the food is great  : [' the food is a great place to go ', ' good food ']\n"
     ]
    }
   ],
   "source": [
    "var_ids = [8]\n",
    "alt_text, alt_params = model._get_alternative_sentences(samples, None, var_ids, 2, 16, complete=None,)\n",
    "for i in range(len(text)):\n",
    "    print(text[i], ':', alt_text[i::len(text)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==varying content==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " i 've been sorry  : [\" i 'm great for the price \", ' i love this place ']\n the service was great and the food was great  : [' the service is great , and the food is great ', ' great place to go for a quick lunch ']\n i have been here for years and i 've had a better experience  : [' the food is great ', ' they have a good selection of food and the service is great ']\n i had a great experience with this place  : [' i enjoy their variety of beers ', \" i 've tried this location \"]\n the food is great  : [' the bar is great ', ' this is a great place ']\n==varying structure==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " i 've been sorry  : [' great place ', \" i 'm a fan of this year \", ' i love this place ', ' my favorite thing about this place is the best ']\n the service was great and the food was great  : [' service was very good ', ' service was great ', ' the service is great ', ' the food is great ']\n i have been here for years and i 've had a better experience  : [' we had a fantastic experience ', \" i 've been here for years , and i love it \", \" i 've been here for years , and again \", \" i have been here for years and i 've ever had \"]\n i had a great experience with this place  : [\" it 's a great place to go \", ' i had a great experience with this place ', \" i had a great experience with a friend and it 's a disappointment \", \" it 's a great place to go \"]\n the food is great  : [' the food is good , but the service is great ', ' the food is great and the price is great ', ' the food is good and a good job ', ' the food is good and good ']\n"
     ]
    }
   ],
   "source": [
    "def _get_alternative_sentences(mdl, prev_latent_vals, params, var_z_ids, n_samples, gen_len, complete=None):\n",
    "        h_params = mdl.h_params\n",
    "        has_struct = mdl.h_params.graph_generator in (get_qkv_graph, get_hqkv_graph)\n",
    "        has_zg = mdl.h_params.graph_generator = get_hqkv_graph\n",
    "\n",
    "        n_orig_sentences = prev_latent_vals['z1'].shape[0]\n",
    "        go_symbol = torch.ones([n_samples * n_orig_sentences]).long() * \\\n",
    "                    mdl.index[mdl.generated_v].stoi['<go>']\n",
    "        go_symbol = go_symbol.to(mdl.h_params.device).unsqueeze(-1)\n",
    "        x_prev = go_symbol\n",
    "        if complete is not None:\n",
    "            for token in complete.split(' '):\n",
    "                x_prev = torch.cat(\n",
    "                    [x_prev, torch.ones([n_samples * n_orig_sentences, 1]).long().to(mdl.h_params.device) * \\\n",
    "                     mdl.index[mdl.generated_v].stoi[token]], dim=1)\n",
    "            gen_len = gen_len - len(complete.split(' '))\n",
    "\n",
    "        orig_zs = [prev_latent_vals['z{}'.format(i+1)].repeat(n_samples, 1) for i in range(len(h_params.n_latents))]\n",
    "        zs = [mdl.gen_bn.name_to_v['z{}'.format(i+1)] for i in range(len(h_params.n_latents))]\n",
    "        gen_input = {**{'z{}'.format(i+1): orig_zs[i].unsqueeze(1) for i in range(len(orig_zs))},\n",
    "                     'x_prev': torch.zeros((n_samples * n_orig_sentences, 1, mdl.generated_v.size)).to(\n",
    "                         mdl.h_params.device)}\n",
    "        if has_struct:\n",
    "            orig_zst = prev_latent_vals['zs'].repeat(n_samples, 1)\n",
    "            zst = mdl.gen_bn.name_to_v['zs']\n",
    "            gen_input['zs'] = orig_zst.unsqueeze(1)\n",
    "        if has_zg:\n",
    "            orig_zg = prev_latent_vals['zg'].repeat(n_samples, 1)\n",
    "            zg = mdl.gen_bn.name_to_v['zg']\n",
    "            # gen_input['zg'] = zg.prior_sample((n_samples * n_orig_sentences,))[0]\n",
    "            gen_input['zg'] = orig_zg.unsqueeze(1)\n",
    "        mdl.gen_bn(gen_input)\n",
    "        if has_zg:\n",
    "            z1_sample = zs[0].posterior_sample(mdl.gen_bn.name_to_v['z1'].post_params)[0].squeeze(1)\n",
    "            if has_struct:\n",
    "                zst_sample = zst.posterior_sample(mdl.gen_bn.name_to_v['zs'].post_params)[0].squeeze(1)\n",
    "        else:\n",
    "            z1_sample = zs[0].prior_sample((n_samples * n_orig_sentences,))[0]\n",
    "            if has_struct:\n",
    "                zst_sample = zst.prior_sample((n_samples * n_orig_sentences,))[0]\n",
    "        zs_sample = [z1_sample] +\\\n",
    "                    [z.post_samples.squeeze(1) for z in zs[1:]]\n",
    "\n",
    "        for id in var_z_ids:\n",
    "            # id == sum(h_params.n_latents) means its zst\n",
    "            if id == sum(h_params.n_latents) and has_struct:\n",
    "                orig_zst = zst_sample\n",
    "            else:\n",
    "                assert id < sum(h_params.n_latents)\n",
    "                z_number = sum([id > sum(h_params.n_latents[:i + 1]) for i in range(len(h_params.n_latents))])\n",
    "                z_index = id - sum(h_params.n_latents[:z_number])\n",
    "                start, end = int(h_params.z_size / max(h_params.n_latents) * z_index), int(\n",
    "                    h_params.z_size / max(h_params.n_latents) * (z_index + 1))\n",
    "                source, destination = zs_sample[z_number], orig_zs[z_number]\n",
    "                destination[:, start:end] = source[:, start:end]\n",
    "\n",
    "        z_input = {'z{}'.format(i+1): orig_zs[i].unsqueeze(1) for i in range(len(orig_zs))}\n",
    "        if has_struct:\n",
    "            z_input['zs'] = orig_zst.unsqueeze(1)\n",
    "        if has_zg:\n",
    "            z_input['zg'] = orig_zg.unsqueeze(1)\n",
    "\n",
    "        # Normal Autoregressive generation\n",
    "        for i in range(gen_len):\n",
    "            mdl.gen_bn({'x_prev': x_prev, **{k: v.expand(v.shape[0], i + 1, v.shape[-1])\n",
    "                                             for k, v in z_input.items()}})\n",
    "            samples_i = mdl.generated_v.post_params['logits']\n",
    "\n",
    "            x_prev = torch.cat([x_prev, torch.argmax(samples_i, dim=-1)[..., -1].unsqueeze(-1)],\n",
    "                               dim=-1)\n",
    "\n",
    "        text = mdl.decode_to_text2(x_prev, mdl.h_params.vocab_size, mdl.index[mdl.generated_v])\n",
    "        samples = {'z{}'.format(i+1): zs_sample[i].tolist() for i in range(len(orig_zs))}\n",
    "        if has_struct:\n",
    "            samples['zs'] = zst_sample.tolist()\n",
    "        if has_zg:\n",
    "            samples['zg'] = orig_zg.tolist()\n",
    "        return text, samples\n",
    "print(\"==varying content==\")\n",
    "var_ids = list(range(8))\n",
    "alt_text, alt_params = _get_alternative_sentences(model, samples, None, var_ids, 2, 16, complete=None)\n",
    "for i in range(len(text)):\n",
    "    print(text[i], ':', alt_text[i::len(text)])\n",
    "print(\"==varying structure==\")\n",
    "var_ids = [8]\n",
    "alt_text, alt_params = _get_alternative_sentences(model, samples, None, var_ids, 4, 16, complete=None)\n",
    "for i in range(len(text)):\n",
    "    print(text[i], ':', alt_text[i::len(text)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " no server , he was very friendly and helpful  : [' great service and great food ', ' great service ']\n the staff is friendly and helpful and friendly  : [' our food was very good and loved it and it disappeared ', ' the staff is friendly and helpful and so hard to pay her cafe ']\n the place is way more than the food is good  : [' the food was delicious and the service was great ', ' the food was delicious and the service was great ']\n amazing service , what a good time  : [' amazing people ', ' the food is always good , service is great ']\n i 've been here twice and we 're quite happy with my experience  : [' the food is always fresh and delicious ', ' i will definitely be back again ']\n"
     ]
    }
   ],
   "source": [
    "var_ids = [3, 4, 5, 6, 7]\n",
    "alt_text, alt_params = model._get_alternative_sentences(samples, None, var_ids, 2, 16, complete=None)\n",
    "for i in range(len(text)):\n",
    "    print(text[i], ':', alt_text[i::len(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['z1', 'x', 'x_prev', 'zs', 'zg'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\" i 've been sorry \", ' the service was great and the food was great ', \" i have been here for years and i 've had a better experience \", ' i had a great experience with this place ', ' the food is great ']\nz_from:   i 've been sorry  |z_to:   the service was great and the food was great  |result:   service was very good\nz_from:   i 've been sorry  |z_to:   i have been here for years and i 've had a better experience  |result:   we love this location\nz_from:   i 've been sorry  |z_to:   i had a great experience with this place  |result:   it 's a disappointment\nz_from:   i 've been sorry  |z_to:   the food is great  |result:   the food is great\nz_from:   the service was great and the food was great  |z_to:   i 've been sorry  |result:   i 've been a lot of food and the service is great\nz_from:   the service was great and the food was great  |z_to:   i have been here for years and i 've had a better experience  |result:   i 've been here for years and i love this place\nz_from:   the service was great and the food was great  |z_to:   i had a great experience with this place  |result:   i had a great experience with this place\nz_from:   the service was great and the food was great  |z_to:   the food is great  |result:   the food is great and the service is great\nz_from:   i have been here for years and i 've had a better experience  |z_to:   i 've been sorry  |result:   i had a great experience with my friend and i 've ever had\nz_from:   i have been here for years and i 've had a better experience  |z_to:   the service was great and the food was great  |result:   the staff is very friendly and the service is always very quick\nz_from:   i have been here for years and i 've had a better experience  |z_to:   i had a great experience with this place  |result:   i had a great experience with a friend and it 's a disappointment\nz_from:   i have been here for years and i 've had a better experience  |z_to:   the food is great  |result:   i had a great experience and they did a great job on it\nz_from:   i had a great experience with this place  |z_to:   i 've been sorry  |result:   i had a great experience with this place\nz_from:   i had a great experience with this place  |z_to:   the service was great and the food was great  |result:   i 'm not sure why i 'm not sure\nz_from:   i had a great experience with this place  |z_to:   i have been here for years and i 've had a better experience  |result:   i 'm not sure why i 'm not sure\nz_from:   i had a great experience with this place  |z_to:   the food is great  |result:   the food is great\nz_from:   the food is great  |z_to:   i 've been sorry  |result:   i love this place\nz_from:   the food is great  |z_to:   the service was great and the food was great  |result:   the service was very friendly\nz_from:   the food is great  |z_to:   i have been here for years and i 've had a better experience  |result:   i love this location\nz_from:   the food is great  |z_to:   i had a great experience with this place  |result:   i love this place\n"
     ]
    }
   ],
   "source": [
    "def decode_to_text(x_hat_params, vocab_size, vocab_index):\n",
    "    # It is assumed that this function is used at test time for display purposes\n",
    "    # Getting the argmax from the one hot if it's not done\n",
    "    while x_hat_params.shape[-1] == vocab_size and x_hat_params.ndim > 3:\n",
    "        x_hat_params = x_hat_params.mean(0)\n",
    "    while x_hat_params.ndim > 2 and x_hat_params.shape[-1] != self.h_params.vocab_size:\n",
    "        x_hat_params = x_hat_params[0]\n",
    "    if x_hat_params.shape[-1] == vocab_size:\n",
    "        x_hat_params = torch.argmax(x_hat_params, dim=-1)\n",
    "    assert x_hat_params.ndim == 2, \"Mis-shaped generated sequence: {}\".format(x_hat_params.shape)\n",
    "    \n",
    "    samples = [' '.join([vocab_index.itos[w]\n",
    "                         for w in sen]).split('<eos>')[0].split(' !')[0].split(' .')[0].replace('<go>', '').replace('</go>', '')\n",
    "               .replace('<pad>', '_').replace('_unk', '<?>')\n",
    "               for sen in x_hat_params]\n",
    "\n",
    "    return samples\n",
    "print(model.gen_bn.name_to_v.keys())\n",
    "\n",
    "def swap_latents(mdl, prev_latent_vals, var_z_ids, gen_len, complete=None, no_unk=True):\n",
    "            has_struct = 'zs' in mdl.gen_bn.name_to_v\n",
    "            has_zg = 'zg' in mdl.gen_bn.name_to_v\n",
    "            \n",
    "            \n",
    "            n_orig_sentences = prev_latent_vals['z1'].shape[0]\n",
    "            n_samples = n_orig_sentences\n",
    "            go_symbol = torch.ones([n_samples * n_orig_sentences]).long() * \\\n",
    "                        mdl.index[mdl.generated_v].stoi['<go>']\n",
    "            go_symbol = go_symbol.to(mdl.h_params.device).unsqueeze(-1)\n",
    "            x_prev = go_symbol\n",
    "            if complete is not None:\n",
    "                for token in complete.split(' '):\n",
    "                    x_prev = torch.cat([x_prev, torch.ones([n_samples * n_orig_sentences, 1]).long().to(mdl.h_params.device) * \\\n",
    "                        mdl.index[mdl.generated_v].stoi[token]], dim=1)\n",
    "                gen_len = gen_len - len(complete.split(' '))\n",
    "            temp = 1.\n",
    "            orig_z = prev_latent_vals['z1'].unsqueeze(1).repeat(1, n_samples, 1)\n",
    "            z_sample = orig_z.reshape(n_samples*n_orig_sentences, -1)\n",
    "            orig_z = orig_z.transpose(0, 1).reshape(n_samples*n_orig_sentences, -1)\n",
    "            if has_struct:\n",
    "                orig_zst = prev_latent_vals['zs'].unsqueeze(1).repeat(1, n_samples, 1)\n",
    "                zst_sample = orig_zst.reshape(n_samples*n_orig_sentences, -1)\n",
    "                orig_zst = orig_zst.transpose(0, 1).reshape(n_samples*n_orig_sentences, -1)\n",
    "            if has_zg:\n",
    "                orig_zg = prev_latent_vals['zg'].unsqueeze(1).repeat(1, n_samples, 1)\n",
    "                orig_zg = orig_zg.transpose(0, 1).reshape(n_samples*n_orig_sentences, -1)\n",
    "            \n",
    "\n",
    "            for id in var_z_ids:\n",
    "                if id < sum(h_params.n_latents):\n",
    "                    z_number = sum([id> sum(h_params.n_latents[:i+1]) for i in range(len(h_params.n_latents))])\n",
    "                    z_index = id - sum(h_params.n_latents[:z_number])\n",
    "                    start, end = int(h_params.z_size/max(h_params.n_latents)*z_index),\\\n",
    "                                 int(h_params.z_size/max(h_params.n_latents)*(z_index+1))\n",
    "                    source, destination = [z_sample][z_number], [orig_z][z_number]\n",
    "                    destination[:, start:end] = source[:, start:end]\n",
    "                elif id == sum(h_params.n_latents) and has_struct:\n",
    "                    orig_zst = zst_sample\n",
    "                else:\n",
    "                    raise IndexError(\"You gave a too high z_id for swapping with this model\")\n",
    "                    \n",
    "            z_input = {'z1': orig_z.unsqueeze(1), **({'zs':orig_zst.unsqueeze(1)} if has_struct else {}), \n",
    "                       **({'zg':orig_zg.unsqueeze(1)} if has_zg else {})}\n",
    "            \n",
    "            # Normal Autoregressive generation\n",
    "            for i in range(gen_len):\n",
    "                mdl.gen_bn({'x_prev': x_prev, **{k: v.expand(v.shape[0], i+1, v.shape[-1])\n",
    "                                                  for k, v in z_input.items()}})\n",
    "                samples_i = mdl.generated_v.post_params['logits']\n",
    "                if no_unk:\n",
    "                    annul_vector = 1-F.one_hot(torch.tensor([data.vocab.stoi['<unk>']]).to(DEVICE), h_params.vocab_size)\n",
    "                    samples_i *= annul_vector\n",
    "                \n",
    "                x_prev = torch.cat([x_prev, torch.argmax(samples_i,     dim=-1)[..., -1].unsqueeze(-1)],\n",
    "                                   dim=-1)\n",
    "            \n",
    "            text = decode_to_text(x_prev, mdl.h_params.vocab_size, mdl.index[mdl.generated_v])\n",
    "            return text, {'z1': orig_z}\n",
    "sw_zs = [8]\n",
    "sw_text, sw_samples = swap_latents(model, samples, sw_zs, 16, complete=None, no_unk=True)\n",
    "print(text)\n",
    "for i in range(len(text)):\n",
    "    for j in range(len(text)):\n",
    "        if i!=j:\n",
    "            print(\"z_from: \", text[i], \"|z_to: \", text[j], \"|result: \", sw_text[len(text)*i+j])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['z1', 'x', 'x_prev', 'zs', 'zg'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\" i 've been sorry \", ' the service was great and the food was great ', \" i have been here for years and i 've had a better experience \", ' i had a great experience with this place ', ' the food is great ']\nz_from:   i 've been sorry  |z_to:   the service was great and the food was great  |result:   service was very good\nz_from:   i 've been sorry  |z_to:   i have been here for years and i 've had a better experience  |result:   we love this location\nz_from:   i 've been sorry  |z_to:   i had a great experience with this place  |result:   it 's a disappointment\nz_from:   i 've been sorry  |z_to:   the food is great  |result:   the food is great\nz_from:   the service was great and the food was great  |z_to:   i 've been sorry  |result:   i 've been a lot of food and the service is great\nz_from:   the service was great and the food was great  |z_to:   i have been here for years and i 've had a better experience  |result:   i 've been here for years and i love this place\nz_from:   the service was great and the food was great  |z_to:   i had a great experience with this place  |result:   i had a great experience with this place\nz_from:   the service was great and the food was great  |z_to:   the food is great  |result:   the food is great and the service is great\nz_from:   i have been here for years and i 've had a better experience  |z_to:   i 've been sorry  |result:   i had a great experience with my friend and i 've ever had\nz_from:   i have been here for years and i 've had a better experience  |z_to:   the service was great and the food was great  |result:   the staff is very friendly and the service is always very quick\nz_from:   i have been here for years and i 've had a better experience  |z_to:   i had a great experience with this place  |result:   i had a great experience with a friend and it 's a disappointment\nz_from:   i have been here for years and i 've had a better experience  |z_to:   the food is great  |result:   i had a great experience and they did a great job on it\nz_from:   i had a great experience with this place  |z_to:   i 've been sorry  |result:   i had a great experience with this place\nz_from:   i had a great experience with this place  |z_to:   the service was great and the food was great  |result:   i 'm not sure why i 'm not sure\nz_from:   i had a great experience with this place  |z_to:   i have been here for years and i 've had a better experience  |result:   i 'm not sure why i 'm not sure\nz_from:   i had a great experience with this place  |z_to:   the food is great  |result:   the food is great\nz_from:   the food is great  |z_to:   i 've been sorry  |result:   i love this place\nz_from:   the food is great  |z_to:   the service was great and the food was great  |result:   the service was very friendly\nz_from:   the food is great  |z_to:   i have been here for years and i 've had a better experience  |result:   i love this location\nz_from:   the food is great  |z_to:   i had a great experience with this place  |result:   i love this place\n"
     ]
    }
   ],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' i love this place ', ' i would not recommend this place to anyone ', \" i was n't even run to the restaurant \", \" i 've had a great experience \", ' i would not recommend this place to anyone who ']\n"
     ]
    }
   ],
   "source": [
    "text, samples, params = model.get_sentences(5, gen_len=16, sample_w=False, vary_z=True, complete=None, \n",
    "                                            contains=None, max_tries=100)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Changing Structure=======\n-->  i 've been sorry  |<  i had a great experience with my friend at this location >< my favorite thing about this place is the best >< i 'm a fan of my favorite joint >< i 've been a regular for years and love  >\n-->  the service was great and the food was great  |<  service was very friendly and the food was great >< service was very friendly and the food was great >< the food is great >< the staff is always friendly  >\n-->  i have been here for years and i 've had a better experience  |<  we had a fantastic experience with this >< we love this location >< i love this location >< unfortunately , i was n't impressed  >\n-->  i had a great experience with this place  |<  i had a great experience tonight >< i had a great experience with a friend and it 's a great experience >< it 's a disappointment >< it 's a great place to eat  >\n-->  the food is great  |<  i had a great experience here >< good food and a good price >< the food is good , but the service is great >< the food is good and not good  >\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Changing Content=======\n-->  i 've been sorry  |<  great place and fast >< the atmosphere was great >< i just went there >< i have always had  >\n-->  the service was great and the food was great  |<  the staff is always friendly and helpful >< i 've never had a bad experience >< i have been here for years and it 's a great place >< i love this place  >\n-->  i have been here for years and i 've had a better experience  |<  i 'm not sure why i do n't go to the bar >< i have been here for years and it 's a bad experience >< i had a great experience with the other day , i 'm disappointed >< i had a great experience with the staff and they were very helpful  >\n-->  i had a great experience with this place  |<  i love this place >< i 'm not sure why i 'm not sure why i 'm going back >< i love this place >< i 'm not sure why i 'm not sure why  >\n-->  the food is great  |<  the food was ok >< i love this place >< i love this place >< i love this store  >\n"
     ]
    }
   ],
   "source": [
    "alt_text, alt_samples = model._get_alternative_sentences(samples, params, [8], 4, 16, complete=None)\n",
    "print(\"====== Changing Structure=======\")\n",
    "for i in range(len(text)):\n",
    "    print(\"-->\", text[i], '|<', '><'.join(alt_text[i::len(text)]), '>')\n",
    "alt_text, alt_samples = model._get_alternative_sentences(samples, params, list(range(8)), 4, 16, complete=None)\n",
    "print(\"====== Changing Content=======\")\n",
    "for i in range(len(text)):\n",
    "    print(\"-->\", text[i], '|<', '><'.join(alt_text[i::len(text)]), '>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  50%|█████     | 1/2 [00:01<00:01,  1.68s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences: 100%|██████████| 2/2 [00:02<00:00,  1.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences: 100%|██████████| 2/2 [00:02<00:00,  1.09s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n\rProcessing sample 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  50%|█████     | 1/2 [00:11<00:11, 11.55s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0: 100%|██████████| 2/2 [00:22<00:00, 11.51s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0: 100%|██████████| 2/2 [00:22<00:00, 11.48s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n\rProcessing sample 1:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  50%|█████     | 1/2 [00:11<00:11, 11.73s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1: 100%|██████████| 2/2 [00:22<00:00, 11.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1: 100%|██████████| 2/2 [00:22<00:00, 11.41s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from disentanglement_qkv.models import template_match, tqdm, pd\n",
    "def get_generation_TMA(self, n_samples=2000, n_alterations=1, batch_size=100):\n",
    "        stats = []\n",
    "        has_struct = 'zs' in self.gen_bn.name_to_v\n",
    "        assert has_struct\n",
    "        alter_lvs = [list(range(sum(self.h_params.n_latents))), [sum(self.h_params.n_latents)]]\n",
    "        n_lvs = sum(self.h_params.n_latents) + 1\n",
    "        # Generating n_samples sentences\n",
    "        text, samples, _ = self.get_sentences(n_samples=batch_size, gen_len=self.h_params.max_len - 1,\n",
    "                                              sample_w=False, vary_z=True, complete=None)\n",
    "        for _ in tqdm(range(int(n_samples / batch_size)), desc=\"Generating original sentences\"):\n",
    "            text_i, samples_i, _ = self.get_sentences(n_samples=batch_size, gen_len=self.h_params.max_len - 1,\n",
    "                                                       sample_w=False, vary_z=True, complete=None)\n",
    "            text.extend(text_i)\n",
    "            for k in samples.keys():\n",
    "                samples[k] = torch.cat([samples[k], samples_i[k]])\n",
    "        for i in range(int(n_samples / batch_size)):\n",
    "            for alvs in tqdm(alter_lvs, desc=\"Processing sample {}\".format(str(i))):\n",
    "                # Altering the sentences\n",
    "                alt_text, _ = self._get_alternative_sentences(\n",
    "                    prev_latent_vals={k: v[i * batch_size:(i + 1) * batch_size]\n",
    "                                      for k, v in samples.items()},\n",
    "                    params=None, var_z_ids=alvs, n_samples=n_alterations,\n",
    "                    gen_len=self.h_params.max_len - 1, complete=None)\n",
    "                # Getting alteration statistics\n",
    "                orig_texts = [text[(i * batch_size) + k % batch_size] for k in range(n_alterations * batch_size)]\n",
    "                tma2 = template_match(orig_texts, alt_text, 2)\n",
    "                tma3 = template_match(orig_texts, alt_text, 3)\n",
    "                altered_var = 'zc' if alvs[0]!=(n_lvs-1) else 'zs'\n",
    "                for k in range(n_alterations * batch_size):\n",
    "                    stats.append([orig_texts[k], alt_text[k], altered_var, tma2[k], tma3[k]])\n",
    "\n",
    "        header = ['original', 'altered', 'alteration_id', 'tma2', 'tma3']\n",
    "        df = pd.DataFrame(stats, columns=header)\n",
    "        var_wise_scores = df.groupby('alteration_id').mean()[['tma2', 'tma3']]\n",
    "        return var_wise_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  50%|█████     | 1/2 [00:01<00:01,  1.68s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences: 100%|██████████| 2/2 [00:02<00:00,  1.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences: 100%|██████████| 2/2 [00:02<00:00,  1.09s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n\rProcessing sample 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0:  50%|█████     | 1/2 [00:11<00:11, 11.55s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0: 100%|██████████| 2/2 [00:22<00:00, 11.51s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 0: 100%|██████████| 2/2 [00:22<00:00, 11.48s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n\rProcessing sample 1:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1:  50%|█████     | 1/2 [00:11<00:11, 11.73s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1: 100%|██████████| 2/2 [00:22<00:00, 11.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing sample 1: 100%|██████████| 2/2 [00:22<00:00, 11.41s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tma_mat = get_generation_TMA(model, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                tma2   tma3\nalteration_id              \nzc             0.315  0.160\nzs             0.275  0.105\n"
     ]
    }
   ],
   "source": [
    "print(tma_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from disentanglement_qkv.models import bleu_score, tqdm, template_match\n",
    "import os\n",
    "def get_swap_tma(self, n_samples=2000, batch_size=50, beam_size=2):\n",
    "    with torch.no_grad():\n",
    "        has_struct = 'zs' in self.gen_bn.name_to_v\n",
    "        assert has_struct\n",
    "        # Generating n_samples sentences\n",
    "        text, samples, _ = self.get_sentences(n_samples=batch_size, gen_len=self.h_params.max_len - 1,\n",
    "                                              sample_w=False, vary_z=True, complete=None)\n",
    "        for _ in tqdm(range(int(n_samples / batch_size)-1), desc=\"Generating original sentences\"):\n",
    "            text_i, samples_i, _ = self.get_sentences(n_samples=batch_size, gen_len=self.h_params.max_len - 1,\n",
    "                                                      sample_w=False, vary_z=True, complete=None)\n",
    "            text.extend(text_i)\n",
    "            for k in samples.keys():\n",
    "                samples[k] = torch.cat([samples[k], samples_i[k]])\n",
    "        source_sents, target_sents = text[:int(n_samples / 2)], text[int(n_samples / 2):]\n",
    "        source_lvs, target_lvs = {k: v[:int(n_samples / 2)] for k, v in samples.items()}, \\\n",
    "                                 {k: v[int(n_samples / 2):] for k, v in samples.items()}\n",
    "        result_sents = []\n",
    "        inv_result_sents = []\n",
    "        go_symbol = torch.ones((1, 1)).long() * self.index[self.generated_v].stoi['<go>']\n",
    "        go_symbol = go_symbol.to(self.h_params.device)\n",
    "        temp = 1.\n",
    "        for i in tqdm(range(int(n_samples / (2 * batch_size))),\n",
    "                      desc=\"Getting Model Swap TMA\"):\n",
    "            z_input = {'zs': source_lvs['zs'][i * batch_size:(i + 1) * batch_size].unsqueeze(1),\n",
    "                       **{'z{}'.format(i + 1): target_lvs['z{}'.format(i + 1)][\n",
    "                                               i * batch_size:(i + 1) * batch_size].unsqueeze(1)\n",
    "                          for i in range(len(self.h_params.n_latents))}}\n",
    "            inv_z_input = {'zs': target_lvs['zs'][i * batch_size:(i + 1) * batch_size].unsqueeze(1),\n",
    "                           **{'z{}'.format(i + 1): source_lvs['z{}'.format(i + 1)][\n",
    "                                                   i * batch_size:(i + 1) * batch_size].unsqueeze(1)\n",
    "                              for i in range(len(self.h_params.n_latents))}}\n",
    "            # for z_in, sents in zip([z_input, inv_z_input], [result_sents, inv_result_sents]):\n",
    "            #     x_prev = go_symbol.repeat((batch_size, 1))\n",
    "            #     x_prev = self.generate_from_z2(z_in, x_prev, mask_unk=False, beam_size=beam_size)\n",
    "            #     if beam_size > 1:\n",
    "            #         x_prev = x_prev[:int(x_prev.shape[0] / beam_size)]\n",
    "            #     sents.extend(self.decode_to_text2(x_prev, self.h_params.vocab_size,\n",
    "            #                                       self.index[self.generated_v]))\n",
    "            x_prev = go_symbol.repeat((batch_size, 1))\n",
    "            x_prev = self.generate_from_z2(z_input, x_prev, mask_unk=False, beam_size=beam_size)\n",
    "            if beam_size > 1:\n",
    "                x_prev = x_prev[:int(x_prev.shape[0] / beam_size)]\n",
    "            result_sents.extend(self.decode_to_text2(x_prev, self.h_params.vocab_size,\n",
    "                                              self.index[self.generated_v]))\n",
    "            x_prev = go_symbol.repeat((batch_size, 1))\n",
    "            x_prev = self.generate_from_z2(inv_z_input, x_prev, mask_unk=False, beam_size=beam_size)\n",
    "            if beam_size > 1:\n",
    "                x_prev = x_prev[:int(x_prev.shape[0] / beam_size)]\n",
    "            inv_result_sents.extend(self.decode_to_text2(x_prev, self.h_params.vocab_size,\n",
    "                                              self.index[self.generated_v]))\n",
    "        test_name = self.h_params.test_name.split(\"\\\\\")[-1].split(\"/\")[-1]\n",
    "        dump_location = os.path.join(\".data\", \n",
    "                    \"{}_tempdump.tsv\".format(test_name))\n",
    "        with open(dump_location, 'w', encoding=\"UTF-8\") as f:\n",
    "            for s, t, r, i in zip(source_sents, target_sents, result_sents, \n",
    "                                  inv_result_sents):\n",
    "                f.write('\\t'.join([s, t, r, i])+'\\n')\n",
    "            \n",
    "        print(\"Calculating zs tma...\")\n",
    "        zs_tma2, zs_tma3 = np.mean(template_match(source_sents, result_sents, 2))*100, \\\n",
    "                               np.mean(template_match(source_sents, result_sents, 3))*100\n",
    "        print(\"Calculating zc tma...\")\n",
    "        zc_tma2, zc_tma3 = np.mean(template_match(source_sents, inv_result_sents, 2))*100, \\\n",
    "                                       np.mean(template_match(source_sents, inv_result_sents, 3))*100\n",
    "        print(\"Calculating copy tma...\")\n",
    "        copy_tma2, copy_tma3 = np.mean(template_match(source_sents, target_sents, 2))*100, \\\n",
    "                               np.mean(template_match(source_sents, target_sents, 3))*100\n",
    "\n",
    "        print(\"Calculating zs bleu...\")\n",
    "        zs_bleu = bleu_score(predictions=[s.split() for s in source_sents],\n",
    "                                   references=[[s.split()] for s in result_sents])['bleu']*100\n",
    "        print(\"Calculating zc bleu...\")\n",
    "        zc_bleu = bleu_score(predictions=[s.split() for s in source_sents],\n",
    "                                   references=[[s.split()] for s in inv_result_sents])['bleu']*100\n",
    "        print(\"Calculating copy bleu...\")\n",
    "        copy_bleu = bleu_score(predictions=[s.split() for s in source_sents],\n",
    "                                   references=[[s.split()] for s in target_sents])['bleu']*100\n",
    "        \n",
    "        return zs_tma2, zs_tma3, zc_tma2, zc_tma3, copy_tma2, copy_tma3, zs_bleu, zc_bleu, copy_bleu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:   0%|          | 0/99 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:   1%|          | 1/99 [00:00<00:27,  3.62it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:   2%|▏         | 2/99 [00:00<00:26,  3.62it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:   3%|▎         | 3/99 [00:00<00:26,  3.60it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:   4%|▍         | 4/99 [00:01<00:26,  3.60it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:   5%|▌         | 5/99 [00:01<00:25,  3.72it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:   6%|▌         | 6/99 [00:01<00:25,  3.67it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:   7%|▋         | 7/99 [00:01<00:24,  3.72it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:   8%|▊         | 8/99 [00:02<00:24,  3.76it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:   9%|▉         | 9/99 [00:02<00:23,  3.85it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  10%|█         | 10/99 [00:02<00:23,  3.84it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  11%|█         | 11/99 [00:02<00:22,  3.87it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  12%|█▏        | 12/99 [00:03<00:22,  3.90it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  13%|█▎        | 13/99 [00:03<00:22,  3.89it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  14%|█▍        | 14/99 [00:03<00:22,  3.82it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  15%|█▌        | 15/99 [00:03<00:22,  3.81it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  16%|█▌        | 16/99 [00:04<00:21,  3.81it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  17%|█▋        | 17/99 [00:04<00:21,  3.88it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  18%|█▊        | 18/99 [00:04<00:20,  3.90it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  19%|█▉        | 19/99 [00:04<00:20,  3.89it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  20%|██        | 20/99 [00:05<00:20,  3.85it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  21%|██        | 21/99 [00:05<00:20,  3.89it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  22%|██▏       | 22/99 [00:05<00:19,  3.87it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  23%|██▎       | 23/99 [00:06<00:20,  3.79it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  24%|██▍       | 24/99 [00:06<00:19,  3.85it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  25%|██▌       | 25/99 [00:06<00:19,  3.87it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  26%|██▋       | 26/99 [00:06<00:18,  3.96it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  27%|██▋       | 27/99 [00:07<00:18,  3.90it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  28%|██▊       | 28/99 [00:07<00:18,  3.85it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  29%|██▉       | 29/99 [00:07<00:18,  3.84it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  30%|███       | 30/99 [00:07<00:18,  3.72it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  31%|███▏      | 31/99 [00:08<00:19,  3.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  32%|███▏      | 32/99 [00:08<00:19,  3.50it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  33%|███▎      | 33/99 [00:08<00:18,  3.55it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  34%|███▍      | 34/99 [00:09<00:19,  3.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  35%|███▌      | 35/99 [00:09<00:19,  3.23it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  36%|███▋      | 36/99 [00:09<00:20,  3.08it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  37%|███▋      | 37/99 [00:10<00:19,  3.11it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  38%|███▊      | 38/99 [00:10<00:19,  3.17it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  39%|███▉      | 39/99 [00:10<00:18,  3.24it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  40%|████      | 40/99 [00:10<00:17,  3.34it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  41%|████▏     | 41/99 [00:11<00:17,  3.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  42%|████▏     | 42/99 [00:11<00:16,  3.43it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  43%|████▎     | 43/99 [00:11<00:16,  3.46it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  44%|████▍     | 44/99 [00:12<00:15,  3.55it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  45%|████▌     | 45/99 [00:12<00:15,  3.49it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  46%|████▋     | 46/99 [00:12<00:15,  3.51it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  47%|████▋     | 47/99 [00:12<00:14,  3.61it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  48%|████▊     | 48/99 [00:13<00:13,  3.73it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  49%|████▉     | 49/99 [00:13<00:12,  3.87it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  51%|█████     | 50/99 [00:13<00:12,  3.99it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  52%|█████▏    | 51/99 [00:13<00:11,  4.12it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  53%|█████▎    | 52/99 [00:14<00:11,  4.24it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  54%|█████▎    | 53/99 [00:14<00:10,  4.31it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  55%|█████▍    | 54/99 [00:14<00:10,  4.25it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  56%|█████▌    | 55/99 [00:14<00:10,  4.17it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  57%|█████▋    | 56/99 [00:15<00:11,  3.86it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  58%|█████▊    | 57/99 [00:15<00:12,  3.28it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  59%|█████▊    | 58/99 [00:15<00:12,  3.23it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  60%|█████▉    | 59/99 [00:16<00:12,  3.25it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  61%|██████    | 60/99 [00:16<00:12,  3.21it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  62%|██████▏   | 61/99 [00:16<00:11,  3.18it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  63%|██████▎   | 62/99 [00:17<00:11,  3.11it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  64%|██████▎   | 63/99 [00:17<00:11,  3.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  65%|██████▍   | 64/99 [00:17<00:11,  3.10it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  66%|██████▌   | 65/99 [00:18<00:10,  3.24it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  67%|██████▋   | 66/99 [00:18<00:09,  3.31it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  68%|██████▊   | 67/99 [00:18<00:09,  3.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  69%|██████▊   | 68/99 [00:18<00:09,  3.27it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  70%|██████▉   | 69/99 [00:19<00:09,  3.10it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  71%|███████   | 70/99 [00:19<00:09,  3.08it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  72%|███████▏  | 71/99 [00:19<00:09,  3.09it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  73%|███████▎  | 72/99 [00:20<00:08,  3.03it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  74%|███████▎  | 73/99 [00:20<00:08,  2.95it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  75%|███████▍  | 74/99 [00:21<00:08,  2.91it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  76%|███████▌  | 75/99 [00:21<00:08,  2.86it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  77%|███████▋  | 76/99 [00:21<00:07,  2.99it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  78%|███████▊  | 77/99 [00:21<00:07,  3.13it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  79%|███████▉  | 78/99 [00:22<00:06,  3.23it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  80%|███████▉  | 79/99 [00:22<00:06,  3.23it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  81%|████████  | 80/99 [00:22<00:05,  3.31it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  82%|████████▏ | 81/99 [00:23<00:05,  3.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  83%|████████▎ | 82/99 [00:23<00:04,  3.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  84%|████████▍ | 83/99 [00:23<00:04,  3.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  85%|████████▍ | 84/99 [00:24<00:04,  3.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  86%|████████▌ | 85/99 [00:24<00:04,  3.43it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  87%|████████▋ | 86/99 [00:24<00:03,  3.45it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  88%|████████▊ | 87/99 [00:24<00:03,  3.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  89%|████████▉ | 88/99 [00:25<00:03,  3.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  90%|████████▉ | 89/99 [00:25<00:02,  3.46it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  91%|█████████ | 90/99 [00:25<00:02,  3.49it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  92%|█████████▏| 91/99 [00:26<00:02,  3.48it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  93%|█████████▎| 92/99 [00:26<00:01,  3.51it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  94%|█████████▍| 93/99 [00:26<00:01,  3.50it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  95%|█████████▍| 94/99 [00:26<00:01,  3.54it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  96%|█████████▌| 95/99 [00:27<00:01,  3.54it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  97%|█████████▋| 96/99 [00:27<00:00,  3.49it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  98%|█████████▊| 97/99 [00:27<00:00,  3.46it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences:  99%|█████████▉| 98/99 [00:28<00:00,  3.43it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences: 100%|██████████| 99/99 [00:28<00:00,  3.43it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGenerating original sentences: 100%|██████████| 99/99 [00:28<00:00,  3.49it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n\rGetting Model Swap TMA:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:   2%|▏         | 1/50 [00:03<02:31,  3.09s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:   4%|▍         | 2/50 [00:05<02:22,  2.97s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:   6%|▌         | 3/50 [00:08<02:15,  2.89s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:   8%|▊         | 4/50 [00:11<02:13,  2.91s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  10%|█         | 5/50 [00:14<02:08,  2.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  12%|█▏        | 6/50 [00:16<02:03,  2.81s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  14%|█▍        | 7/50 [00:19<01:59,  2.77s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  16%|█▌        | 8/50 [00:22<01:54,  2.72s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  18%|█▊        | 9/50 [00:25<01:54,  2.78s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  20%|██        | 10/50 [00:29<02:05,  3.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  22%|██▏       | 11/50 [00:32<02:05,  3.22s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  24%|██▍       | 12/50 [00:36<02:06,  3.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  26%|██▌       | 13/50 [00:39<02:01,  3.28s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  28%|██▊       | 14/50 [00:43<02:05,  3.47s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  30%|███       | 15/50 [00:47<02:15,  3.87s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  32%|███▏      | 16/50 [00:51<02:10,  3.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  34%|███▍      | 17/50 [00:55<02:04,  3.78s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  36%|███▌      | 18/50 [00:58<01:51,  3.47s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  38%|███▊      | 19/50 [01:00<01:41,  3.26s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  40%|████      | 20/50 [01:03<01:31,  3.06s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  42%|████▏     | 21/50 [01:07<01:37,  3.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  44%|████▍     | 22/50 [01:11<01:40,  3.59s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  46%|████▌     | 23/50 [01:15<01:39,  3.69s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  48%|████▊     | 24/50 [01:19<01:38,  3.81s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  50%|█████     | 25/50 [01:23<01:37,  3.92s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  52%|█████▏    | 26/50 [01:27<01:33,  3.88s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  54%|█████▍    | 27/50 [01:30<01:21,  3.55s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  56%|█████▌    | 28/50 [01:33<01:12,  3.28s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  58%|█████▊    | 29/50 [01:35<01:05,  3.11s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  60%|██████    | 30/50 [01:38<00:59,  2.99s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  62%|██████▏   | 31/50 [01:41<00:55,  2.90s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  64%|██████▍   | 32/50 [01:43<00:51,  2.86s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  66%|██████▌   | 33/50 [01:46<00:48,  2.83s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  68%|██████▊   | 34/50 [01:49<00:44,  2.80s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  70%|███████   | 35/50 [01:52<00:42,  2.82s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  72%|███████▏  | 36/50 [01:55<00:39,  2.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  74%|███████▍  | 37/50 [01:57<00:36,  2.79s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  76%|███████▌  | 38/50 [02:00<00:33,  2.79s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  78%|███████▊  | 39/50 [02:03<00:30,  2.76s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  80%|████████  | 40/50 [02:06<00:27,  2.77s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  82%|████████▏ | 41/50 [02:08<00:25,  2.79s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  84%|████████▍ | 42/50 [02:11<00:22,  2.79s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  86%|████████▌ | 43/50 [02:14<00:19,  2.79s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  88%|████████▊ | 44/50 [02:17<00:16,  2.79s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  90%|█████████ | 45/50 [02:20<00:14,  2.80s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  92%|█████████▏| 46/50 [02:22<00:11,  2.79s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  94%|█████████▍| 47/50 [02:25<00:08,  2.81s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  96%|█████████▌| 48/50 [02:28<00:05,  2.81s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA:  98%|█████████▊| 49/50 [02:31<00:02,  2.82s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA: 100%|██████████| 50/50 [02:34<00:00,  2.79s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rGetting Model Swap TMA: 100%|██████████| 50/50 [02:34<00:00,  3.08s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating zs tma...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating zc tma...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating copy tma...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating bleu scores...\n"
     ]
    }
   ],
   "source": [
    "# zs_tma2, zs_tma3, zc_tma2, zc_tma3, copy_tma2, copy_tma3, zs_bleu, zc_bleu, copy_bleu\\\n",
    "#     = get_swap_tma(model, n_samples=200, batch_size=20)\n",
    "tma_res = model.get_swap_tma(n_samples=2000, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paraphrase results 1 :  {'template': {'zs': 0.53272, 'zc': 0.51238}, 'paraphrase': {'zs': 0.55836, 'zc': 0.50162}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paraphrase detection: with zs 0.502, with zc 0.468\n{'template': {'zs': 0.53272, 'zc': 0.51238}, 'paraphrase': {'zs': 0.55836, 'zc': 0.50162}, 'hard': {'zs': 0.468, 'zc': 0.468}}\n"
     ]
    }
   ],
   "source": [
    "# print(tma_res)\n",
    "res_enc = model.get_syn_disent_encoder(batch_size=20)\n",
    "print(res_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        \t tma2 \t tma3 \t bleu\ncopy\t 34.599999999999994 \t 6.3 \t 3.2425404362197012\nzc    \t 39.0 \t 7.000000000000001 \t 3.211958605276792\nzs    \t 46.800000000000004 \t 13.5 \t 5.234059907070587\n"
     ]
    }
   ],
   "source": [
    "print(\"        \\t tma2 \\t tma3 \\t bleu\")\n",
    "print(\"copy\\t\", copy_tma2, \"\\t\", copy_tma3, \"\\t\", copy_bleu)\n",
    "print(\"zc    \\t\", zc_tma2, \"\\t\", zc_tma3, \"\\t\", zc_bleu)\n",
    "print(\"zs    \\t\", zs_tma2, \"\\t\", zs_tma3, \"\\t\", zs_bleu)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this place is great  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ |||| i will not be back  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ |||| the food is delicious  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ |||| the service is outstanding  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ |||| this place is the best i 've ever been to  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ |||| the chicken was good  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ |||| the service was <unk> and the food was delicious  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ |||| also way too much cheese  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ |||| gross  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ngreat place  great  '' and and and and                                          |||| i will not  back  ''  and and                                          |||| food food  delicious  '' and and and and                                          |||| service service  outstanding  '' and and and and                                          |||| i place is awesome worst  ever ever had to  ''                                        |||| food food  good  ''  and and and                                          |||| the food was great   food was great  ''                                         |||| it way too    ''  and and and                                         |||| gross  ''\n"
     ]
    }
   ],
   "source": [
    "sents1 = [\"This place is great .\",\n",
    "         \"I will not be back .\",\n",
    "         \"The food is delicious .\",\n",
    "         \"The service is outstanding .\",\n",
    "         \"This place is the best I 've ever been to .\",\n",
    "          \"The chicken was good .\",\n",
    "          \"The service was great, and the food was delicious .\",\n",
    "          \"also way too much cheese .\",\n",
    "          \"gross .\"]\n",
    "\n",
    "# sents1 = [\n",
    "# \"no more service from me .\",\n",
    "# \"worst service i 've ever experienced .\",\n",
    "# \"did n't even acknowledge us .\",\n",
    "# \"do n't go here if you want any kind of service .\",\n",
    "# \"horrible , horrible service they do n't deserve any stars .\",\n",
    "# ]\n",
    "model.eval()\n",
    "# print(data.tokenizer.encode(sents1))\n",
    "def embed_sents(self, sents):\n",
    "    zs_infer, z_infer, x_gen = self.infer_bn.name_to_v['zs'], \\\n",
    "                               {'z{}'.format(i + 1): self.infer_bn.name_to_v['z{}'.format(i + 1)]\n",
    "                                for i in range(len(self.h_params.n_latents))}, self.gen_bn.name_to_v['x']\n",
    "\n",
    "    bsz, max_len = len(sents), max([len(s) for s in sents])\n",
    "    stoi = self.index[self.generated_v].stoi\n",
    "    inputs = torch.zeros((bsz, max_len)).to(self.h_params.device).long()+stoi['<pad>']\n",
    "    for i, sen in enumerate(sents):\n",
    "        for j, tok in enumerate(sen.lower().split()):\n",
    "            inputs[i, j] = stoi[tok] if tok in stoi else stoi['<unk>']\n",
    "\n",
    "    self.infer_bn({'x': inputs})\n",
    "    orig_zs, orig_z = zs_infer.rep(zs_infer.infer(zs_infer.post_params))[..., 0, :], \\\n",
    "                      torch.cat([v.post_params['loc'][..., 0, :] for k, v in z_infer.items()], dim=-1)\n",
    "\n",
    "    return orig_zs, orig_z\n",
    "ezs, ezc = embed_sents(model, sents1)\n",
    "enc_samples = {\"z1\":ezc, \"zs\":ezs, \"zg\":torch.zeros_like(ezs)}\n",
    "print(ezs.shape, ezc.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this place is great  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ |||| i will not be back  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ |||| the food is delicious  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ |||| the service is outstanding  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ |||| this place is the best i 've ever been to  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ |||| the chicken was good  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ |||| the service was <unk> and the food was delicious  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ |||| also way too much cheese  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ |||| gross  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ngreat place  great  '' and and and and                                          |||| i will not  back  ''  and and                                          |||| food food  delicious  '' and and and and                                          |||| service service  outstanding  '' and and and and                                          |||| i place is awesome worst  ever ever had to  ''                                        |||| food food  good  ''  and and and                                          |||| the food was great   food was great  ''                                         |||| it way too    ''  and and and                                         |||| gross  ''\n"
     ]
    }
   ],
   "source": [
    "sents = sents1\n",
    "model.h_params.zs_anneal_kl, model.h_params.zg_anneal_kl = [7000, 10000], [7000, 10000]\n",
    "bsz, max_len = len(sents), max([len(s) for s in sents])+1\n",
    "stoi = model.index[model.generated_v].stoi\n",
    "inputs = torch.zeros((bsz, max_len)).to(model.h_params.device).long()+stoi['<pad>']\n",
    "for i, sen in enumerate(sents):\n",
    "    inputs[i, 0] = stoi['<go>']\n",
    "    for j, tok in enumerate(sen.lower().split()):\n",
    "        inputs[i, j+1] = stoi[tok] if tok in stoi else stoi['<unk>']\n",
    "    # inputs[i, j+1] = stoi['<eos>']\n",
    "model({'x': inputs[..., 1:], 'x_prev': inputs[..., :-1]}, eval=True)\n",
    "orig_text = model.decode_to_text(model.gen_bn.variables_star[model.generated_v])\n",
    "dec_text = model.decode_to_text(model.generated_v.post_params['logits'])\n",
    "print(orig_text.strip().replace('\\n', '')) \n",
    "print(dec_text.strip().replace('\\n', '')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' great place', ' not impressed', ' food is delicious', ' service is outstanding', ' love this place', ' good food', ' food was delicious', ' too bad', ' gross', ' great place', ' i will not', ' the food is delicious', ' the service is outstanding', ' love this place', ' the food was delicious', ' food was delicious', ' it was good', ' it was terrible', ' great place', ' not impressed', ' food is delicious', ' service is outstanding', ' love this place', ' food was good', ' food was delicious', ' it was good', ' love it', ' great place', ' not impressed', ' food is delicious', ' service is outstanding', ' love this place', ' food was good', ' food was delicious', ' it was good', ' love it', ' this place is great', ' i will not be back', ' the food is delicious', ' the service is outstanding', ' i love this place', ' the food was good', ' the food was delicious', ' the food was good', ' the food was terrible', ' great place', ' not impressed', ' food is delicious', ' service is outstanding', ' love this place', ' food was good', ' food was delicious', ' it was good', ' love it', ' great place', ' i will not', ' the food is delicious', ' great service', ' love this place', ' the food was delicious', ' the food was delicious', ' it was good', ' it was terrible', ' this place is great', ' i will not', ' the food is delicious', ' the service is outstanding', ' this place rocks', ' the food was good', ' the food was delicious', ' it was good', ' it was gross', ' great place', ' _num_', ' delicious', ' horrible', ' love this place', ' good food', ' delicious', ' too bad', ' gross']\nz_from:  This place is great . |z_to:  I will not be back . |result:   not impressed\nz_from:  This place is great . |z_to:  The food is delicious . |result:   food is delicious\nz_from:  This place is great . |z_to:  The service is outstanding . |result:   service is outstanding\nz_from:  This place is great . |z_to:  This place is the best I 've ever been to . |result:   love this place\nz_from:  This place is great . |z_to:  The chicken was good . |result:   good food\nz_from:  This place is great . |z_to:  The service was great, and the food was delicious . |result:   food was delicious\nz_from:  This place is great . |z_to:  also way too much cheese . |result:   too bad\nz_from:  This place is great . |z_to:  gross . |result:   gross\nz_from:  I will not be back . |z_to:  This place is great . |result:   great place\nz_from:  I will not be back . |z_to:  The food is delicious . |result:   the food is delicious\nz_from:  I will not be back . |z_to:  The service is outstanding . |result:   the service is outstanding\nz_from:  I will not be back . |z_to:  This place is the best I 've ever been to . |result:   love this place\nz_from:  I will not be back . |z_to:  The chicken was good . |result:   the food was delicious\nz_from:  I will not be back . |z_to:  The service was great, and the food was delicious . |result:   food was delicious\nz_from:  I will not be back . |z_to:  also way too much cheese . |result:   it was good\nz_from:  I will not be back . |z_to:  gross . |result:   it was terrible\nz_from:  The food is delicious . |z_to:  This place is great . |result:   great place\nz_from:  The food is delicious . |z_to:  I will not be back . |result:   not impressed\nz_from:  The food is delicious . |z_to:  The service is outstanding . |result:   service is outstanding\nz_from:  The food is delicious . |z_to:  This place is the best I 've ever been to . |result:   love this place\nz_from:  The food is delicious . |z_to:  The chicken was good . |result:   food was good\nz_from:  The food is delicious . |z_to:  The service was great, and the food was delicious . |result:   food was delicious\nz_from:  The food is delicious . |z_to:  also way too much cheese . |result:   it was good\nz_from:  The food is delicious . |z_to:  gross . |result:   love it\nz_from:  The service is outstanding . |z_to:  This place is great . |result:   great place\nz_from:  The service is outstanding . |z_to:  I will not be back . |result:   not impressed\nz_from:  The service is outstanding . |z_to:  The food is delicious . |result:   food is delicious\nz_from:  The service is outstanding . |z_to:  This place is the best I 've ever been to . |result:   love this place\nz_from:  The service is outstanding . |z_to:  The chicken was good . |result:   food was good\nz_from:  The service is outstanding . |z_to:  The service was great, and the food was delicious . |result:   food was delicious\nz_from:  The service is outstanding . |z_to:  also way too much cheese . |result:   it was good\nz_from:  The service is outstanding . |z_to:  gross . |result:   love it\nz_from:  This place is the best I 've ever been to . |z_to:  This place is great . |result:   this place is great\nz_from:  This place is the best I 've ever been to . |z_to:  I will not be back . |result:   i will not be back\nz_from:  This place is the best I 've ever been to . |z_to:  The food is delicious . |result:   the food is delicious\nz_from:  This place is the best I 've ever been to . |z_to:  The service is outstanding . |result:   the service is outstanding\nz_from:  This place is the best I 've ever been to . |z_to:  The chicken was good . |result:   the food was good\nz_from:  This place is the best I 've ever been to . |z_to:  The service was great, and the food was delicious . |result:   the food was delicious\nz_from:  This place is the best I 've ever been to . |z_to:  also way too much cheese . |result:   the food was good\nz_from:  This place is the best I 've ever been to . |z_to:  gross . |result:   the food was terrible\nz_from:  The chicken was good . |z_to:  This place is great . |result:   great place\nz_from:  The chicken was good . |z_to:  I will not be back . |result:   not impressed\nz_from:  The chicken was good . |z_to:  The food is delicious . |result:   food is delicious\nz_from:  The chicken was good . |z_to:  The service is outstanding . |result:   service is outstanding\nz_from:  The chicken was good . |z_to:  This place is the best I 've ever been to . |result:   love this place\nz_from:  The chicken was good . |z_to:  The service was great, and the food was delicious . |result:   food was delicious\nz_from:  The chicken was good . |z_to:  also way too much cheese . |result:   it was good\nz_from:  The chicken was good . |z_to:  gross . |result:   love it\nz_from:  The service was great, and the food was delicious . |z_to:  This place is great . |result:   great place\nz_from:  The service was great, and the food was delicious . |z_to:  I will not be back . |result:   i will not\nz_from:  The service was great, and the food was delicious . |z_to:  The food is delicious . |result:   the food is delicious\nz_from:  The service was great, and the food was delicious . |z_to:  The service is outstanding . |result:   great service\nz_from:  The service was great, and the food was delicious . |z_to:  This place is the best I 've ever been to . |result:   love this place\nz_from:  The service was great, and the food was delicious . |z_to:  The chicken was good . |result:   the food was delicious\nz_from:  The service was great, and the food was delicious . |z_to:  also way too much cheese . |result:   it was good\nz_from:  The service was great, and the food was delicious . |z_to:  gross . |result:   it was terrible\nz_from:  also way too much cheese . |z_to:  This place is great . |result:   this place is great\nz_from:  also way too much cheese . |z_to:  I will not be back . |result:   i will not\nz_from:  also way too much cheese . |z_to:  The food is delicious . |result:   the food is delicious\nz_from:  also way too much cheese . |z_to:  The service is outstanding . |result:   the service is outstanding\nz_from:  also way too much cheese . |z_to:  This place is the best I 've ever been to . |result:   this place rocks\nz_from:  also way too much cheese . |z_to:  The chicken was good . |result:   the food was good\nz_from:  also way too much cheese . |z_to:  The service was great, and the food was delicious . |result:   the food was delicious\nz_from:  also way too much cheese . |z_to:  gross . |result:   it was gross\nz_from:  gross . |z_to:  This place is great . |result:   great place\nz_from:  gross . |z_to:  I will not be back . |result:   _num_\nz_from:  gross . |z_to:  The food is delicious . |result:   delicious\nz_from:  gross . |z_to:  The service is outstanding . |result:   horrible\nz_from:  gross . |z_to:  This place is the best I 've ever been to . |result:   love this place\nz_from:  gross . |z_to:  The chicken was good . |result:   good food\nz_from:  gross . |z_to:  The service was great, and the food was delicious . |result:   delicious\nz_from:  gross . |z_to:  also way too much cheese . |result:   too bad\n"
     ]
    }
   ],
   "source": [
    "# sw_zs = [8]\n",
    "# sw_zs = [14]\n",
    "# sw_zs = [16]\n",
    "# sw_zs = [6]\n",
    "# sw_zs = [9]\n",
    "\n",
    "sw_zs = [8]\n",
    "\n",
    "# 8 ==> subject and 6\n",
    "#14 ==> dobj\n",
    "sw_text, sw_samples = swap_latents(model, enc_samples, sw_zs, 16, complete=None, no_unk=True)\n",
    "print(sw_text)\n",
    "for i in range(len(sents1)):\n",
    "    for j in range(len(sents1)):\n",
    "        if i!=j:\n",
    "            print(\"z_from: \", sents1[i], \"|z_to: \", sents1[j], \"|result: \", sw_text[len(sents1)*i+j])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\nFor files \"dev_input.txt, test_input.txt\" with 1300 samples:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 6.00 GiB total capacity; 1.44 GiB already allocated; 2.69 GiB free; 1.64 GiB reserved in total by PyTorch)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-0f71ba0bab86>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mezs1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mezc1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_repeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mezs1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrep_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmy_repeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mezc1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrep_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mezs2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mezc2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_repeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mezs2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrep_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmy_repeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mezc2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrep_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mezs3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mezc3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mezs1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mperm_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mezc1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mperm_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0ms12sims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms13sims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mezs1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mezs2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mezs1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mezs3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 6.00 GiB total capacity; 1.44 GiB already allocated; 2.69 GiB free; 1.64 GiB reserved in total by PyTorch)"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "def my_repeat(tens, n):\n",
    "    return tens.unsqueeze(0).expand(n, *tens.shape).reshape(tens.shape[0]*n, *tens.shape[1:])\n",
    "\n",
    "\n",
    "batch_size = 100\n",
    "for file_names in [[\"E:\\\\Experiments\\\\GLUE_BENCH\\\\.data\\\\paranmt2\\\\dev_input.txt\",\n",
    "                   \"E:\\\\Experiments\\\\GLUE_BENCH\\\\.data\\\\paranmt2\\\\test_input.txt\"],\n",
    "                   [\"E:\\\\Experiments\\\\GLUE_BENCH\\\\.data\\\\paranmt2\\\\dev.txt\",\n",
    "                   \"E:\\\\Experiments\\\\GLUE_BENCH\\\\.data\\\\paranmt2\\\\test.txt\"],\n",
    "                   [\"E:\\\\Experiments\\\\GLUE_BENCH\\\\.data\\\\qqp\\\\pos_hard.tsv\"],\n",
    "                   [\"E:\\\\Experiments\\\\GLUE_BENCH\\\\.data\\\\qqp\\\\pos.tsv\"],\n",
    "                   [\"E:\\\\Experiments\\\\GLUE_BENCH\\\\.data\\\\qqp\\\\neg.tsv\"]]:\n",
    "    print(\"---------------------------------------------------\")\n",
    "    t1, t2 = [], []\n",
    "    for file_name in file_names:\n",
    "        with open(file_name, encoding=\"UTF-8\") as f:\n",
    "            for i, l in enumerate(f):\n",
    "                if \"\\t\" in l:\n",
    "                    t1.append(l.split(\"\\t\")[0])\n",
    "                    t2.append(l.split(\"\\t\")[1])\n",
    "    \n",
    "    f_names = \"\\\"\"+', '.join([fn.split(os.sep)[-1]for fn in file_names])+\"\\\"\"\n",
    "    print(\"For files {} with {} samples:\".format(f_names, len(t1)))\n",
    "    ezs1, ezc1, ezs2, ezc2, ezs3, ezc3 = None, None, None, None, None, None\n",
    "    for i in range(int(len(t1)/batch_size)):\n",
    "        ezs1i, ezc1i = model.embed_sents(t1[i*batch_size:(i+1)*batch_size])\n",
    "        ezs2i, ezc2i = model.embed_sents(t2[i*batch_size:(i+1)*batch_size])\n",
    "        if ezs1 is None:\n",
    "            ezs1, ezc1 = ezs1i, ezc1i\n",
    "            ezs2, ezc2 = ezs2i, ezc2i\n",
    "        else:     \n",
    "            ezs1, ezc1 = torch.cat([ezs1, ezs1i]), torch.cat([ezc1, ezc1i])\n",
    "            ezs2, ezc2 = torch.cat([ezs2, ezs2i]), torch.cat([ezc2, ezc2i])\n",
    "    rep_n = 100\n",
    "    perm_idx = torch.randperm(ezs1.shape[0]*rep_n)\n",
    "    ezs1, ezc1 = my_repeat(ezs1, rep_n), my_repeat(ezc1, rep_n)\n",
    "    ezs2, ezc2 = my_repeat(ezs2, rep_n), my_repeat(ezc2, rep_n)\n",
    "    ezs3, ezc3 = ezs1[perm_idx], ezc1[perm_idx]\n",
    "    \n",
    "    s12sims, s13sims = torch.cosine_similarity(ezs1, ezs2), torch.cosine_similarity(ezs1, ezs3)\n",
    "    c12sims, c13sims = torch.cosine_similarity(ezc1, ezc2), torch.cosine_similarity(ezc1, ezc3)\n",
    "    print(\"expanded measure size to \", len(s12sims))\n",
    "    print(\"syntactic accuracy\", np.mean(s12sims.cpu().detach().numpy()>s13sims.cpu().detach().numpy()))\n",
    "    print(\"semantic accuracy\", np.mean(c12sims.cpu().detach().numpy()>c13sims.cpu().detach().numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'template': {'syn_emb': 0.6106, 'cont_emb': 0.6307}, 'paraphrase': {'syn_emb': 0.66988, 'cont_emb': 0.90536}}\n"
     ]
    }
   ],
   "source": [
    "def my_repeat(tens, n):\n",
    "    return tens.unsqueeze(0).expand(n, *tens.shape).reshape(tens.shape[0]*n, *tens.shape[1:])\n",
    "\n",
    "def my_sim(a, b):\n",
    "    dist = (a-b).square().sum(-1).sqrt()\n",
    "    sim = 1/(1+dist)\n",
    "    return sim\n",
    "\n",
    "sim = my_sim\n",
    "# sim = torch.cosine_similarity\n",
    "def _get_syn_disent_encoder_easy(self, split=\"valid\", batch_size=100):\n",
    "    template_file = {\"valid\": \"E:\\\\Experiments\\\\GLUE_BENCH\\\\.data\\\\paranmt2\\\\dev_input.txt\",\n",
    "                       \"test\": \"E:\\\\Experiments\\\\GLUE_BENCH\\\\.data\\\\paranmt2\\\\test_input.txt\"}[split]\n",
    "    paraphrase_file = {\"valid\": \"E:\\\\Experiments\\\\GLUE_BENCH\\\\.data\\\\paranmt2\\\\dev.txt\",\n",
    "                       \"test\": \"E:\\\\Experiments\\\\GLUE_BENCH\\\\.data\\\\paranmt2\\\\test.txt\"}[split]\n",
    "    file_names = {\"template\": template_file, \"paraphrase\": paraphrase_file}\n",
    "    accuracies = {\"template\": {}, \"paraphrase\": {}}\n",
    "    for task, file_n in file_names.items():\n",
    "        t1, t2 = [], []\n",
    "        with open(file_n, encoding=\"UTF-8\") as f:\n",
    "            for i, l in enumerate(f):\n",
    "                if \"\\t\" in l:\n",
    "                    t1.append(l.split(\"\\t\")[0])\n",
    "                    t2.append(l.split(\"\\t\")[1])\n",
    "        \n",
    "        ezs1, ezc1, ezs2, ezc2 = None, None, None, None\n",
    "        for i in range(int(len(t1)/batch_size)):\n",
    "            ezs1i, ezc1i = self.embed_sents(t1[i*batch_size:(i+1)*batch_size])\n",
    "            ezs2i, ezc2i = self.embed_sents(t2[i*batch_size:(i+1)*batch_size])\n",
    "            if ezs1 is None:\n",
    "                ezs1, ezc1 = ezs1i, ezc1i\n",
    "                ezs2, ezc2 = ezs2i, ezc2i\n",
    "            else:     \n",
    "                ezs1, ezc1 = torch.cat([ezs1, ezs1i]), torch.cat([ezc1, ezc1i])\n",
    "                ezs2, ezc2 = torch.cat([ezs2, ezs2i]), torch.cat([ezc2, ezc2i])\n",
    "        rep_n = 100\n",
    "        perm_idx = torch.randperm(ezs1.shape[0]*rep_n)\n",
    "        ezs1, ezc1 = my_repeat(ezs1, rep_n), my_repeat(ezc1, rep_n)\n",
    "        ezs2, ezc2 = my_repeat(ezs2, rep_n), my_repeat(ezc2, rep_n)\n",
    "        ezs3, ezc3 = ezs1[perm_idx], ezc1[perm_idx]\n",
    "        \n",
    "        s12sims, s13sims = sim(ezs1, ezs2), sim(ezs1, ezs3)\n",
    "        c12sims, c13sims = sim(ezc1, ezc2), sim(ezc1, ezc3)\n",
    "        syn_emb_sc = np.mean(s12sims.cpu().detach().numpy()>s13sims.cpu().detach().numpy())\n",
    "        cont_emb_sc = np.mean(c12sims.cpu().detach().numpy()>c13sims.cpu().detach().numpy())\n",
    "        accuracies[task] = {\"syn_emb\": syn_emb_sc, \"cont_emb\": cont_emb_sc}\n",
    "    self.writer.add_scalar('test/zs_enc_para_acc', accuracies[\"paraphrase\"][\"syn_emb\"], self.step)\n",
    "    self.writer.add_scalar('test/zc_enc_para_acc', accuracies[\"paraphrase\"][\"cont_emb\"], self.step)\n",
    "    self.writer.add_scalar('test/zs_enc_temp_acc', accuracies[\"template\"][\"syn_emb\"], self.step)\n",
    "    self.writer.add_scalar('test/zc_enc_temp_acc', accuracies[\"template\"][\"cont_emb\"], self.step)\n",
    "    return accuracies\n",
    "print(_get_syn_disent_encoder_easy(model, split=\"valid\", batch_size=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expanded measure size to  800\nParaphrase detection: with zs 0.61875, with zc 0.7475\n(0.38125, 0.7475)\n"
     ]
    }
   ],
   "source": [
    "def _get_syn_disent_encoder_hard(self, split=\"valid\", batch_size=100):\n",
    "    pair_fn = {\"valid\": \".data\\\\paranmt2\\\\dev_input.txt\", \n",
    "               \"test\": \".data\\\\paranmt2\\\\test_input.txt\"}[split]\n",
    "    ref_fn = {\"valid\": \".data\\\\paranmt2\\\\dev_ref.txt\", \n",
    "              \"test\": \".data\\\\paranmt2\\\\test_ref.txt\"}[split]\n",
    "    t1, t2, t3 = [], [], []\n",
    "    with open(pair_fn, encoding=\"UTF-8\") as f:\n",
    "        for i, l in enumerate(f):\n",
    "            if \"\\t\" in l:\n",
    "                t1.append(l.split(\"\\t\")[0])\n",
    "                t2.append(l.split(\"\\t\")[1][:-1])\n",
    "    with open(ref_fn, encoding=\"UTF-8\") as f:\n",
    "        for i, l in enumerate(f):\n",
    "            if len(l):\n",
    "                t3.append(l[:-1])    \n",
    "        \n",
    "    ezs1, ezc1, ezs2, ezc2, ezs3, ezc3 = None, None, None, None, None, None\n",
    "    for i in range(int(len(t1)/batch_size)):\n",
    "        ezs1i, ezc1i = self.embed_sents(t1[i*batch_size:(i+1)*batch_size])\n",
    "        ezs2i, ezc2i = self.embed_sents(t2[i*batch_size:(i+1)*batch_size])\n",
    "        ezs3i, ezc3i = self.embed_sents(t3[i*batch_size:(i+1)*batch_size])\n",
    "        if ezs1 is None:\n",
    "            ezs1, ezc1 = ezs1i, ezc1i\n",
    "            ezs2, ezc2 = ezs2i, ezc2i\n",
    "            ezs3, ezc3 = ezs3i, ezc3i\n",
    "        else:     \n",
    "            ezs1, ezc1 = torch.cat([ezs1, ezs1i]), torch.cat([ezc1, ezc1i])\n",
    "            ezs2, ezc2 = torch.cat([ezs2, ezs2i]), torch.cat([ezc2, ezc2i])\n",
    "            ezs3, ezc3 = torch.cat([ezs3, ezs3i]), torch.cat([ezc3, ezc3i])\n",
    "        \n",
    "    s13sims, s23sims = sim(ezs1, ezs3), sim(ezs2, ezs3)\n",
    "    c13sims, c23sims = sim(ezc1, ezc3), sim(ezc2, ezc3)\n",
    "    \n",
    "    zs_acc = np.mean(s13sims.cpu().detach().numpy()>s23sims.cpu().detach().numpy())\n",
    "    zc_acc = np.mean(c13sims.cpu().detach().numpy()>c23sims.cpu().detach().numpy())\n",
    "    print(\"expanded measure size to \", len(s13sims))\n",
    "    print(\"Paraphrase detection: with zs {}, with zc {}\".format(zs_acc, zc_acc))\n",
    "    self.writer.add_scalar('test/hard_zs_enc_acc', 1-zs_acc, self.step)\n",
    "    self.writer.add_scalar('test/hard_zc_enc_acc', zc_acc, self.step)\n",
    "    return 1-zs_acc, zc_acc\n",
    "print(_get_syn_disent_encoder_hard(model, split=\"test\", batch_size=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100000, 96])\n"
     ]
    }
   ],
   "source": [
    "print(ezs1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-20 15:04:03,746 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing evaluation with zs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-20 15:04:05,441 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-20 15:04:05,452 : Computing embeddings for train/dev/test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-20 15:07:33,968 : Computed embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-20 15:07:33,969 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-20 15:08:03,845 : [('reg:1e-05', 51.57), ('reg:0.0001', 51.54), ('reg:0.001', 51.53), ('reg:0.01', 51.46)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-20 15:08:03,848 : Validation : best param found is reg = 1e-05 with score             51.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-20 15:08:03,848 : Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-20 15:08:11,485 : \nDev acc : 51.6 Test acc : 50.6 for BIGRAMSHIFT classification\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-20 15:08:11,489 : ***** (Probing) Transfer task : DEPTH classification *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-20 15:08:11,885 : Loaded 100000 train - 10000 dev - 10000 test for Depth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-20 15:08:11,952 : Computing embeddings for train/dev/test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-20 15:11:38,548 : Computed embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-20 15:11:38,549 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-20 15:12:05,075 : [('reg:1e-05', 18.56), ('reg:0.0001', 18.52), ('reg:0.001', 18.49), ('reg:0.01', 18.43)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-20 15:12:05,076 : Validation : best param found is reg = 1e-05 with score             18.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-20 15:12:05,076 : Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-20 15:12:11,655 : \nDev acc : 18.6 Test acc : 18.4 for DEPTH classification\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-20 15:12:11,660 : ***** (Probing) Transfer task : TOPCONSTITUENTS classification *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-20 15:12:11,984 : Loaded 100000 train - 10000 dev - 10000 test for TopConstituents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-20 15:12:12,041 : Computing embeddings for train/dev/test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-20 15:15:25,016 : Computed embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-20 15:15:25,016 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-20 15:16:05,660 : [('reg:1e-05', 12.79), ('reg:0.0001', 11.39), ('reg:0.001', 9.58), ('reg:0.01', 8.19)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-20 15:16:05,661 : Validation : best param found is reg = 1e-05 with score             12.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-20 15:16:05,661 : Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-20 15:16:16,579 : \nDev acc : 12.8 Test acc : 12.3 for TOPCONSTITUENTS classification\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-20 15:16:16,659 : ***** (Probing) Transfer task : BIGRAMSHIFT classification *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing evaluation with zc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-20 15:16:16,938 : Loaded 100000 train - 10000 dev - 10000 test for BigramShift\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-20 15:16:16,946 : Computing embeddings for train/dev/test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-20 15:20:34,001 : Computed embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-20 15:20:34,002 : Training pytorch-MLP-nhid0-rmsprop-bs128 with standard validation..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-20 15:21:06,924 : [('reg:1e-05', 50.67), ('reg:0.0001', 50.57), ('reg:0.001', 50.39), ('reg:0.01', 50.39)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-20 15:21:06,925 : Validation : best param found is reg = 1e-05 with score             50.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-20 15:21:06,925 : Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-20 15:21:15,007 : \nDev acc : 50.7 Test acc : 50.5 for BIGRAMSHIFT classification\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-20 15:21:15,035 : ***** (Probing) Transfer task : DEPTH classification *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-20 15:21:15,318 : Loaded 100000 train - 10000 dev - 10000 test for Depth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-20 15:21:15,387 : Computing embeddings for train/dev/test\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 293. MiB for an array with shape (100000, 768) and data type float32",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b67a633ad974>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresults_zs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults_zc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m \u001b[0mse_result_zs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mse_results_zc\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mget_sent_eval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-b67a633ad974>\u001b[0m in \u001b[0;36mget_sent_eval\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     42\u001b[0m                           'BigramShift', 'Depth', 'TopConstituents']\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mresults_zc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransfer_tasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresults_zs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults_zc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Experiments\\GLUE_BENCH\\disentanglement_qkv\\senteval\\senteval\\engine.py\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;31m# evaluate on evaluation [name], either takes string or list of strings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Experiments\\GLUE_BENCH\\disentanglement_qkv\\senteval\\senteval\\engine.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;31m# evaluate on evaluation [name], either takes string or list of strings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Experiments\\GLUE_BENCH\\disentanglement_qkv\\senteval\\senteval\\engine.py\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_prepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatcher\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Experiments\\GLUE_BENCH\\disentanglement_qkv\\senteval\\senteval\\probing.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, params, batcher)\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatcher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m                 \u001b[0mtask_embed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'X'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m             \u001b[0mtask_embed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'X'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtask_embed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'X'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m             \u001b[0mtask_embed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtask_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Computed embeddings'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mvstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[0marrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 293. MiB for an array with shape (100000, 768) and data type float32"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from disentanglement_qkv.models import SE\n",
    "import logging\n",
    "def get_sent_eval(self):\n",
    "        def prepare(params, samples):\n",
    "            pass\n",
    "\n",
    "        def batcher_zs(params, batch):\n",
    "            batch = [' '.join(sent) if sent != [] else '.' for sent in batch]\n",
    "            embeddings = self.embed_sents(batch)[0]\n",
    "            return embeddings.detach().cpu().clone()\n",
    "        \n",
    "\n",
    "        def batcher_zc(params, batch):\n",
    "            batch = [' '.join(sent) if sent != [] else '.' for sent in batch]\n",
    "            embeddings = self.embed_sents(batch)[1]\n",
    "            return embeddings.detach().cpu().clone()\n",
    "\n",
    "        logging.basicConfig(format='%(asctime)s : %(message)s', level=logging.DEBUG)\n",
    "        # Set params for SentEval\n",
    "        print(\"Performing evaluation with zs\")\n",
    "        task_path = os.path.join(\"disentanglement_qkv\", \"senteval\", \"data\")\n",
    "        params = {'task_path': task_path, 'usepytorch': True, 'kfold': 10}\n",
    "        params['classifier'] = {'nhid': 0, 'optim': 'rmsprop', 'batch_size': 128,\n",
    "                                 'tenacity': 3, 'epoch_size': 2}\n",
    "            #{'nhid': 50, 'optim': 'adam', 'batch_size': 64, 'tenacity': 5, 'epoch_size': 4}\n",
    "        se = SE(params, batcher_zs, prepare)\n",
    "\n",
    "        transfer_tasks = [#'STS12', 'STS13', 'STS14', 'STS15', 'STS16', 'STSBenchmark',\n",
    "                          'BigramShift', 'Depth', 'TopConstituents']\n",
    "\n",
    "        results_zs = se.eval(transfer_tasks)\n",
    "\n",
    "        print(\"Performing evaluation with zc\")\n",
    "        task_path = os.path.join(\"disentanglement_qkv\", \"senteval\", \"data\")\n",
    "        params = {'task_path': task_path, 'usepytorch': True, 'kfold': 10}\n",
    "        params['classifier'] = {'nhid': 0, 'optim': 'rmsprop', 'batch_size': 128,\n",
    "                                 'tenacity': 3, 'epoch_size': 2}\n",
    "        #{'nhid': 50, 'optim': 'adam', 'batch_size': 64, 'tenacity': 5, 'epoch_size': 4}\n",
    "        se = SE(params, batcher_zc, prepare)\n",
    "\n",
    "        transfer_tasks = [#'STS12', 'STS13', 'STS14', 'STS15', 'STS16', 'STSBenchmark',\n",
    "                          'BigramShift', 'Depth', 'TopConstituents']\n",
    "\n",
    "        results_zc = se.eval(transfer_tasks)\n",
    "        return results_zs, results_zc \n",
    "\n",
    "se_result_zs, se_results_zc  = get_sent_eval(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from supar import Parser\n",
    "\n",
    "const_parser = Parser.load('crf-con-en')\n",
    "\n",
    "def truncate_tree(tree, lv):\n",
    "    tok_i = 0\n",
    "    curr_lv = 0\n",
    "    tree_toks = tree.split()\n",
    "    while tok_i != len(tree_toks):\n",
    "        if tree_toks[tok_i].startswith('('):\n",
    "            curr_lv += 1\n",
    "        else:\n",
    "            closed_lvs = int(tree_toks[tok_i].count(')'))\n",
    "            if curr_lv - closed_lvs <= lv:\n",
    "                tree_toks[tok_i] = ')'*(closed_lvs - (curr_lv-lv))\n",
    "            curr_lv -= closed_lvs\n",
    "        if lv >= curr_lv and tree_toks[tok_i]!='':\n",
    "            tok_i += 1\n",
    "        else:\n",
    "            tree_toks.pop(tok_i)\n",
    "    return ' '.join(tree_toks)\n",
    "\n",
    "def get_lin_parse_tree(sens):\n",
    "    tree_parses = const_parser.predict(sens, lang='en', verbose=False)\n",
    "    lin_parses = []\n",
    "    for p in tree_parses:\n",
    "        lin_p = repr(p)\n",
    "        if lin_p.startswith(\"(TOP\"):\n",
    "            lin_p = lin_p[5:-1]\n",
    "        lin_parses.append(lin_p)\n",
    "    return lin_parses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def template_match(l1, l2, lv, verbose=0, filter_empty=True):\n",
    "    if filter_empty:\n",
    "        not_empty1 = [any([c != \" \" for c in li1]) for li1 in l1]\n",
    "        not_empty2 = [any([c != \" \" for c in li2]) for li2 in l2]\n",
    "        l1 = [li1 for li1, ne1, ne2 in zip(l1, not_empty1, not_empty2) if ne1 and ne2]\n",
    "        l2 = [li2 for li2, ne1, ne2 in zip(l2, not_empty1, not_empty2) if ne1 and ne2]\n",
    "        print(not_empty1)\n",
    "        print(not_empty2)\n",
    "    docs1, docs2 = get_lin_parse_tree(l1), get_lin_parse_tree(l2)\n",
    "    temps1 = [truncate_tree(doc, lv) for doc in docs1]\n",
    "    temps2 = [truncate_tree(doc, lv) for doc in docs2]\n",
    "    if verbose:\n",
    "        for l, t in zip(l1+l2, temps1+temps2):\n",
    "            print(l, \"-->\", t)\n",
    "        print(\"+++++++++++++++++++++++++\")\n",
    "    return [int(t1 == t2) for t1, t2 in zip(temps1, temps2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, True, True]\n[True, True, True]\n[0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "sens1 = ['Hello dear friend', \"how is the day ?\", \"how is the weather ?\"]\n",
    "sens2 = ['My feet are on the table', \"     .\", \"where are my ladies ?\"]\n",
    "res = template_match(sens1, sens2, 2)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
